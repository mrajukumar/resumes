
CKA Certification Course â€“ Certified Kubernetes Administrator --> Tech Tutorials with piyush

Advanced Jenkins --> DevOps Shack

GitHub Actions --> kodekloud

HashiCorp Vault--> TeKanAid


Killercoda


STANDARD PORT NUMBERS:

Mysql --> 3306 --> version 8.0.28
PostgreSQL --> 5432
ssh   --> 22 
Tomcat--> 8080 -->  version 9
Jenkins-->8080 -->  version 2.346.2
RDP   --> 3389 
SMTP  --> 25 or 587
nginx --> 80   -->  version 1.26.0
Sonarqube-->9000 
 
Docker -->    version 25.0.3
Ansible -->   version 2.16
Kubernetes--> version 1.29
Github -->    version 2.43.0



MAX:

VPC - 5 --> per region
EC2 - 20 --> per region
S3 bucket - 100 --> per AWS account
Max no of EBS Volumes can attach to one EC2 instane - 27




NetWorking:

curl ifconfig.me -s --> to get your public IP (Which is used over the internet)

we will have 65000 ports.

port 0 - 1023 are reserved ports.

ports 1024 - 49152 are reserved for applications.

after 49152 we can use 

like for react app --> 3000
Mysql --> 3306
PostgreSQL --> 5432
Tomcat--> 8080
Sonarqube-->9000 
Http runs on port 80.


--> OSI Model (Open Systems Interconnection)

The OSI (Open Systems Interconnection) Model is a set of rules that explains how different computer systems communicate over a network.

Application layer --> Implemented in software  (Http),  Provides services that applications (like browsers, email clients) use to communicate over the network.
		|
Presentation layer --> Ensures data is in a format both sender and receiver understand, encoding, encryption, data compression (SSL/TLS)
		|
Session layer --> Before sending data it will do like some kind session authentication, authorization, session estblish.
		|
Transport layer --> Data will be divided into segements. (TCP, UDP). It adds source and destination port.
		|
Network layer --> It adds Ip address of sender and reciever to every segment and forms an IP Data packet.This is where router lives. It performs routing transport one Data packet from source to destination. (IP)
		|
Data link layer --> Physical addressing done at this layer.Mac addresses are addded to the data packet to form a frame.
Frame is a data unit of the data link layer.Mac adress is the 12 digit alpha numeric number assigned to network interface of the computer.
		|
Physical layer --> From data link layer the data is in 0s and 1s. if we use opticle fiber it will convert data to electrical signal, if you use WIfi its radio signal.

--> In physical layer our physical router sends the data to the destination physical router.


Protocols:

--> Web protocols:

	--> TCP/IP:
			HTTP : How data is transfered.
			DHCP : Dynamically allocates IPs to devices.
			FTP : How files can be transfered.
			SMTP: How emials work (Used to send emails)
			POP3 & IMAP : Used to recieve emails.
			SSH: Secure shell
			
	--> Telnet:(Port-23)
			It enables users to manage device remotely
	--> UDP:
		 Stateless connection.(Data may be lost in this protocol)
	

--> HTTP: It tells us how a client request data to a server and it also tells us how server will send back data to the client.

--> HTTP application layer protocol

--> HTTP uses TCP protocol internally.

--> HTTP is stateless protocol

-->  Status codes:

	100 range --> Informational purpose
	
	200 range --> Successful
	
	300 range --> Redirecting purpose
	
	400 range --> Client side errors
		
    500 range --> Server side errors
	
	
--> Data transfer from one computer to another computer is done by network layer.

--> From network layer to application layer data transfer with in the same network is done by transport layer.

--> UDP --> connetionless protocol

--> TCP --> Once the connection is established, then data transfer happens.

--> We will use checksums to check whether data is corrupted or not at both the source and destination ends.

--> Both TCP and UDP use checksums

--> In UDP at destination ends it use cheksums if data is corrupted it won't do anything.

--> Why UDP is faster than TCP.

--> In TCP When you send the data to destination, then from the destination you will get acknowledgement repsonse to sender to tell that packet has successfully reached the destination.

--> In UDP you won't get that.

--> TCP connection oriented.

--> 3 Way handshake:

	1. Client send a request to server with SYN flag and seq no to server.
	
	2. Server takes that seq no and do some calculations and sends a acknowledgement with SYN flag and ACK no and seq no.
	
	3. Client acknowledges the connection and sends acknowledgement with ACK no and seq no
	
--> Then connection established and communication happens.

--> 192.168.2.30 --> 192.168.2 --> Network Address     .30 --> Device Address

--> IPv4 --> 32 bits

--> IPv6 --> 128 bits

--> Class of IP address:

	A : 0.0.0.0 --> 127.255.255.255
	
	B : 128.0.0.0 --> 191.255.255.255
	
	C : 192.0.0.0 --> 223.255.255.255
	
	D : 224.0.0.0 --> 239.255.255.255
	
	E : 240.0.0.0 --> 255.255.255.255
	
--> NAT  : -- NAT is a process of translating the source and destination IP (Internet Protocol) addresses and ports (ie.,) changing one IP address into another IP address. NAT can be processed in routers or firewalls.

--> Subnets:

	Subnet Mask for
	
	 Class A  --> 255.0.0.0   --> Network bits --> 8  Host Bits: 24    Usable Hosts per Network: 16.7 million (2^24 - 2)
	 
	 Class B  --> 255.255.0.0  --> Network bits --> 16  Host Bits: 16  Usable Hosts per Network: 65,534 (2^16 - 2)
	 
	 Class C  --> 255.255.255.0 --> Network bits --> 24  Host Bits: 8  Usable Hosts per Network: 254 (2^8 - 2)



Application Layer (Layer 7)

You type https://myapp.example.com in your browser.

Browser builds an HTTP request:

GET / HTTP/1.1
Host: myapp.example.com


Protocol: HTTP/HTTPS.

At this stage, itâ€™s just human-readable data.

ðŸ‘‰ Interview line: â€œAt the Application Layer, protocols like HTTP/HTTPS define how applications (like browsers) communicate with servers.â€

ðŸ”¹ Presentation Layer (Layer 6)

Since itâ€™s HTTPS, data must be encrypted.

Browser and server (or ALB in AWS) perform TLS handshake.

Data (HTTP request) gets encrypted.

ðŸ‘‰ Interview line: â€œAt the Presentation Layer, TLS/SSL ensures encryption and secure transmission.â€

ðŸ”¹ Session Layer (Layer 5)

Manages the session between browser and server (or ALB).

Example: TLS handshake ensures session keys are exchanged and session established.

ðŸ‘‰ Interview line: â€œAt the Session Layer, a secure session is established so client and server can continue communication reliably.â€

ðŸ”¹ Transport Layer (Layer 4)

Data is split into segments because networks canâ€™t send unlimited-size messages.

Source Port: random (e.g., 51515).

Destination Port: 443.

Protocol: TCP ensures reliable delivery (acknowledgments, retransmission, sequencing).

ðŸ‘‰ Interview line: â€œAt the Transport Layer, TCP segments are created with source and destination ports. Port 443 tells the server itâ€™s HTTPS traffic.â€

ðŸ”¹ Network Layer (Layer 3)

Each segment is wrapped into an IP packet.

Source IP: your machineâ€™s public IP.

Destination IP: resolved by DNS (say, ALB IP in AWS).

Routers forward packets based on IP addresses until they reach the destination network.

ðŸ‘‰ Interview line: â€œAt the Network Layer, IP addresses are used for end-to-end delivery across the Internet.â€

ðŸ”¹ Data Link Layer (Layer 2)

Each IP packet is placed into a frame.

Adds MAC addresses (physical addresses).

Source MAC: your NICâ€™s address.

Destination MAC: your routerâ€™s MAC.

Each hop updates MACs to forward packets closer to destination.

ðŸ‘‰ Interview line: â€œAt the Data Link Layer, frames with MAC addresses allow delivery between devices in the same local network segment.â€

ðŸ”¹ Physical Layer (Layer 1)

Frames are converted into electrical signals, light pulses, or radio waves depending on medium (Ethernet, Fiber, Wi-Fi).

These signals carry your request across routers, ISPs, and AWS backbone until it reaches the server.

ðŸ‘‰ Interview line: â€œAt the Physical Layer, bits are transmitted as actual signals across cables or wireless media.â€



GIT COMMANDS:




Basic GIT Commands:





To check git version:

git --version

To intialize git:

git init

To add files from working area to staging area:

git add filename --> 1 file

git add filename1 filename2 --> 2 files

git add * --> All files

To commit files from staging area to local repo:

git commit -m "comments" filename -- > 1 file

git commit -m "comments" filename1 filename2 --> 2 files

git commit -m "comments" --> All files

To check remote repository:

git remote -v

To add remote repository url:

git remote add origin url

To push to remote repository:

git push -u origin branchname

To configure username and mail while commiting to github for the first time

git config --global user.email "rushikeshn@xyz.com"
git config --global user.name "Rushi"

To Unconfigure username and mail:

git config --global --unset user.name
git config --global --unset user.email


To check logs:

git log
git log --oneline

Logs based on author:

git log --author=rushikeshn@xyz.com
git log --author=Rushi

Logs based on time period:

git log --since=2018-01-01 --author=rushikeshn@xyz.com --oneline
git log --until=2020-01-01
git log --since=2018-01-01 --until=2020-01-01 --author=rushikeshn@xyz.com --oneline




BRANCH:

If you are doing it for the first time

git clone https://SureshZenius@github.com/zeniusit/SFDC_Avaya_SoftPhone.git


To create a branch:

git branch branchname

To push branch to central repo:

git push origin branchname

To check how many branches are available in local repo:

git branch

To check how many branches are available in central repo:

git branch -r

To download repo:

git clone url

To download specific branch:

git clone --branch branchname url

To delete branch from central repo:

git push -d origin branchname

To delete in local repo:

git branch -d branchname



To downolad single folder from github repo:



# Step 1: Clone the repo, but avoid checking out files immediately

git clone --filter=blob:none --no-checkout https://github.com/USER/REPO.git

# Move into the repo directory

cd REPO

# Step 2: Enable sparse-checkout in "cone" mode (simplifies folder matching)

git sparse-checkout init --cone

# Step 3: Specify the folder you want

git sparse-checkout set path/to/folder

# Step 4: Checkout the branch to actually download that folder

git checkout main  # or replace with the correct branch if different




Pull the Remote Branch with --allow-unrelated-histories: Use the --allow-unrelated-histories flag to allow merging the two separate histories:

git pull https://github.com/zeniusit/SFDC_Avaya_SoftPhone.git main --allow-unrelated-histories


To clone seperate branch

git clone --branch V1 --single-branch <your-github-repo-url>




STASH:





To save files from staging area to stash memory (Temporary memory):

git stash push -m "Comments"

To list files in stash memory:

git stash list

To copy file from stash memory to staging area:

git stash apply stash@{filename}

To Cut file from stash memory to staging area:

git stash pop stash@{filename}

To delete a file which is in stash memory:

git stash drop stash@{filename}

To delete all file which is in stash memory:

git stash drop







TAGS:



To create a tag:

git tag tagname

To create a tag for specific commitID:

git tag tagname cid 

To push all tags to central repo:

git push origin --tags

To delete a tag :

git tag -d tagname1 tagname2

To delete all local tags:

git tag -d $(git tag -l)

To delete all Central repo tags:

git push origin --delete $(git tag -l)

To rename existing tag:

  git tag new old
  git tag -d old






ALIAS:

To configure alias commands:

git config --global alias.l "log --oneline"






RETRIEVE:



-->	--soft moves HEAD to the given commit, but doesn't touch staging or working directory.It does not pull a specific file to staging.This is typically used to undo a commit while keeping all changes staged

git reset --soft commitID

--> If you want to retrieve a file from a previous commit to staging:

git restore --staged <filename>


--> To retrieve a file from staging area to working area:

git reset head filename

--> To retrieve all files from staging area to working area:

git reset 

--> If you want to bring a file from a previous commit to the working directory:

git restore <filename>







CLONE, PULL & FETCH



To download repository:

git clone url

Already repository downloaded and you want to download updated file:

git pull

Already repository downloaded and you want to download updated file not to the local:

git fetch

	All fetched data goes to your .git folder:

	Commits: .git/objects/

	Remote branches: .git/refs/remotes/origin/<branch>

	Packfiles: .git/objects/pack/ (for efficient storage)
	
	
Inspect Fetched State Without Merging

git diff HEAD origin/main

Then if you want to download into local:

git merge






MAVEN COMMANDS:


Maven install pre-requisites java


To check maven version:

mvn --version


To install maven in linux server:

sudo apt install maven

To remove maven:

sudo apt-get remove maven

To generate maven project

mvn archetype:generate


To set environment varibles for maven in linux:

export MAVEN_HOME=/usr/share/maven/bin


To check already downloaded maven files storage location ( it will store in .m2 repository)

sudo find / -name .m2


Build lifecycle:

1. mvn validate
2. mvn compile
3. mvn test
4. mvn package
5. mvn verify
6. mvn install ( you can directly give mvn install it will check for all above commands)
7. mvn deploy 





JAVA LINUX COMMANDS:


To install specific version of java on linux server:

sudo apt install openjdk-11-jdk

To set environment varibles for java in linux:

export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64


To check environment variables:

env


To start & stop java service we need to go to respective pom.xml directory

nohup java -jar target/jar-name.jar &


To check whether java service running or not:

sudo ss -lptn 'sport = :port-number' | tail -n1



To up java services using script:


#!/bin/bash

timestamp=`date +"%Y-%m-%d %H:%M:%S"`
STATUS=$(curl -s -o /dev/null -w "%{http_code}" -I http://localhost:port-number/getAlltemplateNames)

if [[ ${STATUS} -eq 200 ]]
then
echo â€œ $timestamp Everything is Fineâ€ >> /home/service/reporting_log.txt
else
cd /path to pom.xml directory
nohup java -jar target/jar-name.jar &
echo " $timestamp Restarted successfully" >> /home/service/reporting_log.txt
echo " Services" |mutt -s "Buzz Reporting Restarted $timestamp " -- sandeep@acure.com 
fi





Tomcat:


Tomcat install pre-requisites java

To install tomcat on linux server:

sudo wget url

Tomcat folders:

bin           --> To start and stop the services
sh startup.sh --> Used to start the script

To start the tomcat you need to give persmissions to catalina.sh and startup.sh
Then create a folder logs/catalina.out in apache-tomcat-9.0.84-src# path


Now adding credentials for weblogin to tomcat, delete existing data from the tomcat-users.xml file and add the below lines:

<tomcat-users> 
<role rolename="manager-gui"/>
<user username="admin" password="Admin" roles="manager-gui, manager-script, manager-admin, manager-status"/>
</tomcat-users>

To change tomcat port number:

tomcat/conf/server.xml






Jenkins:


To install jenkins follow the steps in below URL:

https://www.digitalocean.com/community/tutorials/how-to-install-jenkins-on-ubuntu-20-04


https://pkg.jenkins.io/debian-stable/


To start jenkins:

sudo systemctl start jenkins


If jenkins failed to start check the log by below command:

journalctl -xeu jenkins.service

Check if port 8080 is already in use or not :

sudo lsof -i :8080

Result for above command:

COMMAND PID USER   FD   TYPE DEVICE SIZE/OFF NODE NAME
java    962 root   43u  IPv6  19490      0t0  TCP *:http-alt (LISTEN)


If port is under use kill the application by PID:

sudo kill -9 PID

Then check the status of jenkins:

sudo systemctl status jenkins

To restart jenkins from UI:

IP:8080/restart

To delete all build history in jenkins:

def jobName = "Job-name"  
def job = Jenkins.instance.getItem(jobName)  
job.getBuilds().each { it.delete() }  
job.nextBuildNumber = 1   
job.save()


Number of executors for master -- 2 (you can increase it in system configuration)
Number of executors for slave -- 1

All the jobs executed in jenkins stored in following path:

/var/lib/jenkins/workspace

Most used jenkins plugins:

1.Discard old build plugins
2.View plugins
3.Nested view plugins
4.Poll SCM
5.Build after other projects are built
6.Build periodically
7.GitHub hook trigger for GITScm polling
8.Trigger builds remotely (e.g., from scripts)
9.Delete workspace before build starts
10.Notification plugin
11.Backup plugins
12.Add timestamps to the Console Output
13.Terminate a build if it's stuck
14.Restrict where this project can be run
15.Build Executor Status
16.Global tool configuration
17.Invoke top-level Maven targets
18.RENAME
19.DELETE JOB
20.Description job
21.Oracle Java SE Development Kit Installer (For multiple java version builds)
22.s3 plugin (simple storage service)
23.Sonarqube plugin
24.aws 
25.docker
26.Periodic backup
27.Thin backup
28.Backup next build number file
29.Restart (service jenkins restart) 




DOCKER:




To install docker 2 methods

1.sudo apt install docker.io

2.Script Method ( Officially maintaining by docker)

  https://get.docker.com/
  
   1. download the script

   $ curl -fsSL https://get.docker.com -o install-docker.sh

   2. verify the script's content

   $ cat install-docker.sh
   
   3. Give permissions to file.
   
   $ sudo chmod +x install-docker.sh
   
   4. Give permissions to /var/run/docker.sock
   
   $ sudo chmod 666 /var/run/docker.sock

   3. run the script either as root, or using sudo to perform the installation.

   $ sudo sh install-docker.sh    or  
   $ sudo source install-docker.sh
   
   
To download ubuntu image:

docker pull ubuntu

To see the downloaded images:

docker images

To run docker image and to go into container:

docker run -it imagename
               imageID
			   
   it - interactive terminal

To run a container in the background:

docker run -itd imagename

   itd - interactive terminal in detached mode.
			   
To enter into running container:

docker attach containerID
			   
To exit from container and stopping the container:

 exit
 
To check the running and stopped containers:

 docker ps -a
 
To check only the containerID

 docker ps -aq
 
To change the container name:

 docker rename old_name New_name
 
To start a container:

 docker start containerID
 
To stop a container:

 docker stop containerID
 
To stop all the containers:
 
 docker stop $(docker ps -aq)
 
To restart a container:

 docker restart containerID
 
To exit from container without stopping the container:
 
 Ctrl pq
 
To delete or remove a container:

 docker rm containerID
 
 docker rm containerID -f ( to remove a running container forcefully)
 
To remove all the docker containers

 docker rm $(docker ps -aq)
  
To remove a image:

 docker rmi imageID
 docker rmi imageID1 imageID2 imageID3
 
To remove all the docker images:

 docker rmi $(docker images -q)
 

To add present user to docker user:

 sudo usermod -aG docker $USER 

To assign name and port of the container at the time of creation.

 docker run -itd -p 80:80 --name containername imageID
 

To save a docker image to a file.

 docker save -o HelloDocker.tar 74cc54e27dc4

Saves the Docker image with ID 74cc54e27dc4

Outputs it into a file called HelloDocker.tar
 
 
To build docker image from docker file 

 docker build -t imagename path of Docker file
 
 
To check only exited containers:

 docker ps -f "status=exited"
 
To backup the container:

 docker commit containerID backupimagename
 
To change the dockerimage name

 docker tag old_name new_name
 
To enter running container and if you type exit container will run in the backend:

 docker exec -it cid bash

To avoid timezone question while creating dockerimage add below one in dockerfile.

 ARG DEBIAN_FRONTEND=noninteractive

 
To check the network list:

 docker network ls
 
To initiate a cluster execute below command in master:

 docker swarm init
 
To check the joined nodes list
 
 docker node ls
 
To create 5 services in docker:

 docker service create --replicas 5 -p 80:80 --name servicename imagename
 
To list replicted services in nodes wise:

 docker service ps servicename
 
To scale the containers to 10:

 docker service scale servicename=10
 
To update devopsbyrushi/myapplication:v2 application in myappv1 service:

 dokcer service update --image devopsbyrushi/myapplication:v2 myappv1
 
 dokcer service update --image applicationname servicename
 

To remove service:

 docker service rm servicename
 
To remove the node from cluster( go to that particular node and enter below command):

 docker swarm leave
 
To delete node:

 docker node rm nodename




Docker compose:



Follow the steps to install docker compose:

The following command will download the 1.29.2 release and save the executable file at /usr/local/bin/docker-compose, which will make this software globally accessible as docker-compose

1.sudo curl -L "https://github.com/docker/compose/releases/download/1.29.2/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose


Next, set the correct permissions so that the docker-compose command is executable:

2. sudo chmod +x /usr/local/bin/docker-compose

To verify that the installation was successful, you can run:

3. docker-compose --version


To create a compose file (Name of the file should be docker-compose):

 sudo vi docker-compose.yml
 
To run the file:

 sudo docker-compose up -d 
 
 
 
 
 
 
 DATABASE:
 
 
 
 
 
 
 RDS default username --> admin
 
 MYSQL default username--> root
 
 //Master username --> admin
 //Master password --> EkbPfTOCGClrLqDWKgOm

 
 
 
To install mysql database on linux server follow the URL:

https://www.fosstechnix.com/how-to-install-mysql-5-7-on-ubuntu-20-04-lts/


While downloading if you encounter below issue:


W: GPG error: http://repo.mysql.com/apt/ubuntu bionic InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY B7B3B788A8D3785C
 
 
Add below one to resolve the issue:

sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys B7B3B788A8D3785C   


 
To enter into your database:

 mysql -u username -p
 
To change the password for user:

	ALTER USER 'username'@'localhost' IDENTIFIED BY 'New_PASSWORD'
 
To create a database:

 CREATE DATABASE DATABASE_NAME;
 
To check databases:

 show databases;
 
To check tables:

 show tables;
 
To delete database:

 DROP DATABASE DATABASE_NAME;
 
To enter into DB:

 USE DATABASE_NAME;
 
Taking mysql backup:

mysqldump -u root -p database_name > /path/database_name.sql
 
Taking particular table backup in mysql database:

 mysqldump -u root -p database_name table_name > /path/table_name.sql
 
 
 To restore database:

 mysql -u root -p database_name < /path/database_name.sql

To create a user on mysql:

 CREATE USER 'username'@'localhost' IDENTIFIED BY 'password';
 GRANT ALL PRIVILEGES ON * . * TO 'username'@'localhost';
 FLUSH PRIVILEGES;
 
 
To check the grants gor a user:

	SHOW GRANTS FOR 'username'@'localhost';	
 
Taking mysql backup thruogh cron expressions:


1. change to the root directory:

   cd 
   


2. Create a file

   sudo vi file-name.sh
  
3. Add the below content:

YEAR=`date +%Y`
MONTH=`date +%m`
DAY=`date +%d`
HOUR=`date +%H`
sudo mkdir -p $YEAR/$MONTH/$DAY/$HOUR

mysqldump -uroot -proot schemaname > $YEAR/$MONTH/$DAY/$HOUR/schema.sql



5. Give executable permissions:

   sudo chmod +x file-name.sh 
   
6. type below command:

   crontab -e
   
7. Add the cron job:

   0 2 * * 1-5 /path to/file-name.sh
   
8. To run manually :

   sh file-name.sh

 
 
 
 ANSIBLE:
 
 
 
 
Ansible --> Configuration management tool.
 
 
To install ansible :

 https://www.digitalocean.com/community/tutorials/how-to-install-and-configure-ansible-on-ubuntu-20-04
 
 


MASTER TO CLIENT 1 PASSWORDLESS AUTHENTICATION:


STEP1: 


MASTER:

Create a super user:

sudo adduser master

Set the password authentication to yes:

sudo vi /etc/ssh/sshd_config

Restart the services:

sudo service ssh restart

Add the user below root user:

sudo visudo

Add below one:

master ALL=(ALL:ALL) NOPASSWD: ALL



CLIENT1:

Create a super user:

sudo adduser client

Set the password authentication to yes:

sudo vi /etc/ssh/sshd_config

Restart the services:

sudo service ssh restart

Add the user below root user:

sudo visudo

Add below one:

client ALL=(ALL:ALL) NOPASSWD: ALL



STEP2:

MASTER:

Switch to super user you have created:

su amaster

Type below command:

cd 

Type below command to generate a key:

ssh-keygen


CLIENT1:

Switch to super user you have created:

su master

Type below command:

cd 

Create a directory .ssh:

sudo mkdir .ssh

Enter into that directory:

cd .ssh/

Type the below command and paste the public key copied from master and save the file.

sudo vi authorized_keys



STEP3:

MASTER:

Setup the inventory file:

sudo vi /etc/ansible/hosts

Add the client server:

[client] --> group name

IPv4 ansible_user=username
               
            



STEP4:

Stop and start the servers then you will be able to connect from master to client without password.


STEP5:

Test the connection by entering below command in master:

ansible client -m ping -o
          ||
        group name



To check destination server opt activities from master:

ansible client -a "ls -l /opt"
           

To create a file in a specified path

ansible test -m file -a "path=/opt/file/test1.txt state=touch" --become


To delete a file in a specified path

ansible test -m file -a "path=/opt/file1.txt state=absent" --become


To create a directory in a specified path

ansible test -m file -a "path=/opt/file state=directory" --become


To delete a directory in a specified path

ansible test -m file -a "path=/opt/file state=absent" --become


To create a file with content

ansible test -m copy -a "content='this is a test line' dest=/opt/file2.txt" --become
		
		
To create a file in a client group:

ansible client -a "sudo touch filename"


To copy from master to all client group servers:

ansible groupname -m copy -a "src=/path dest=/path" -b


To install applications in destination servers: (if any errors first update destination server)

ansible groupname -m apt -a "name=maven state=present" 



To un-install applications in destination servers:

ansible groupname -m apt -a "name=maven state=absent" 


Service related commands:


To stop apache server:

ansible groupname -m service -a "name=apache2 state=stopped" -b


To start apache server:

ansible groupname -m service -a "name=apache2 state=started" -b


To check the status of apache2

ansible test -a "systemctl status apache2" -b





Ansible playbook: (To run multi commands):

1. The file extension should be .yaml or .yml

 Example:
 
 maven.yaml
 
---
- hosts: groupname1 groupname2 groupname3
  become: true
  tasks:
    - name: install apache2
	  apt: name=apache2 state=present
	  
    - name: copying files to destination servers
	  copy: "src=/path dest=/path"
	  
	- name: creating users
	  user: name=username

2. Run the playbook with below command:

  ansible-playbook maven.yaml
                      ||
				  playbook name  
				  
				  
3. Ansible-playbooks lab:

  https://rushiinfotech.in/real-time-scenerios-ansible-playbooks-with-labs/
  
  
4. Ansible vault lab:

  https://rushiinfotech.in/what-is-ansible-vault/
  
  
To check ansible vault version
 
 ansible-vault --version
 
 
To create ansible vault file:
 
 sudo ansible-vault create file.yml
 
To encrypt existing text file:

 ansible-vault encrypt file.yml
 
To decrypt existing text file:

 ansible-vault decrypt file.yml
 
To change the current vault password of a file:

 ansible-vault rekey file.yml
 
To edit ansible vault file:
 
 ansible-vault edit file.yml
 
To use an encrypted file in a playbook, you can execute ansible-playbook command with the --ask-vault-pass flag:

Before this you have to give full permissions to that file:
 
 sudo chmod 777 file.yml

 ansible-playbook file.yml --ask-vault-pass
 
 
 
 
AMAZON S3:




To upload data into aws from aws linux server:

1.install awscli in linux:

 sudo apt-get install awscli
 
2.If you want to connect to aws from linux you must have access key and secret key.

3. After you got access key and secret key type below command in linux server.
  
  aws configure
  
3. configure aws access key, secret key, region name and output format.

4. To list the buckets in s3:
 
   aws s3 ls
   
5. To create a bucket in s3:

  aws s3 mb s3://bucket-name
  
6. To upload file from linux to s3 bucket:

  aws s3 cp file-name s3://bucket-name
  
7. To upload a directory in s3 :

  aws s3 cp --recursive directory-name s3://bucket-name
  
8. To check bucket total stored data :

  aws s3 ls s3://bucket-name --recursive --human-readable --summarize
  
9. To download files from s3 to linux server:

  aws s3 cp s3://bucket-name/file-name /path
  
10.To download directories from s3 to linux server:

  aws s3 cp s3://bucket-name/directory-name /path --recursive
  
10.To delete a bucket :

 aws s3 rb s3://bucket-name --force
 





EBS VOLUMES:






To list internal storage:

 df -h
 
 
To attach EBS volume to instance:
 
 
1. To list external storage (Attached EBS volume):

   lsblk
 
2. To check if the attached volume has any data:

   sudo file -s /dev/xvdf
 
   if the output is "/dev/xvdf: data", means your volume is empty.

3. Format the volume to ext4 filesystem:

   sudo mkfs -t ext4 /dev/xvdf
 
4. Create a directory of your choice to mount our new ex4 volume:

   sudo mkdir /directory-name
   
5. Mount the volume to above created directory:

   sudo mount /dev/xvdf /directory-name
   
6. When you reboot instance the attached EBS volume removed.



To Automount EBS volume on reboot:


1. Back up the /etc/fstab file.

  sudo cp /etc/fstab /etc/fstab.bak
  
2. Open /etc/fstab file and add below line:

  device_name  mount_point  file_system_type  fs_mntops     fs_freq    fs_passno 
  
  /dev/xvdf    /directory-name    ext4      defaults,nofail    0           0
  
3. Execute below command for any errors, If there is no errors means fstab entry is valid.

   sudo mount -a
   
 Now, on every reboot the extra ebs volumes get mounted automatically.
 
 
 
 
 
 
 KUBERNETES:
 
 
 
 There are 3 methods of creating cluster:
 
 1. Minikube --> single node    --> use for dev,test and practice purpose
 
 2. Kubeadm method --> 1 master and 1 node  --> Everything we have to maintain.
 
 3. EKS  --> Managed by AWS.
 
 
 
 KUBERNETES Components:
 
 
 API server : 
 
  --> To create, delete, and update and scale pods.
  
  --> It generally validates requests received and then forwards them to other processes.
      No request can be directly passed to the cluster, it has to be passed through the API Server.
 
 Scheduler :   
  
  --> When API Server receives a request for Scheduling Pods then the request is passed on to the Scheduler. 
      It intelligently decides on which node to schedule the pod for better efficiency of the cluster.
	  
 Control-Manager:
 
 --> Controllers include the replication controller, which ensures that the desired number of replicas of a given application is running, and the node controller, which ensures that nodes are correctly marked as â€œreadyâ€ or â€œnot readyâ€ based on their current state.
 
 etcd:
 
 --> It is a key-value store of a Cluster. The Cluster State Changes get stored in the etcd. It acts as the Cluster brain because it tells the Scheduler and other processes about which resources are available and about cluster state changes.
 
 kubelet:
 
 --> It can register the node with the apiserver.
 
 --> The Kubelet is responsible for managing the deployment of pods to Kubernetes nodes.
 
 --> It makes sure that containers are running in a Pod.

 kube-proxy:
 
 --> Responsible for forwarding the request from Services to the pods. It has intelligent logic to forward the request to the right pod in the worker node.
 
 
 
 
 
 MINIKUBE:
 
 
 
 How to install kubernetes cluster with minikube on ubuntu:
 
 1. Update system packages:
 
    sudo apt-get update
	
 2. Install docker:
    
	sudo apt install -y docker.io
	
	To check docker version
	
	docker --version
	
 3. start docker service:
   
    sudo systemctl start docker
	
	Give permissions to /var/run/docker.sock
   
    sudo chmod 666 /var/run/docker.sock
	
 4. Enable docker service:
   
    sudo systemctl enable docker
	
 5. To cehck docker service status:
  
    sudo systemctl status docker
	
 6. Add the current user to the docker group:
 
    sudo usermod -aG docker $USER
	
	sudo newgrp docker
	
 7. Install kubectl:
   
    sudo curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
	
 8. Give executable permissions:
 
    sudo chmod +x kubectl
	
 9. TO run kubectl file we need to move it to another location.
  
    sudo mv kubectl /usr/local/bin/
	
 10. Install minikube:
  
    wget https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64
	
 11. Give executable permissions:
 
    chmod +x minikube-linux-amd64
	
 12. TO run minikube file we need to move it to another location.
  
    sudo mv minikube-linux-amd64 /usr/local/bin/minikube
	
 13. Start minikube with docker driver
 
    minikube start --driver=docker --force
	
	We have created kubernetes cluster.
	
 To check minikube status:
  
  minikube status
  
 To check cluster-info:
 
  kubectl cluster-info
 
 To create a pod from image in kubernetes cluster:
 
 kubectl run pod-name --image=image-name
 
 To check pods
  
 kubectl get pods
 
 To check pods
  
 kubectl get nodes
 
 
 
 
 
 
 EKS:
 
 
 
 
 
How to install kubernetes cluster with EKS on ubuntu:
 

1. Install awscli:

  sudo apt-get update
  
  sudo apt-get install awscli -y
  
  
2. Create IAM user with admin permissions and configure in your linux server:

  aws configure
  
  AWS Access Key ID [None]: AKIA2QE54JTSQ7RR
  AWS Secret Access Key [None]: /PvMVbAWCMECEXSe9Ev2aCF7BsODjH
  Default region name [None]: us-east-2
  Default output format [None]:


3. Installing kubectl:


https://docs.aws.amazon.com/eks/latest/userguide/install-kubectl.html

or

https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/



4. Install eksctl:


https://linux.how2shout.com/how-to-install-eksctl-cli-tool-on-ubuntu-linux/


5. Create Cluster using below commands:

ubuntu@Rushi:~$ eksctl create cluster --name=Test \
                                --region=us-east-1 \
                               --zones=us-east-1a,us-east-1b \
                              --without-nodegroup


6. To get cluster Info

   eksctl get cluster
   
   

7. Create & Associate IAM OIDC Provider for our EKS Cluster:

# Replace with region & cluster name:

eksctl utils associate-iam-oidc-provider \
                            --region us-east-1 \
                           --cluster Test \
                           --approve



8. Create Node Group with additional Add-Ons in Public Subnets:

# Create Public Node Group:

eksctl create nodegroup --cluster=Test \
                               --region=us-east-1 \
                                --name=Test-cluster
                               --node-type=t3.medium \
                             --nodes=2 \
                             --nodes-min=2 \
                             --nodes-max=4 \
                            --node-volume-size=20 \
                            --ssh-access \
                            --ssh-public-key=key-pair \	
                           --managed \
                           --asg-access \
                           --external-dns-access \
                          --full-ecr-access \
                          --appmesh-access \
                          --alb-ingress-access



9.  Once executed succesfully verify with below commands:

    kubectl get nodes  
  
10. If you get no kind "ExecCredential is registered version" error run below commands:

    curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
	
	unzip awscliv2.zip
	
	sudo ./aws/install

11. verify with below commands:

    kubectl get nodes 
	
12. Install docker:

    sudo apt install -y docker.io
	
13. To create pod run the image:

    kubectl run pod-name --image=image-name
	
14. To check the pods:
 
    kubectl get pods
	
15. To get cluster:

    # List EKS clusters:
	
     eksctl get cluster

16. List NodeGroups in a cluster

   eksctl get nodegroup --cluster=cluster-Name

17. Delete Node Group:

eksctl delete nodegroup --cluster=cluster-Name --name=nodegroup-Name

eksctl delete nodegroup --cluster=test1 --name=test1-ng-public1


18. Delete Cluster:

eksctl delete cluster cluster-Name






  KUBEADM:
  
  
   How to install kubernetes cluster with kubeadm on ubuntu:
   
   
 1. Change the hostnames as master and nodes for identification purpose:
 
    sudo vi /etc/hostname
	
 2. Execute below command in master and node as well:
 
    sudo apt-get update
	
 3. Switch to root user in both:
 
    sudo su
	
 4. Install docker in both:
 
    curl -fsSL https://get.docker.com -o install-docker.sh
	
 5. Run the docker in both:
 
    sudo sh install-docker.sh
	
 6. Add the Docker Daemon configurations to use systemd as the cgroup driver in both:
 
 
    cat <<EOF | sudo tee /etc/docker/daemon.json
	{
	  "exec-opts": ["native.cgroupdriver=systemd"],
	  "log-driver": "json-file",
	  "log-opts": {
	  "max-size": "100m"
	  },
	  "storage-driver": "overlay2"
	}
	EOF
	
  7. exit from the root user in both:
  
     exit
	 
  8. Check the docker images in both:
  
     docker images
	 
  9. If you get any permission denied error solve by below commands in both:
  
     sudo usermod -aG docker $USER
	 
	 sudo chmod 666 /var/run/docker.sock
	 
  10.Add kubernetes repository in both:
  
     curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
	 
	 
	 echo "deb https://apt.kubernetes.io/ kubernetes-xenial main" | sudo tee /etc/apt/sources.list.d/kubernetes.list
	 
	 
  11. Update the server in both:
  
     sudo apt-get update
	 
  12. Install kubeadm,kubelet, and kubectl in both:
  
      sudo apt install -y kubelet kubeadm kubectl
	  
  13. To stop packages from auto-upgrade in both:
  
      sudo apt-mark hold kubelet kubeadm kubectl
	  
	  
	  
	  Setting up the master node:
	  
	  
  14. Initialize master node in master only:
  
      sudo kubeadm init
	  
  15. You will get error, to remove error you need to remove config.toml file in both:
      
	  sudo rm /etc/containerd/config.toml
	  
  16. Restart the containerd in both:
  
      sudo systemctl restart containerd
	  
  17. Initialize master node in master only:
  
      sudo kubeadm init
	  
  18. After the initialization completes, setup the local kubeconfig:
  
      mkdir -p $HOME/.kube
	  
	  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
	  
	  sudo chown $(id -u):$(id -g) $HOME/.kube/config
	  
  18. Go to edit inbound rules and open required ports:
  
  19. You will get a token copy that token and paste it in node server:
  
      sudo TOKEN
	  
  20. Execute below command in master:
  
      kubectl get nodes
	  
  21. We will use the calico as pod network in master:
  
      curl https://raw.githubusercontent.com/projectcalico/calico/v3.26.1/manifests/calico.yaml -O
	  
  22. To apply the calico file in master:
  
      kubectl apply -f calico.yaml
	  
  23. pull the nginx image:
  
     docker pull nginx
	 
  24. check the images:
  
     docker images
	 
  25. To create a pod:
  
    kubectl run pod-name --image=image-name
	
  26. To check the pods:
  
    kubectl get pods
	
	
	
	
	
	MANIFEST FILE:
	



 
To stop pods:

 kubectl stop pod pod-name
 
To check where the pod deployed:

 kubectl get pods -o wide
 
 
 

    POD MANIFEST FILE:


To create a manifest file:

 sudo nano file-name.yml
 
 
 
apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels:
    app: nginx-app
spec:
  containers:
    - name: web
      image: devopsbyrushi/myapplication:v1
      ports:
        - name: nginx
          containerPort: 80
          protocol: TCP

 
To create a pod for the first time from manifest file:

 kubectl create -f file-name.yml
 
To apply changes to the existing pod from manifest file:

 kubectl apply -f file-name.yml
 
To delete pods:

 kubectl delete pod pod-name
 
 
 
 DEPLOY MANIFEST FILE:



To create a manifest file:

 sudo nano file-name.yml
 
 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: live-prod
  labels:
    app: nginx-app
spec:
  replicas: 5
  selector:
    matchLabels:
      app: nginx-app
  template:
    metadata:
      labels:
        app: nginx-app
    spec:
      containers:
      - name: live-prod
        image: devopsbyrushi/myapplication:v1
        ports:
        - containerPort: 80

 
To create a pod for the first time from manifest file:

 kubectl create -f file-name.yml
 
 
To check deploy pods:

 kubectl get deploy
 
To delete deploy pods:

 kubectl delete deploy pod-name
 
 
 
 NODEPORT MANIFEST FILE: 
 
 (To access the deployed pods we need to create node port services)
 
 
 
 To create a manifest file:

 sudo nano file-name.yml
 
 
apiVersion: v1
kind: Service	
metadata:
  name: my-service
  labels:
    app: nginx-app
spec:
  selector:
    app: nginx-app
  type: NodePort
  ports:
  - nodePort: 31111
    port: 80
    targetPort: 80


To apply changes to the pod from manifest file:

 kubectl apply -f file-name.yml
 
To check services:

 kubectl get svc
 
To delete node port services:

 kubectl delete svc svc-name
 
 
 
 
 LOADBALANCER MANIFEST FILE:
 
 
To create a manifest file:

 sudo nano file-name.yml
 
 
apiVersion: v1
kind: Service	
metadata:
  name: my-service
  labels:
    app: nginx-app
spec:
  selector:
    app: nginx-app
  type: LoadBalancer
  ports:
  - nodePort: 31000
    port: 80
    targetPort: 80

 
To check services:

 kubectl get svc
 
To delete loadbalancer services:

 kubectl delete svc svc-name
 
 
 
 
 To delete cluster:
 
 1. delete loadbalancer  --> kubectl delete svc svc-name
 
 2. delete cluster 
   
    To get cluster Info

     eksctl get cluster
	 
	Delete Cluster:

    eksctl delete cluster cluster-Name
	
	
	
	
To clear buffer and cache in server :


1. If you are doing it for particular user switch to that user.

2. If you are doing it general switch to Root user:

   sudo su
   
3. change to the root directory:

   cd 
   
4. Create a file that ends with .sh:

   sudo nano file-name.sh
   
5. Add below content:

    a=$(vmstat -s | awk  ' $0 ~ /total memory/ {total=$1 } $0 ~/free memory/ {free=$1} $0 ~/buffer memory/ {buffer=$1} $0 ~/cache/ {cache=$1} END{print (total-free-buffer-cache)/total*100}')
	echo ${a%.*}
	echo "Present cache and swap memory is  $a"
	if [ ${a%.*} -ge 85 ];then
	echo "clearing cache"
	echo 3 > /proc/sys/vm/drop_caches && swapoff -a && swapon -a && printf '\n%s\n' 'Ram-cache and Swap Cleared'
	else
	echo "No Need to clear cache right now"
	fi
	
6. Give executable permissions:

   sudo chmod +x file-name.sh
   
7. Take the path
  
   pwd
   
8. Type below command:

   crontab -e
   
9. Choose the editor from 1-4:

10.write below cron job:

   * * * * * /root/path to /file-name.sh
   



To check which path has occupied lot space:


1. change to home directory:

   cd /
   
2. To check file wise occupied storage:

   du -sh *  or  df -h
   
   



   
   
   
 TERRAFORM:  (Infrastructure Automation Tool)
 



Pre-requisites for installing terraform on windows:


1. Install Visual Studio code

        Install AWS CLI Extension
        Hashicorp Terraform Extension
		
2. Create an AWS IAM user with Admin Privileges:

3. Download Terraform on windows:

   After downoading the terraform, Unzip and Unzipped file copy to below path:
   
   C:\Windows\System32
   
	
4. Install AWS CLI on Windows

5. Open the visual studio code

6. Conect to aws through viusal studio code powershell:

   1. aws --version

   2. aws configure
  
	  AWS Access Key ID [None]: AKIA2QE54JTSQ7RR
	  AWS Secret Access Key [None]: /PvMVbAWCMECEXSe9Ev2aCF7BsODjH
	  Default region name [None]: us-east-2
	  Default output format [None]:
	  
   3. Write your terraform file with extension tf
   
   4. Type below commands:
   
      terraform init  -- for initialization of terraform
	  terraform plan  -- Dry run
	  terraform apply
	  terraform destroy
	  
	  
   
   Terraform configuration file for creating an EC2 instance:
   
   
# Configure the AWS provider
provider "aws" {
region = "us-east-1" # Change to your desired AWS region
}
# Define the EC2 instance

resource "aws_instance" "example" {
ami = "ami-0261755bbcb8c4a84" # Amazon Linux 2 AMI (us-east-1)
instance_type = "t2.micro" # Micro instance type
# Tags for the EC2 instance (optional)
tags = {
Name = "MyEC2Instance"
}
}





Creating Multiple EC2 Instances with Terraform:


# Configure the AWS provider
provider "aws" {
region = "us-east-1" # Change to your desired AWS region
}
# Define multiple EC2 instances
resource "aws_instance" "example" {
count = 3 # Number of instances to create
ami = "ami-0261755bbcb8c4a84" # Specify an AMI ID for your desired OS
instance_type = "t2.micro" # Specify the instance type
# Tags for the EC2 instances (optional)
tags = {
Name = "MyEC2Instance-${count.index + 1}"
}
}




Creating Multiple EC2 Instances with Varied Configurations in Terraform:


# Configure the AWS provider
provider "aws" {
region = "us-east-1" # Change to your desired AWS region
}
# Define the first EC2 instance
resource "aws_instance" "instance1" {
ami = "ami-0261755bbcb8c4a84" # Specify an AMI ID for the first instance
instance_type = "t2.micro" # Specify the instance type
tags = {
Name = "Instance1"
Environment = "Development"
}
}
# Define the second EC2 instance
resource "aws_instance" "instance2" {
ami = "ami-0261755bbcb8c4a84" # Specify an AMI ID for the second instance
instance_type = "t2.small" # Specify the instance type for the second instance
tags = {
Name = "Instance2"
Environment = "Production"
}
}



Destroying a Specific EC2 Instance with Terraform:

   terraform destroy -target='aws_instance.instance-name'





Creating a Basic Amazon RDS Instance with Terraform:


# Configure the AWS provider
provider "aws" {
region = "us-east-1" # Change to your desired AWS region
}
# Define the RDS instance
resource "aws_db_instance" "db" {
allocated_storage = 20
storage_type = "gp2"
engine = "mysql"
engine_version = "5.7"
instance_class = "db.t2.micro"
identifier = "mydatabase"
username = "admin"
password = "YourPasswordHere"

# Backup configuration
skip_final_snapshot = true # Optional: Specify whether to skip the final snapshot
backup_retention_period = 7
# Network configuration
parameter_group_name = "default.mysql5.7" # Specify the parameter group name
# Tags for the RDS instance (optional)
tags = {
Name = "MySimpleRDSInstance"
}
}




Creating Multiple GitHub Repositories with Terraform:


terraform {
required_providers {
github = {
source = "integrations/github"
version = "~> 5.0"
}
}
}
provider "github" {
token = "" # or `GITHUB_TOKEN`
}
resource "github_repository" "example1" {
name = "example1"
description = "My awesome codebase"
visibility = "public"
}
resource "github_repository" "example2" {
name = "example2"
description = "My awesome codebase"
visibility = "public"
}



Working with multiple security groups with Terraform:



provider "aws" {
region = "us-east-1" # Change to your desired AWS region
}

resource "aws_security_group" "allow_tls" {
  name        = "production-sg"
  description = "Allow TLS production-sg traffic"
  

  ingress {
    description      = "TLS from VPC"
    from_port        = 443
    to_port          = 443
    protocol         = "tcp"
    cidr_blocks      = ["116.74.153.13/32"]
    
  }

  ingress {
    description      = "TLS from VPC"
    from_port        = 22
    to_port          = 22
    protocol         = "tcp"
    cidr_blocks      = ["116.74.153.13/32"]
  
  }

  ingress {
    description      = "TLS from VPC"
    from_port        = 3306
    to_port          = 3306
    protocol         = "tcp"
    cidr_blocks      = ["116.74.153.13/32"]
  
  }

  egress {
    from_port        = 0
    to_port          = 0
    protocol         = "-1"
    cidr_blocks      = ["0.0.0.0/0"]
  
  }

  tags = {
    Name = "allow_tls"
  }
}





Auto Assign elastic ip to an instance:



provider "aws" {
region = "us-east-1" # Change to your desired AWS region
}

resource "aws_eip" "lb" {
  domain   = "vpc"
}

resource "aws_security_group" "allow_tls" {
  name        = "production-sg"
  description = "Allow TLS production-sg traffic"
  

  ingress {
    description      = "TLS from VPC"
    from_port        = 443
    to_port          = 443
    protocol         = "tcp"
    cidr_blocks      = ["${aws_eip.lb.public_ip}/32"]
    
  }
  egress {
    from_port        = 0
    to_port          = 0
    protocol         = "-1"
    cidr_blocks      = ["0.0.0.0/0"]
  
  }

  tags = {
    Name = "allow_tls"
  }
}



To see the output within the vs code write output function:


provider "aws" {
region = "us-east-1" # Change to your desired AWS region
}

resource "aws_eip" "lb" {
  domain   = "vpc"
}

resource "aws_eip" "ep2" {
  domain   = "vpc"
}


output "public-ip" {
    
    value = "https://${aws_eip.lb.public_ip}:8080"
}

output "public-ip2" {
    
    value = "https://${aws_eip.ep2.public_ip}:8081"
}



Use variables to change ip:


provider "aws" {
region = "us-east-1" # Change to your desired AWS region
}

resource "aws_security_group" "allow_tls" {
  name        = "allow_tls"
  description = "Allow TLS inbound traffic"

  ingress {
    description      = "TLS from VPC"
    from_port        = 443
    to_port          = 443
    protocol         = "tcp"
    cidr_blocks      = [var.cidr-ip]
    
  }

   ingress {
    description      = "TLS from VPC"
    from_port        = 22
    to_port          = 22
    protocol         = "tcp"
    cidr_blocks      = [var.cidr-ip]
    
  }


 ingress {
    description      = "TLS from VPC"
    from_port        = 3306
    to_port          = 3306
    protocol         = "tcp"
    cidr_blocks      = [var.cidr-ip]
    
  }


  egress {
    from_port        = 0
    to_port          = 0
    protocol         = "-1"
    cidr_blocks      = ["0.0.0.0/0"]
  
  }

  tags = {
    Name = "allow_tls"
  }
}



Write below one in another file:


variable "cidr-ip" {
    default = "22.22.33.44/32"
}









    NAGIOS:
	
	
Installation



https://tecadmin.net/install-nagios-monitoring-server-on-ubuntu/



If you get any error regarding unable to find libgd2-xpm-dev package, enter below command:


 sudo apt install libgd-dev
 
 
If you get any php error, add below command:


 sudo apt-get install php libapache2-mod-php php-mysql
 
 
 
To add destnation server (host) into NAGIOS server follow the below blog:


https://www.linuxhelp.com/how-to-add-host-into-nagios-server#!#google_vignette


In the NAGIOS server 


servers --> objects

host.cfg--> localhost.cfg


add the below configurations in localhost.cfg file in NAGIOS server:

# Ubuntu Host configuration file

define host {
        use                          linux-server
        host_name                    icici-prod1
        alias                        Ubuntu Host
        address                      34.229.139.51
        register                     1
}

define service {
      host_name                       icici-prod1
      service_description             PING
      check_command                   check_ping!100.0,20%!500.0,60%
      max_check_attempts              2
      check_interval                  2
      retry_interval                  2
      check_period                    24x7
      check_freshness                 1
      contact_groups                  admins
      notification_interval           2
      notification_period             24x7
      notifications_enabled           1
      register                        1
}

define service {
      host_name                       icici-prod1
      service_description             Check Users
      check_command           check_local_users!20!50
      max_check_attempts              2
      check_interval                  2
      retry_interval                  2
      check_period                    24x7
      check_freshness                 1
      contact_groups                  admins
      notification_interval           2
      notification_period             24x7
      notifications_enabled           1
      register                        1
}

define service {
      host_name                       icici-prod1
      service_description             Local Disk
      check_command                   check_local_disk!20%!10%!/
      max_check_attempts              2
      check_interval                  2
      retry_interval                  2
      check_period                    24x7
      check_freshness                 1
      contact_groups                  admins
      notification_interval           2
      notification_period             24x7
      notifications_enabled           1
      register                        1
}

define service {
      host_name                       icici-prod1
      service_description             Check SSH
      check_command                   check_ssh
      max_check_attempts              2
      check_interval                  2
      retry_interval                  2
      check_period                    24x7
      check_freshness                 1
      contact_groups                  admins
      notification_interval           2
      notification_period             24x7
      notifications_enabled           1
      register                        1
}

define service {
      host_name                       icici-prod1
      service_description             Total Process
      check_command                   check_local_procs!250!400!RSZDT
      max_check_attempts              2
      check_interval                  2
      retry_interval                  2
      check_period                    24x7
      check_freshness                 1
      contact_groups                  admins
      notification_interval           2
      notification_period             24x7
      notifications_enabled           1
      register                        1
}



Go to inbound security and open All ICMP-IPv4 




SONAR QUBE: (Code quality tool)



Pre-requisite to install sonar qube --> Java 17 or 18


Install Sonar qube with below blog:

https://www.fosstechnix.com/how-to-install-sonarqube-on-ubuntu-20-04/



Default username and password for sonarqube web login --> admin








#################################     ABHISHEK VEERAMALLA:   #################################################









What is DevOps:

DevOps is a process of improving application delivery by ensuring that there is a proper 
Automation
With application quality maintained
by ensuring continuous monitoring and continuous testing in place.


SDLC (Software development Life Cycle):

The Software Development Life Cycle (SDLC) is a systematic, structured process for developing software applications. It defines the steps involved in planning, creating, testing, deploying, and maintaining software to ensure that it meets quality standards, fulfills user requirements.


Pillars of SDLC:

Planning --> the projectâ€™s scope, goals, budget, timeline, and resource needs are analyzed and defined.
Defining --> documentation of functional and non-functional requirements.Software Requirements Specification document.
Design --> May involve high-level design (HLD) and low-level design (LLD).
Building --> coding and development of the software
Testing --> Includes unit testing, integration testing, system testing, acceptance testing.
Deployment --> Can involve phased rollout, full deployment, or pilot launches.


SDLC Models:


SDLC can be implemented using various models, depending on project requirements:


Waterfall Model:

Sequential process where each phase must be completed before the next begins.

Best for projects with clear and unchanging requirements.


Agile Model:

Iterative and incremental approach.
Emphasizes collaboration, flexibility, and frequent delivery of small portions of the product.

Agile delivers small, functional pieces of the product in cycles called iterations.


DevOps:

Focuses on collaboration between development and operations teams.
Automates and integrates development, testing, and deployment.


-->  A sprint is a short, time-boxed period (usually 1â€“4 weeks) during which a team works to complete a specific set of tasks or deliverables.





Hypervisor:


It's a software and it enables us to logically isolate server on a single physical server.

Example: VMware is a popular Hypervisor.

Each and every virtual machine has their own CPU, Memory and Harddisk.



Bash:




Bash is a powerful tool that allows users to interact with the Linux system through commands.



To login to aws virtual machine from command prompt:


ssh -i /path to/pem file ubuntu@ip

if you still get permission denied error change pem file permissions:

chmod 600 /path to/pem file




OS --> responsible for establishing a communication between hardware and software.


Linux OS:



Free
Opensource
secure
Fast


							    	
				systemsoftware |  Userprocess | Compilers
				------------------------------------------
							System libraries              
				------------------------------------------
								Kernel
				------------------------------------------

								 OS
									

--> kernel is heart of the OS.

--> There are 4 important aspects handled by kernel:

	1.Device Management
	2.Memory Management
	3.Process Management
	4.Handling system related calls
	
--> System libraries responsible for performing a task.


Feature	  		df -h						du -h

Scope		Entire filesystem			Specific files and directories

Focus	Disk space allocated 
				and available			Space used by files or directories


To check free space on the disk:

df -h
		

To check cpu's:

nproc

To check memory, cpu and performace:

top






Shell scripting for Devops:



--> For a shell script file the file extension should be .sh

--> To know more about the command you can use "man".

		 ex: man ls
		 
--> Touch command will create a file.

		touch filename
		
--> vi or nano commands will create and open the file.


--> #!/bin/bash --> means the following script will excute by the bash.

--> #!/bin/sh --> Previously it is redirected to the #!/bin/bash by linking concept, but from couple years some OS redirecting it to the #!/bin/dash as well.


--> To print something in linux, you need to use "echo"
		
--> To execute a shell script file.

		sh filename.sh    OR      ./filename.sh
		
--> To grant permission to a file "chmod".

--> chmod 777 filename

	--> 1st 7 -> owner
	--> 2nd 7 -> group
	--> 3rd 7 -> Others
	
--> 4->read, 2->write, 1->execute

--> To check what are all the processes running:

		ps -ef
		
		ps --> process status
		-e --> The -e flag tells ps to list all processes on the system (not just those belonging to the current user or terminal).
		-f --> full-format listing.
		
		
--> Check Open Ports

		ss -tuln
		
		-t â†’ TCP

		-u â†’ UDP

		-l â†’ Listening

		-n â†’ Show port numbers instead of service names
		
--> Check if a process is using the port

	lsof -i:<port_number>

		
--> To check the process particularly:

grep stands for "Global Regular Expression Print".

		ps -ef | grep "amazon" 
		
		ps -ef | grep 1
		
--> In above the pipe will send the first command output to the second command.

--> date | echo "this"

The output of above command is this.


Because date is a systemdefault command and it send its output to stdin driver.
but pipe will not be able to recieve the info from stdin.
It can only recieve the info when its not sending the info to stdin.


--> ps -ef | grep amazon | awk -F" " '{print $2}'

The above command will print the second column on which amazon services are running.

-F --> means field sepeartor which is seperating each column.


--> To find the 99th line of a file using only tail and head command.

		tail +99 file_name|head -1
		
--> I want to create a directory such that anyone in the group can create a file and access any personâ€™s file in it but none should be able to delete a file other than the one created by himself.

		chmod +t direc1

--> To monitor a continuously updating log file, We can use tail â€“f filename. This will cause only the default last 10 lines to be displayed on std o/p which continuously shows the updating part of the file.


--> Best practices to write a script.

		set -x --> Run your script in debug mode.
		
		set -e --> Exit the script when there is an error. It won't catch pipe failures.
		
		set -o --> Pipefail
		
		
--> "curl" is used to get or retrieve the info from internet.

--> "wget" is used to download the info from internet.

--> sudo find / -name test.sh

The above command find test.sh named file in all directories.



--> You can view the exit code whether its successful or not--> $?

		between 0 to 255

		0 - Successful
		Any other number -- Unsuccessful
		
	
--> view the exit code whether its successful or not--> $?
	
--> To know no. of arguments passed --> $#

--> To know the name of the script --> $0

--> PID of the current shell or script. --> $$
	
--> process ID (PID) of last background command --> $!

--> All arguments as a single string. --> $*	

--> All arguments as separate strings.--> $@	


			Use case									Syntax
			
			Test strings/files							[[ ... ]]
			Do arithmetic or compare numbers			(( ... ))
			Store an arithmetic result					$(( ... ))


--> The unset command directs a shell to delete a variable and its stored data from list of variables. It can be used as follows:

#!/bin/bash

var1="Devil"
var2=23
echo $var1 $var2

unset var1

echo $var1 $var2
Output:

DEVIL 23
23

--> These variables are read only i.e., their values could not be modified later in the script. Following is an example:

#!/bin/bash

var1="Devil"
var2=23
readonly var1
echo $var1 $var2
var1=23
echo $var1 $var2
Output:

Devil 23
./bash1: line 8: var1: readonly variable
Devil 23


--> How to Store User Data in a Variable?


  #!/bin/bash
  
  echo "Enter the length of the rectangle"
  read length
  echo "Enter the width of the rectangle"
  read width
  area=$((length * width))
  echo "The area of the rectangle is: $area"
  
  
--> To print length of string inside Bash Shell: 

--> â€˜#â€˜ symbol is used to print the length of a string.

Syntax:

variableName=value
echo ${#variablename}


--> str="welcome to geeks"

		echo ${str:0:10}
		
		o/p: welcome to
	


--> We also have the ability to check for any NULL or empty parameters passed using the -z or -n flags.


--> [ ... ] (Single Brackets) --> Basic tests (strings, files).

--> [[ ... ]] (Double Brackets) --> Advanced tests (regex, logical ops).


	Operator							Meaning
	
	  -z								String has zero length
	  -f								File exists and is regular file
	  -d								Directory exists
	  -r								File is readable
	  -e								File or directory exists
	  -x                                File is executable or not

#!/bin/bash	

if [[ -z "$1" ]];
then 
    echo "No parameter passed."
else
    echo "Parameter passed = $1"
fi


-->  We can use â€œ@â€ variable to access every parameter passed to the script via the command line.

#!/bin/bash

for i in $@
do 
 echo -e "$i\n"
done


--> -d file: The operator looks over if the file is present as a directory. If Yes, it returns true else false

#! /bin/bash

echo -e "Enter the name of the file : \c"
read file_name

if [[ -d "$file_name" ]]
then
   echo "$file_name is a directory"
else
   echo "$file_name is not a directory"
fi

-->  -e file: The operator inspects if the file exists or not. Even if a directory is passed, it returns true if the directory exists. 


#! /bin/bash
echo -e "Enter the name of the file : \c"
read file_name

if [ -e "$file_name" ]
then
   echo "$file_name exist"
else
   echo "$file_name not exist"
fi

--> -f file: If the file is an ordinary file or special file, then it returns true else false. It is [-f $file] syntactically.
Script:

#! /bin/bash
echo -e "Enter the name of the file : \c"
read file_name

if [ -f $file_name ]
then
   echo "$file_name is file"
else
   echo "$file_name is not file"
fi


--> -r file: This checks if the file is readable. If found yes, then return true else false. It is [-r $file] syntactically.
Script:

#! /bin/bash
echo -e "Enter the name of the file : \c"
read file_name

if [ -r $file_name ]
then
   echo "$file_name is readable"
else
   echo "$file_name is not readable"
fi


--> -x file: The operator looks over if the file is executable or not, and returns true and false subsequently.




--> if else in shell script

a=4
b=10

if [[ "$a" -lt "$b" ]]
then
   echo "a is less than b"
else
   echo "b is less than a"
fi


a=40
b=10

if (( $a > $b ))
then
   echo "a is greater than b"
else
   echo "b is greater than a"
fi


a=4
b=10

if (( a > b ))
then
   echo "a is greater than b"
else
   echo "b is greater than a"
fi
	

--> For loop 1-->start  5-->end

	for i in {1..5}
	do
	   echo "Welcome $i times"
	done




-->  0-->start  10-->end  2-->Increment by 2

	for i in {0..10..2}
	do
	  echo "Welcome $i times"
	done



--> Write a script to that prints upto 100 numbers which are divisible by 3,5 and not divisible by 15.

#!/bin/bash

# divisible by 3, divisible by 5, not 3*5=15
for i in {1..100}; 
do
if (( (i % 3 == 0 || i % 5 == 0) && (i % 15 != 0) ))
then
echo $i
fi;
done



--> write a shell script to count how many s in mississipi.


#!/bin/bash

x=mississipi

grep -o "s" <<< "$x" | wc -l

#! /bin/bash

echo Enter the filename
read file
c=`cat $file | wc -c`
w=`cat $file | wc -w`
l=`grep -c "." $file`
echo Number of characters in $file is $c
echo Number of words in $file is $w
echo Number of lines in $file is $l


--> A soft link is an actual link to the original file, whereas a hard link is a mirror copy of the original file. 

--> bash is a dynamically typed.why because we don't need to provide datatype while declaring a variable and we can directly assign different types of datatype values.

--> "traceroute" --> for network troubleshooting.

	Ex: traceroute google.com
	
--> To manage logs of a system that generates huge logs everyday we can use "logrotate" command.

you can also zip the log folder.





Write a script to report the usage of aws resources:



Create a file with .sh extension


########################	
#Author: Suresh
#Date: 08/04/2024
#Version: v1

# This script will report the usage of aws resources
############################


# the below line put your script into a debug mode
set -x   


#list s3 buckets

echo "Print list of s3 buckets"
aws s3 ls > resultfile

#list EC2 instances

echo "Print list of EC2 instances"
aws ec2 describe-instances | jq '.Reservations[].Instances[].InstanceId' >> resultfile

#list lambda

echo "Print lambda"
aws lambda list-functions >> resultfile

#list IAM users

echo "Print IAM users"
aws iam list-users


Run the file:

./file-name.sh







--> A fork is a copy of a repository that allows you to make your own changes without impacting the original project.





GIT:


--> its a distributed version control system.

--> SVN also version control system, but its a centralized version control system.

--> In a centralized version control system, if dev1 wants to send his code to dev2, first he need to send it to SVN, then dev2 has to pull the code from SVN.Everything happens through the SVN.

--> In distributed version control system, we can create a multiple copy of the original code and then we can make changes in the copies without affecting the original code.This way it is distributed.



--> To know what are the changes made in the file.

		git diff CID
		
		
		

--> Git Branching Startegy:

1. Master branch --> Main branch 
2. Feature branch --> To add new features
3. Release branch --> To deliver to prod.
4. Hot fix branch --> For short-term period.



--> How to create or initialize git repository:

			git init

--> We can download git repository in 2 ways.

		1. Using HTTPS--> Need to give password for authentication.
		2. Using SSH --> Need to give public key.
		
--> to generate a public key you can use below one.

		ssh-keygen
		
--> To create a new branch and switch to it.

		git checkout -b branch-name
		
--> To switch to different branch from main branch

		git checkout branchname
		
--> To merge the branch

		git cherry-pick commitID
		
--> To merge a branch by using merge

		git merge branch-name
		
--> To merge a branch by using rebase

		git rebase branch-name
		
--> git cherry-pick is good when there is an less no of commits.


--> git rebase and git merge both will merge the branches.

--> if you use git rebase you will get linear commit history.

--> in git merge you won't get linear commit history.

--> when you do git fetch the file will be stored in /.git/refs/remotes/origin/main



--> GitHub Actions:


--> In .github/workflows we need to create a .yml file

-->	In runs on if you specify self it will be a self-hosted runner.Otherwise it's a external runner.





--> Git worktree:




--> Below command creates a worktree along with a branch that is named after the final word in your path.

	git worktree add ../feature-x feature-branch


--> If you want to give you branch a unique name then you can use the -b flag with the add command.

	git worktree add -b feature-xyz ../feature-xyz origin/main
	
--> View the list of worktrees with 

	git worktree list

--> To remove the worktree

	git worktree remove feature-x
	
	Need to do a quick fix without touching your current changes?git worktree add bugfix -b bugfix-branch will create a new directory called â€œbugfixâ€ and create and checkout a new branch in it called â€œbugfix-branch". From there on, continues as you normally would in that directory.

	Want to check out a colleague's branch?git worktree add <target directory> â€” checkout <branch> is your command. It will checkout an existing branch in the target directory.


AWS Services for DevOps:

1. EC2
2. VPC
3. EBS Volumes
4. S3
5. IAM
6. Cloudwatch
7. Lamda
8. Cloud Build services --> AWS CodePipeline, AWS CodeBuild, AWS CodeDeploy
9. AWS Configuration
10.Billing & Costing
11.AWS KMS
12.Cloud Trail
13.AWS EKS
14.AWS ECS
15.Elastic Search (ELK)



Configuration Management:




The below are the popular Configuration Management tools:

1. Puppet
2. Chef
3. Ansible --> Most used --> part of RedHat
4. Salt


Puppet --> Pull mechanism, Master-Slave Archtecture, Puppet laguage

Ansible --> Push mechanism, Agentless, yaml, we can write our Ansible modules.

We can share this ansible modules by using ansible galaxy.

Disadvantages of Ansible:

1. Configuration Management with windows is difficult.
2. it has to improve with the debugging.
2. Need to improve performace as well.


Interview questions:

1.Ansible modules written in python.
2.Ansible supports both windows and linux as well.
For linux it uses SSH protocol, for windows it uses WinRM protocol to connect to windows.

3. Puppet vs Ansible vs Chef
4. Ansible pull or push mechanism
5. Ansible uses yaml to write ansible playbooks.




--> For huge tasks we will use ansible roles



Ansible environments have three main components:


Control node:

A system on which Ansible is installed. You run Ansible commands such as ansible or ansible-inventory on a control node.

Inventory:

A list of managed nodes that are logically organized. You create an inventory on the control node to describe host deployments to Ansible.

Managed node:

A remote system, or host, that Ansible controls.



Rebooting servers:

ansible client -a "/sbin/reboot"

Managing files:

 ansible client -m copy -a "src=/etc/hosts dest=/tmp/hosts"
 
To change ownership:

ansible client -m file -a "dest=/srv/foo/a.txt mode=600"

To create directories

ansible client -m file -a "dest=/path/to/c mode=755 state=directory"


delete directories

ansible client -m file -a "dest=/path/to/c state=absent"



Managing users and groups:


You can create, manage, and remove user accounts on your managed nodes with ad hoc tasks:


ansible all -m user -a "name=foo password=<encrypted password here>"

ansible all -m user -a "name=foo state=absent"


Managing services:

ansible webservers -m service -a "name=httpd state=started"

ansible webservers -m service -a "name=httpd state=restarted"

ansible webservers -m service -a "name=httpd state=stopped"




Check mode:

In check mode, Ansible does not make any changes to remote systems. Ansible prints the commands only. It does not run the commands.

ansible all -m copy -a "content=foo dest=/root/bar.txt" -C


Enabling check mode (-C or --check) in the above command means Ansible does not actually create or update the /root/bar.txt file on any remote systems.



This table lists common patterns for targeting inventory hosts and groups.



All hosts	
all (or *)One host

Multiple hosts	
host1:host2

One group	
webservers

Multiple groups	
webservers:dbservers --> all hosts in webservers plus all hosts in dbservers

Excluding groups	
webservers:!atlanta --> all hosts in webservers except those in atlanta

Intersection of groups	
webservers:&staging --> any hosts in webservers that are also in staging



What are Ansible Roles?

An Ansible Role is a way to organize playbooks into reusable, structured units.
Instead of writing a huge playbook, roles let you break it down into smaller pieces: tasks, variables, templates, handlers, etc.


To generate the initial directory structure for a role

ansible-galaxy init role_name --> test_role




The below is the ansible role basic structure:

defaults â€“  Includes default values for variables of the role. Here we define some default variables, but they have the lowest priority and are usually overridden by other methods to customize the role.

files  â€“ Contains static and custom files that the role uses to perform various tasks.

handlers â€“ A set of handlers that are triggered by tasks of the role. 

meta â€“ Includes metadata information for the role, its dependencies, the author, license, available platform, etc.

tasks â€“ A list of tasks to be executed by the role. This part could be considered similar to the task section of a playbook.

templates â€“ Contains Jinja2 template files used by tasks of the role.

tests â€“ Includes configuration files related to role testing.

vars â€“ Contains variables defined for the role. These have quite a high precedence in Ansible.



--> Using roles:


You can use roles in three ways:

--> at the play level with the roles option: This is the classic way of using roles in a play.

--> at the tasks level with include_role: You can reuse roles dynamically anywhere in the tasks section of a play using include_role.

	Include statements are processed during the execution of the playbook.

--> at the tasks level with import_role: You can reuse roles statically anywhere in the tasks section of a play using import_role.

	Import tasks are processed before the execution of the playbook.

--> When you use the roles option at the play level, Ansible treats the roles as static imports and processes them during playbook parsing.


--> Even if we define a role multiple times, Ansible will execute it only once. Occasionally, we might want to run a role multiple times but with different parameters. Passing a different set of parameters to the same roles allows executing the role more than once.



--> default path to Ansible role: /etc/ansible/roles





Terraform:



--> API as code

--> terraform interact with the api's of providers.

-->Once you wrote the script, terraform will convert the script which is undrestandable by the provider and send a request to the provider API and get the result from API and it will show us on terminal.

--> The below is basic terraform file with required info to create any resources after provider write your resources.

terraform {
required_providers {
aws = {
source = "hashicorp/aws"
version = "~> 4.16"
}
}
}
provider "aws" {
region = "us-west-2"
}

--> Once you hit terraform apply, it will create a terraform.tfstate file in where you run the apply. In this file terraform tracks the information of the infrastructure it is created.


Problems with terraform:


- State file is single source of truth.
- Manual changes to the cloud provider cannot be identified and auto-corrected.
- Not a GitOps friendly tool. Don't play well with Flux or Argo CD.
- Can become very complex and difficult to manage
- Trying to position as a configuration management tool as well.



--> To generate key file to use in provisioner 

	ssh-keygen -t rsa



file Provisioner:

The file provisioner is used to copy files or directories from the local machine to a remote machine. This is useful for deploying configuration files, scripts, or other assets to a provisioned instance.

Example:

provider "aws" {
  region = "ap-southeast-1"
}

resource "aws_instance" "ex1" {
  ami = "ami-003c463c8207b4dfa"
  instance_type = "t2.micro"
  key_name= "id_rsa"
  vpc_security_group_ids = [aws_security_group.main.id]

provisioner "file" {
    source      = "C:/Users/91868/Desk/Desktop/AWS/AWS+DevOps/terraform/test.txt"
    destination = "/home/ubuntu/test.txt"
  }
   connection {
      type        = "ssh"
      host        = self.public_ip
      user        = "ubuntu"
      private_key = file("C:/Users/91868/.ssh/id_rsa")
      timeout     = "4m"
   }
}


resource "aws_security_group" "main" {

  ingress {
    description = "SSH"
    from_port   = 22
    to_port     = 22
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }
}

resource "aws_key_pair" "deployer" {
  key_name   = "id_rsa"
  public_key = file("C:/Users/91868/.ssh/id_rsa.pub")
}


In this example, the file provisioner copies the localfile.txt from the local machine to the /path/on/remote/instance/file.txt location on the AWS EC2 instance using an SSH connection.

remote-exec Provisioner:

The remote-exec provisioner is used to run scripts or commands on a remote machine over SSH or WinRM connections. It's often used to configure or install software on provisioned instances.

Example:

provider "aws" {
  region = "ap-southeast-1"
}

resource "aws_instance" "ex1" {
  ami = "ami-003c463c8207b4dfa"
  instance_type = "t2.micro"
  key_name= "id_rsa"
  vpc_security_group_ids = [aws_security_group.main.id]

  connection {
      type        = "ssh"
      host        = self.public_ip
      user        = "ubuntu"
      private_key = file("C:/Users/91868/.ssh/id_rsa")
      timeout     = "4m"
   }

provisioner "file" {
    source      = "C:/Users/91868/Desk/Desktop/AWS/AWS+DevOps/terraform/test.txt"
    destination = "/home/ubuntu/test.txt"
  }
   

   provisioner "remote-exec" {
    inline = [ 
        "echo ' Hello'",
        "sudo apt update",
        "sudo apt-get install nginx",
        "ststemctl start nginx",
        "nginx --version"
    ]
     
   }
}


resource "aws_security_group" "main" {

  ingress {
    description = "SSH"
    from_port   = 22
    to_port     = 22
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }
}

resource "aws_key_pair" "deployer" {
  key_name   = "id_rsa"
  public_key = file("C:/Users/91868/.ssh/id_rsa.pub")
}


In this example, the remote-exec provisioner connects to the AWS EC2 instance using SSH and runs a series of commands to update the package repositories, install Apache HTTP Server, and start the HTTP server.



How to use variables in terraform:


main.tf file:


provider "aws" {
    region = "us-east-1"
}

variable "ami" {
  description = "This is AMI for the instance"
}

variable "instance_type" {
  description = "This is the instance type, for example: t2.micro"
}

resource "aws_instance" "example" {
    ami = var.ami
    instance_type = var.instance_type
}


vars.tf file

ami = "ami-053b0d53c279acc90"


--> terraform apply '-var-file=dev.tfvars'


export TF_VAR_instance_type="t3.micro"

variable "instance_type" {
  type    = string
  default = "t2.micro"
}

Terraform automatically sets var.instance_type to "t3.micro".


--> Terraform provides a sensitive attribute that can be used to mark variables and outputs as sensitive. When a variable or output is marked as sensitive, Terraform will not print its value in the console output or in the state file.

For example:

variable "aws_access_key_id" { sensitive = true }


--> To create new workspace:

	terraform workspace new workspace-name
	
--> When you create a new workspace at rthe same time it will create a terraform.tfstate.d file. Inside there is the workspace you created.



--> switch to that workspace:

		terraform workspace select workspace-name
		
--> To know the list of available commands:

		terraform workspace -h


--> To check which workspace:

		terraform workspace show


--> To create EC2 for multiple regions


provider "aws" {
alias= "useast"
region = "us-east-1"
}

provider "aws" {
alias="uswest"
region = "us-west-2"
}

resource "aws_instance" "example" {
ami = "ami-0123456789abcdef0"
instance_type = "t2.micro"
provider = aws.useast
}

resource "aws_instance" "example2" {
ami = "ami-0123456789abcdef0"
instance_type = "t2.micro"
provider = aws.uswest
}


	


CI/CD:

GitHub Actions:

--> In .github/workflows/action.yml

--> The above is the default path for writing github action yaml file.



Advantages of GitHub Actions over Jenkins:



â€¢ Hosting: Jenkins is self-hosted, meaning it requires its own server to run, while GitHub Actions is hosted by
GitHub and runs directly in your GitHub repository.

User interface: Jenkins has a complex and sophisticated user interface, while GitHub Actions has a more
streamlined and user-friendly interface that is better suited for simple to moderate automation tasks.

â€¢ Cost: Jenkins can be expensive to run and maintain, especially for organizations with large and complex
automation needs. GitHub Actions, on the other hand, is free for open-source projects and has a tiered
pricing model for private repositories, making it more accessible to smaller organizations and individual
developers.

Advantages of Jenkins over GitHub Actions

â€¢ Integration: Jenkins can integrate with a wide range of tools and services, but GitHub Actions is tightly
integrated with the GitHub platform, making it easier to automate tasks related to your GitHub workflow.


--> We have .jenkins directory. To backup jenkins we need to take backup .jenkins directory using cronjobs.

--> in /var/lib/jenkins we have .jenkins directory.

--> /var/lib/jenkins/

	â”œâ”€â”€ config.xml
	â”œâ”€â”€ credentials.xml
	â”œâ”€â”€ jobs/
	â”‚   â”œâ”€â”€ project-A/
	â”‚   â”‚   â””â”€â”€ config.xml
	â”‚   â””â”€â”€ project-B/
	â”‚       â””â”€â”€ config.xml
	â”œâ”€â”€ plugins/
	â”œâ”€â”€ secrets/
	â”œâ”€â”€ users/
	â””â”€â”€ workspace/
	
	
config.xml â†’ global Jenkins configuration

jobs/ â†’ all Jenkins jobs (with their configurations)

plugins/ â†’ installed Jenkins plugins

secrets/ â†’ encrypted credentials and keys
	
users/ â†’ user account information




--> In Jenkins, a shared library is a way to store commonly used code(reusable code), such as scripts or functions, that can be used by different Jenkins pipelines.

--> Shared library contains src,resource and vars.

--> In vars you have to write your script.

--> To import your shared library in your script.

		@Library("Library-name")_
		
		_ means need to import everything from that particular shared library.
		
		
		
		
		
		
		
Docker:



--> Docker containers are lightweight in nature, because they do not have a full OS.
	
	They use the resources from the base OS or the host on which they are running.

--> Docker Image = Application + Application libraries + system dependencies.

--> Lifecycle of Docker Container:

	1. Docker file
	2. Docker Image
	3. Docker Container


--> When we are building Docker image from docker file it will create a lot of layers.

--> Docker is more dependent on Docker Engine.

--> Docker Engine is single point of failure.If the docker engine down all the docker containers will also be down.

--> Each and every container has their own files.

--> Files and Folders in containers base images

    /bin: contains binary executable files, such as the ls, cp, and ps commands.

    /sbin: contains system binary executable files, such as the init and shutdown commands.

    /etc: contains configuration files for various system services.

    /lib: contains library files that are used by the binary executables.

    /usr: contains user-related files and utilities, such as applications, libraries, and documentation.

    /var: contains variable data, such as log files, spool files, and temporary files.

    /root: is the home directory of the root user.


--> Files and Folders that containers use from host operating system


    The host's file system: Docker containers can access the host file system using bind mounts, which allow the container to read and write files in the host file system.

    Networking stack: The host's networking stack is used to provide network connectivity to the container. Docker containers can be connected to the host's network directly or through a virtual network.

    System calls: The host's kernel handles system calls from the container, which is how the container accesses the host's resources, such as CPU, memory, and I/O.

    Namespaces: Docker containers use Linux namespaces to create isolated environments for the container's processes. Namespaces provide isolation for resources such as the file system, process ID, and network.

    Control groups (cgroups): Docker containers use cgroups to limit and control the amount of resources, such as CPU, memory, and I/O, that a container can access.
    
	
--> The heart of the docker is docker daemon.When you are installing docker means you actually installing docker daemon.

--> In Docker file both ENTRYPOINT and CMD run as your start command.

--> Whatever you defined in an entry point is not overrideable.

     Sets the command and parameters that cannot be overridden from docker run command arguments. It allows the container to be run with additional command-line arguments that are appended to the ENTRYPOINT.

--> CMD is overrideable.

   Can be overridden by the user when starting a container with docker run by specifying a different command.
   

--> When you write a docker file you will install lot of dependecies whether its related to application or system dependencies.All these dependencies are there in final image.

--> To reduce the complexity and size of the docker image we will use multi stage build.

--> In multi stage Docker build we will split docker file into multiple steps and in final image we don't have all the dependencies. we only have what the environment application need to run the application.

--> Distroless image is a very minimalistic image that can contain only run time environments.

--> Distroless image main advantage is security.


--> Docker Bind mounts

 --> In this we will bind specific folder in a conatianer to specific folder in Host.Even if the container goes down we can get the info from Host folder.
 
 --> Docker Volume also will do samething.Using Docker CLI we will create Docker Volumes.
 
 --> By using Docker Volume you can mount external storage devices as well as voulmes to container.
 
 --> To create a docker volume
 
		docker volume create volume-name
		
 --> To list the volumes
 
		docker volume ls
		
--> To get the details of volume

		docker volume inspect volume-name
		
--> To remove volume

		docker volume rm volume-name
		
--> To mount a volume to a container

		docker run -d --mount type=volume,source=suri,target=/app image
		
-->  To inspect a container

		docker inspect containerID


--> To delete a volume first you have to stop the container then only you can delete volume.


--> Docker Network allows you to communicate with other containers and host system.

--> When you create a container on a host it will connects through concept called bridge networking.

	in bridge networking, we have virtual ethernet which is called docker 0. these will by defaultly created when you create a container and it will connect container with host.
	
--> When you delete docker 0 you are not able to access container.


--> We can create custom bridge network using docker network command.

--> To create a new network

	docker network create name
	
--> To attach new network to the container

	docker run -d --name finance --network=network-name image-name.



--> COPY --> if you already have file in your local server and you want to add it to your container.

--> ADD --> If you want to add file which is not in your server you want add directly form external source like interenet. 

--> To save docker file

	docker save file-name > /path/to/save/file-name.tar
	
	docker save -o tarfile-name imageID
	
--> list files in continer

	docker exec kinesis-consumer ls /opt/kconsumer/
 
--> copy file from container

   docker cp <container_id>:/path/to/file_or_directory /path/on/host
   
   
--> 1. Copy from Host to Container

docker cp <host_file_path> <container_id>:<container_target_path>

ðŸ“¥ 2. Copy from Container to Host

docker cp <container_id>:<container_file_path> <host_target_path>

   
--> to remove file from continer

	docker exec -u root kinesis-consumer rm /opt/kconsumer/backup.json

--> To backup the container ( when we backup container it will backup as a image)

	docker commit containerID backupimagename



	Docker Compose:
	
	
	docker compose up
	
	
--> making it easy to manage services, networks, and volumes in a single, comprehensible YAML configuration file.Then, with a single command, you create and start all the services from your configuration file.	
	
--> Docker compose is used to manage multi container applications.

--> To run multiple containers at once you need to use docker-compose up


--> To automatically write docker file and docker compose file you can use docker init command.It will create dockerfile and compose file by following best practices.

--> Docker Compose relies on a YAML configuration file, usually named compose.yaml.

--> The default path for a Compose file is compose.yaml (preferred) or compose.yml that is placed in the working directory.

--> There are three types of volumes: anonymous, named, and host.

--> Docker manages both anonymous and named volumes

--> We can configure host volumes at the service level, and named volumes in the outer level of the configuration.





Kubernetes:



--> Kubenetes is a container orchestration platform.

--> The major diff b/w Docker and Kubernetes:

   1. Host --> Docker runs on single host and kubernetes is a cluster.
   
   2. Auto Scaling--> Docker doesn't support and kuberenets support auto scaling.
   
   3. Auto healing --> If there are any issues container will die and new contaier won't available but in kuberenets it will cretae new container.
   
   4. Enterprie support --> Docker won't support advanced load balancing, advanced networking and security thing.
   
   
--> Kubernetes Architecture:


--> In kubernetes worker node we have 3 components.

-->  To run a container we need container run time docker supports Dockershim.
	 Kubernetes supports Dockershim, crio and containerd as pod runtime.
  
--> Kubelet is responsible to check whether pod is running or not if not it has to inform to master.

--> Docker we have default network like bridge network in kubernetes also we have default network called kubeproxy.

    kube-proxy maintains network rules on each node in cluster. These rules allow network communication to the pods from inside or outside the cluster.
  
  
--> In kubernetes master or control plane we have below components.


--> API server will take all the external world requests.For example if you want to create a pod you need to talk with API server.

--> Scheduler will recieve info from API server based on that it will schedule the tasks.
    The scheduler assigns workloads to worker nodes based on resource availability and other policies

--> etcd is a key value store.Entire kuberentes cluster is stored in key value in etcd.

--> In kubernetes there are multiple controllers like replica sets and there has to be a component which ensures that these controllers are running.which manages all these controllers is controller manager.




--> In pod we have containers and kuberenetes assign cluster ip address to pod and through cluster ip address we can access service running in container.Kubeproxy will generate this cluster ip address.

--> Using kops we can install and  manage 100s of kubernetes clusters.

    kops --> kubernetes operations
	
--> A pod can be a single container or multiple containers.

	If there are multiple containers in a pod we have some advantages like pod allows containers to  shared network, shared storage and communicate through localhost.

--> in a docker when we start container we will give all the commands like port, network, name and volume in command line arguments.
   
    In kubernetes we will write all these specifications in pod.yaml file and we will run the pod with those specifications.
	

--> For docker we have Docker CLI to interact with docker engine in the same way we have kubectl to interact with kubernetes.

    kubectl is a command line interface.
	

--> kubectl cheat sheet


--> kubectl describe pod pod-name --> it will give all the info about pod

--> kubectl logs pod-name --> to check logs
	
--> Pod don't have the capability of auto healing and auto scaling. To achieve these two we will use deployment resource.

    Internally deployment will create Replica sets and replica sets will create pods only.
	
--> Replica set is a kubernetes controller what it actually do means it maintains the desired state which we will write in deployment.yaml file and the actual state in the cluster is same.



--> service:

 --> The main adavantahge of service in kuberentes is 
   
     1. Load balancing.
	 
	 --> in this servcie acts as a middleperson between pods and user and it takes the user request and process it to available pod.
	 
	 2. Service Discovery.
	 
	 --> In this if a pod goes down in cluster deployment will generate another pod but with different ip address.But service keep tracks of the pods by using the concept called Lables&selectors not by the ip address.
	 
	 --> Even if new pod comes the new pod will generate through the existing deployment.yaml file, it has same lables.By using this concept service will get to know if there is any new pod created.
	 
	 --> Label we will write in deployment.yaml file.
	 
     --> Another important thing service can do it exposes our application to external world and outside the cluster people can access our application.
	 
	 --> When you are creating a service you have 3 modes to choose
	 
	     1. cluster ip --> access your service inside cluster only
		 
		 2. NodePort   -->  Whoever access to the node ip addresses they can access
		 
		 3.Load balancer --> Expose to external world
		 
--> In kubernetes a namespace is a logical isolation of resources,network policies, accesses and everything.
    
	Foe example, there are 2 projects using same kubernetes cluster.One project can use one namespace and another can use another namespace without any overlap and authentication problems.
	 

--> Kube-proxy works by maintaining a set of network rules on each node in cluster, which are updated dynamically as services are added or removed. When a client sends a request to a service, the request is intercepted by kube-proxy on the node where it was recieved.

kube-proxy then looks up the destination endpoint for the service and routes the request accordingly.

--> kube-proxy can ensure that services can communicate with each other.





--> Kubenetes Ingress


   1. Kuberenetes will provide only round robin load balancing type, but legacy load balancers will provide a lot of options in load balancing.
   
   2. service- load balancing type will create one IP for one service, if we have 1000 services it will create 1000 IPs.Cloud providers will charge for that 1000 static IPs.
   
 --> To overcome these challenges we need ingress.
 
 --> We will create a ingress resource along with ingress controller.Ingress controller is nothing but a load balancing type we need.




--> RBAC --> Role Based Access Control

  RBAC divided into

  Users --> Controls the users actions in cluster 
  
  Service Accounts --> Managing the access of a services that are running in a cluster.

--> For managing the RBACs in kubernetes we have 3 ways

    1. Service Accounts/ Users
	
	2. Roles/ Cluster Role
	
	3. Role binding/ Cluster Role binding
	

--> kuberenets will not manage the user management instead of this it will offload this to Identity Provider.

  --> Role means Permissions, 
  
      Service accounts means user
	  
	  Role Binding --> Binding the persmissions to a user.
	  
--> If you create a role within the namespace its Role, and if you do within the Cluser its cluster role.




--> Custom Resources in kubernetes


  --> When you want to introduce New resource into kuberenets cluster or kuberenetes we will use Custom Resource.
  
  --> By extending the capabilities of kuberenetes API, we can introduce custom Resource.
  
  --> To extend the capability of API we have 3 resources.
  
      1. CRD --> Custom Resource Defination
	  
	  2. CR --> Custom Resource
	  
	  3. Custom Controller 
  
  --> To extend the capabilities of kubernetes, user has to interact with kubernetes API.
  
  --> We have a component called client go, and it allows us to talk with kubernetes API server.
  
  
  ConfigMap:
  
  
--> configmap is used to store information witin the cluster and we can access this configmap from our container or pod etc...

   --> To create a configmap we need write a YAML file and apply, then configmap will be created. Then API server will store this data in etcd in a key value pair.
   
   --> You can access configmap easily and can read the data.
   
   --> We can use configMap in 2 ways 
   
     1. As environment variable 
	 
	 2. As voulme mounts

  
  Secrets:
  
--> Secrets is used to store sensitive information.

   --> When you create a secret the API server will store the info in etcd but with encryption, untill unless if you don't have decryyption key you cannot decrypt.
   
   --> You can access secrets but for that we need to give strong RBAC policies.









AWS:

























https://158974530940.signin.aws.amazon.com/console


Access key:




Secret access key:




region:

ap-southeast-1





--> pending tasks


1. terraform with VPC
2. terraform vault








3 tier architecure



https://www.showwcase.com/article/35459/...












########################################################################################################################










############################################     Kodekloud:  ########################################




Shell Types:

Bourne shell (Sh shell)
C shell (csh or tcsh)
Z shell (zsh)
Bourne again Shell (bash)


--> TO know the shell type  --> echo $SHELL

--> To run multiple commands  --> cd newdirectory; mkdir www; pwd

--> To create multiple directories --> mkdir -p tmp/asia/india/bangalore

--> To remove directory and its content --> rm -r tmp/mydir1

--> To copy directory and all of its content--> cp -r mydir1 tmp/my1

--> To move or rename the file --> mv newfile samplefile

--> To give permission to all subdirectories in a directory --> chmod -R 755 tmp/asia/india/bangalore

--> To know which user --> whoami

--> To download from internet and save it in a file  --> wget http://www.test.com -O file.txt

--> To check OS details --> cat /etc/os-release

--> CentOS --> yum

--> Debian/Ubuntu --> apt

--> To check repo list --> /etc/apt or /etc/yum

--> To list the packages downloaded --> apt list package-name      yum list package-name

--> To start a service automatically when system on -->  systemctl enable service-name

--> systemctl command used to manage systemd services, so we need to configure the program or service as a systemd service.

--> systemd service is configured using systemd unit file located at-->   /etc/systemd/system

--> To configure a application as a service:

--> Go to /etc/systemd/system.

--> create a file with your application name --> myapp.service

--> In that write below

	 [Unit]
	 
	 Description=basic python web app
	 
	 After = postgresql.service    ## myapp.py will run after postgre up and runnig. This does not enforce a 	dependency; it only sets an order.(USE ANYONE)
	 
	 Requires=postgresql.service   ## If you also want to enforce a dependency (ensuring PostgreSQL must be running for myapp.py to start) (USE ANYONE)

     [Service]
	 
	 ExecStart=/usr/bin/python3  /opt/code/myapp.py
	 ExecStartPre=/path
	 ExecStartPost=/path
	 Restart=always
	 
	 
	 [Install]
	 
	 WantedBy=multiuser.target
	 
--> reload by --> systemctl daemon-reload

--> systemctl start myapp

--> To prepare the system to run myapp on startup -->  systemctl enable myapp



--> Network Basics:




	--> ip link: to list interfaces on the host.
	
	--> To up the the interface --> ip link set dev interface_name up.
	--> ip addr: to see the assigned ip addresses.
	--> ip addr add 192.168.1.0/24 dev eth0 : To set the ip addresses.
	--> ip route : to see the routing table
	--> ip route add 192.168.1.0/24 via 192.168.2.1 : to add ip addresses on the routing table.
	
	--> changes made from above commands will not persist after the system boot
	
	--> command to check if ip forwarding enable or not:  cat /proc/sys/net/ipv4/ip_forward  --> 0 means disable 1 means enable.
	--> When you reboot system above 1 will not persist.
	    To persist we need to change it in /etc/sysctl.conf: net.ipv4.ip_forward = 1


--> To assign name to ip address --> /etc/hosts  --> 192.168.1.11  db

--> In a real world scenario we have lot of hosts and names, so it shouldn't possible to put in hosts file.

	Instead of that we redirect our hosts to look up to DNS server for ip and name.
	
--> How do we point our host to DNS server.
 
    Every host has a DNS resolution configuration file at  /etc/resolv.conf there we need to add DNS server address.
	
		nameserver  192.168.1.100
	
--> we can add name in /etc/hosts and in DNS server as well.

--> If we add same in /etc/hosts and DNS server then it will first check in /etc/hosts if it finds it will move further and if it not finds it will check in DNS server.

--> We can change this order in /etc/nsswicth.conf

	hosts:  files dns
	
--> in DNS server 192.168.1.10 linked to 	web.mycompany.com

--> if you want to ping you need to -->  ping web.mycompany.com

instead of that in /etc/resolv.conf file

		nameserver 192.168.1.100   --> DNS server
		
		search     mycompany.com
		
-->  now you can try ping web



--> Application Basics:


--> To create a JAR file from java application 

	jar cf     Myapp.jar          MyClass.class service.class service2.class
	
	          Jar file name				classes names
			  
--> When you run above command it will generate file at META-INF/MANIFEST.MF inside Myapp.jar.

    This file contains info about the files packaged into the JAR file and other metadata regarding application.
	
-->   MANIFEST.MF

		Manifest-version: 1.0
		Created-By: 1.8.0
		Main-Class: MyClass
		
		
--> jar xf Myapp.jar META-INF/MANIFEST.MF --> It will create accessbile META_INF file there you can check MANIFEST file
		
--> We need to specify the entry point at Main-Class because when someone runs this application it will start from what we mentioned there.

--> To run the JAR file

	java -jar Myapp.jar
	

	
--> In real world scenario its going to be a lot of jar files. To maintain all this is not possible. Thats why we have Build Tools like Maven, Gradle, and ANT.
	
--> For node --> npm
	For python --> pip
	
--> To check where the pyhton is looking for the packages -->  python3 -c "import sys; print(sys.path)"

--> Instead of downloading all the dependencies one by one we can put them in requirements.txt file.

	requirements.txt
	
	Flask
	jinja2
	gunicorn
	
--> pip install -r requirements.txt --> it will download all the dependecies in requirements.txt file.



--> Webserver:

What is a Web Server?

	A web server is software (and sometimes hardware) that:

	Accepts HTTP/HTTPS requests from clients (browsers, apps).

	Returns responses (HTML, JSON, images, files).

	May serve static content or dynamic content via scripts.



--> Static webservers known as Web servers --> nginx, apache-http or apache webserver.
 
	It serves static content like HTML, CSS, JavaScript, and images to clients.

--> Dynamic webservers known as Application servers --> Apache Tomcat,.NET core,  gunicorn.

	An application server is responsible for running business logic and generating dynamic content.


--> Public key --> .crt   .pem


--> Private key --> .key  .pem



--> Ansible can manage windows hosts and cannot run on windows.
	It definitely need linux on where you are installing.

--> The kubernetes control plane run in Liux OS.

--> Home directory is different for different users.

	For example  Bob has --> /home/bob  			Michael has --> /home/michael
	
--> To store personal data we will use home directory.

--> Others users cannot access another users home directory.

--> ~ symbol represents the home directory.

--> echo hello  --> prints "hello" in newline.

--> echo -n hello --> prints "hello" without adding a newline at the end.

--> 		echo    -n      hello
		  command   option  argument
		  
		  
--> command types:

Internal or Built-in commands --> Total 30 commands

External commands

--> To check command is which type -->       type command-name

--> cd     --> move to home directory

--> cd /   --> move to root directory

--> pushd [directory]  --> Changes the current directory to the specified directory and pushes the previous directory onto a stack.

--> popd  --> Removes the top directory from the stack and returns to the directory now on top.



Example:

--> pushd /var/log

This command will change the current directory to /var/log and push /home/user onto the stack.

--> popd

This command removes the top directory from the stack (in this case, /var/log) and changes the directory back to /home/user.


-->  cat > test/test.txt   --> navigate to the text file and ask for your input to write in file.

--> after writing in the file to exit enter CTRL  d.

--> less file_name.txt  -->  up arrow --> scroll up one line     down arrow --> scroll down one line

--> ls -l (long list)--> lists files with permissions and created dates.

--> ls -a --> list all files including hidden

--> ls -lt --> lists files in order created

--> ls -ltr --> lists files in reverse order created

--> man command --> to check command information

--> command --help --> to check command options and arguments.

--> echo $HOME --> to check to home directory.

--> to change shell --> chsh

--> bash shell features --> auto completion,  Alias --> alias dt=date  ,  history

--> to check all the environment variables --> env

--> to set the environment variables --> export envvariable=Value

--> Once you boot up your system the set environment variables will be gone.

--> suri@LAPTOP-SFEIMFAM:--> if you want to change LAPTOP-SFEIMFAM go to /etc/hostname  --> in file your required name.



--> to check kernel version --> uname -r

--> uname -r --> 4.15.0-72-generic

4 --> kernel version
15 --> Major version
0 --> Minor version
72 --> patch release
generic --> Distro specific info


-->     				application
							|
						  kernel
							|
						Hardware
						


--> to monitor the events for USB device  --> udevadm monitor

--> lsblk --> to list information about all available block devices on the system, such as hard drives, SSDs, partitions, and USB devices. 
This command is very useful for examining disk partitions, mount points, and device sizes.

--> to list cpu architecture --> lscpu

--> 32 bit cpu means it can store and access upto 2power32 values in register.

--> register means storage in cpu that can rapidly acceess.

--> to check total, used and free memory --> free -m   (-m --> means in MB)

--> runlevel --> N 5

		Runlevel 3: Multi-user, networking, no GUI (ideal for servers).
		Runlevel 5: Multi-user, networking, with GUI (ideal for desktop systems).
		Systemd Targets: multi-user.target (CLI), graphical.target (GUI).
		
--> multi-user.target sets up a fully functional command-line interface (CLI) environment with networking and multi-user support.

--> To set the default target --> sudo systemctl set-default multi-user.target

--> ls -l --> d --> directory  - --> regular file    b --> block device

--> File system hierarchy:

/home --> Users home directory
/opt  --> Install third party programs (projects)
/media --> External media mounted under file system like USB
/dev  --> contains files for devices external hardisks and devices such as mouse, keyboard.
/bin --> The basic programs and binaries such as cp, mv, mkdir.
/etc --> used to store most of the config files in Linux.
/lib --> shared libraries imported
/usr --> user application and data resides.
/var --> for logs and cache data.


--> If installed via package manager it will be distributed under /usr/lib

--> If installed outside package manager it will be distributed under /usr/local

If unsure about the location of a binary, use the whereis command:

--> whereis docker

docker: /usr/bin/docker /usr/share/man/man1/docker.1.gz


--> Ubuntu and Debian --> DPKG package --> .DEB extension  --> APT package manager  --> advanced version APT-GET package manager
--> RHEL and CentOS   --> RPM package -->  .RPM extension  --> YUM package manager  --> advanced version DNF package manager


--> for yum repos --> /etc/yum.repos.d

--> for apt repos --> /etc/apt/sources.list


--> packages contains software binaries, METADATA, config files.

--> Even packages have dependencies so to install packages with dependecies we need to install it from package managers.

--> package managers --> packages + package dependecies.


--> Sequenece of steps while installing the package:

--> yum install httpd

--> runs the transaction check
--> if the package not installed in the system then checks the configured repo at /etc/yum.repos.d for the availability of the requested package.
--> it also checks the depedency packages are installed in the system or it needs to be upgraded.
--> after this transaction summary displayed for user review.
--> if user agrees then it will download the necessary packages.


Archiving:



--> To check size of file or directory --> du -sh file_name or directory_name

--> To club multiple files into a tar file --> tar -cf test.tar  file_name1 file_name2 file_name3

--> To extract tar file --> tar -xf test.tar

--> To see contents of tar --> tar -tf test.tar

--> To club multiple files into a tar file and compress size --> tar -zcf test.tar  file_name1 file_name2 file_name3

  


GREP:


--> grep second sample.txt --> searching second word in sample.txt.

-->  grep -i second sample.txt --> grep is case-sensitive to search case-insensitive add -i.

--> grep -r "third" /home/suri --> recursively checks third word in /home/suri directory.

--> grep -w second sample.txt --> It will search for Whole word and don't consider part of the word.

--> grep -A1 test sample.txt --> It will give after one line where test word matches. (1-->means no.of lines want)

--> grep -B1 test sample.txt --> It will give before one line where test word matches. (1-->means no.of lines want)



--> There are 3 standard streams in linx.

1. STANDARD INPUT  --> it will take input through STANDARD INPUT
2. STANDARD Output --> Output shown through STANDARD Output
3. STANDARD ERROR  --> Errors shown through STANDARD ERROR

--> To redirect Output to a file --> cat  "test" >> test.txt  if you use single > it will override bin/bash.

		 if you use two >> it will append
		 


--> VI editor

--> copy line --> y y

--> paste line --> p

--> delete letter --> x

--> delete line --> d d

--> delete 3 lines --> d 3 d --> delete 3 lines from first (if you want change 3 accordingly)

--> undo --> u

--> redo --> ctrl r

--> to find --> /word_name

--> to find next --> n

--> to find previous --> N 



--> Networking:


					www.     google         .com
				subdomain	  Name	      top level domain
				

--> we can use ping but its not good option always

--> Use nslookup and DIG --> but these 2 won't consider ips which are in etc/hosts.


--> To troubleshoot issue with routing we will run below command

		traceroute IP
		
--> It will show you the no. of hops(devices) b/w source and destination



--> Security and File permission:

--> The information about user is stored in /etc/passwd --> user has unique ID called UID

--> The information about group is stored in /etc/group --> group has unique ID called GID

--> We have super user account known as root and its UID is 0 and has unlimited access.



-->    Account Types:

1. User Account  --> created when users created

2. Super User Account --> Root 

3. System Accounts --> Created while OS installation (UID < 100 or b/w 500 - 1000) (ssh, mail)

4. Service Accounts --> created when you install services (nginx, tomcat)

--> TO check which users logged in at the moment --> who

--> To restrict the root login directly --> grep -i ^root /etc/passwd

--> in /etc/sudoers file --> bob --> user    %admin --> group


--> managing users:

--> To add user --> useradd username

--> Creates an entry in /etc/passwd but does not create a home directory or set a password.

--> to set password --> passwd username

--> useradd -u 1009 -g 1009 -d /home/robert -s /bin/bash -c "mercury project memeber" robert

--> -u --> UID
--> -g --> GID
--> -d --> custom home directory
--> -s --> specify login shells
--> -c --> comments

--> To delete a user --> userdel username

--> To add a group --> 	groupadd -g 1011 groupname

--> to delete a group --> groupdel groupname

--> sudo useradd -s /usr/sbin/nologin myuser  --> user with no login

--> sudo useradd -e 2025-12-31 myexpuser --> user with expiry

--> /etc/passwd --> contains user info like username UID, GID, home directory, default shell

--> passwords are stored in --> /etc/shadow

--> /etc/group --> stores info about groupname, GID , group members


Permissions:



--> chmod u+rwx test.txt  --> provide full access to owner

--> chmod ugo+r-x test.txt --> read access to owner, group and other, Remove execute access

--> chmod o-rwx test.txt --> remove all access to others

--> chmod u+rwx,g+r-x,o-rwx  test.txt -->  full access for owner, add read remove execute for group, no access for others

--> to change owner for file --> chown username file-name.

--> to change the group for file --> chgrp groupname file-name.




--> To connect to remote user using SSH --> ssh username@IP

--> to copy public key in destination server --> ssh-copy-id username@IP

--> To copy file from local to remote server -->  scp /path/to/file  username@IP:/path/to/paste

--> instead of files to copy direcory ---> scp -r /path/to/directory   username@IP:/path/to/paste



--> To restrict the access to linux OS we can use ip tables and restrict access.

--> you need to insatll ip tables --> sudo apt install iptables

--> sudo iptables -L --> to check iptables info

--> by above command you will see 

--> Chain INPUT  --> for input

--> Chain FORWARD --> for forward

--> Chain OUTPUT --> for output


--> iptables -A INPUT -p tcp -s 172.16.20.13 --dport 22 -j ACCEPT --> configuring iptables to accept connetion from specific ip and port

--> -A --> Add rule

--> -p --> protocol

--> -s --> source ip

--> --dport --> destination port

--> -j --> Action to take


--> iptables -A INPUT -p tcp --dport 22 -j DROP --> To reject connection from anywhere.

--> iptables -A OUTPUT -p tcp -d 172.16.20.13 --dport 5432 -j ACCEPT  --> allow outgoing connection to port 5432

--> iptables -A OUTPUT -p tcp --dport 80 -j DROP  --> drop all outgoing connections

--> In iptables you need to mention ACCEPT rules as well as DROP rules as well.


--> iptables -I INPUT -p tcp -d 172.16.20.13 --dport 443 -j ACCEPT --> -I inputs top of the table instead of bottom


--> To delete rule in iptable --> iptables -D OUTPUT rule no.




--> Cron jobs

--> to write cron job --> crontab -e and write the cron job

	Ex:  0 21 * * * uptime >> /tmp/system-report.txt
	
	

--> service management:

 --> systemctl
 
systemctl is a command-line tool used to manage and control systemd services and system states. It allows administrators to start, stop, enable, disable, and view the status of system services.

--> List All Active Services: --> systemctl list-units 

--> To switch to a multi-user, non-GUI mode:--> sudo systemctl isolate multi-user.target
 
-->  journalctl

journalctl is used to view logs collected by systemd-journald, the systemd logging service. 

--> View All Logs: --> journalctl
Displays all system logs, starting from the earliest logs.

--> View Recent Logs: --> journalctl -r
Displays logs in reverse chronological order (most recent first).

--> View Logs for a Specific Service --> journalctl -u servicename


--> View Logs Since Boot: --> journalctl -b

--> Follow Live Logs :--> journalctl -f



--> Storage:


--> To check list of block devices --> lsblk

--> Why it called block devices because the data  into this written as blocks or chunks.

--> MAJ - 8 means SD,   MAJ - 3 means Harddisk

--> To create ext4 file system --> mkfs.ext4 /dev/devciepath

--> the filesystem can be mounted using --> mkdir /mnt/ext4 ;    mount /dev/devciepath /mnt/ext4

--> To check mounted or not --> mount | grep /dev/devciepath

--> to make this mount available after system reboot --> add entry into /etc/fstab like below

			/dev/devciepath      /         ext4    defaults,relatime,errors=panic  0    1 ~
			
-->			file system     mountpoint    type            options                 dump  pass



-->  External storages:

  --> DAS--> Direct Attached Storage --> External storage directly attached to the host system.
  
  --> NAS--> Network Attached Storage--> Connected through Network
  
  --> SAN--> Storage Area Network --> through fiber channels
  
  
--> Mount Points

Linux uses mount points to access storage devices. A mount point is a directory (e.g., /mnt, /media, or /home) where a storage device (partition, disk, network share) is attached to the file system.

--> Commands like mount and umount allow administrators to attach or detach storage devices from these directories.



Steps to Attach, Format, and Mount an External Hard Disk


1. Physically Connect the Drive
Plug the external hard disk into your Linux machine via USB or another available connection.

2. Identify the Disk
Run lsblk to find the device name of the external hard disk:

lsblk

Look for a new device, typically named something like /dev/sdb or /dev/sdc. Make sure itâ€™s the correct disk by checking the size.


3. Partition the Disk (if needed)
If the disk is new or you want to create custom partitions, youâ€™ll need to use a partitioning tool like fdisk or gdisk.

Hereâ€™s an example using fdisk to create a new partition:

sudo fdisk /dev/sdb


Inside fdisk, follow these steps:

Type n to create a new partition.
Press Enter to select the default partition number.
Press Enter again to select the default first sector.
Press Enter once more to use the default last sector (to use all available space).
Type w to write the changes and exit.
4. Format the Partition
Now that the partition is created (e.g., /dev/sdb1), format it with a file system, such as Ext4:


sudo mkfs.ext4 /dev/sdb1


This command will create an Ext4 file system on the partition /dev/sdb1, making it ready for use.

5. Create a Mount Point


Decide where you want to mount the drive. Common mount points are within /mnt or /media. Hereâ€™s an example of creating a mount point:

sudo mkdir -p /mnt/external_drive


6. Mount the Drive
Mount the formatted partition to the new mount point:

sudo mount /dev/sdb1 /mnt/external_drive


Now, the external hard disk is accessible at /mnt/external_drive.

7. Verify the Mount


Check that the drive is mounted correctly by using df -h or lsblk:

df -h | grep /mnt/external_drive


8. Set Up Automatic Mounting (Optional)
To mount the drive automatically on boot, add an entry to /etc/fstab.

First, find the UUID of the partition with blkid:

sudo blkid /dev/sdb1


Copy the UUID and edit /etc/fstab:

sudo nano /etc/fstab


Add a line to the file in this format:

UUID=your-uuid-here /mnt/external_drive ext4 defaults 0 2


Replace your-uuid-here with the UUID from blkid, save, and close the file.

9. Test the Mounting


Unmount and remount to verify the /etc/fstab entry:

sudo umount /mnt/external_drive

sudo mount -a
If there are no errors, the drive is set to mount automatically on boot.




#########################################           SHELLSCRIPT      ##############################


--> When you run a command on a linux system OS will look for PATH configured at PATH environment variable to locate  executable script or command.

--> To add our script as a command append the path directory containing script to the end of the PATH variable.

--> export PATH=$PATH:/path/to script

--> Now we can run our script as a command in CLI




--> variables:

--> Use lower case with underscore.

--> To assign value to variable:

		mission_name=mars-mission

--> to use that variable --> $mission_name




--> Command Line Arguments:

--> create-script  space_mission   
		|              |
	    $0            $1	

--> when you gave one value in CLI we can access it by its position

	create-script.sh
	
	mission_name=$1
	
-->  $1 replaced by the space_mission value


--> To ask the user to enter or prompt for input  (best usable when manual intervention needed)

	create-script.sh
	
	read -p "Enter the mission_name:" mission_name
	
	--> -p used for to prompt or show the message to the user.
	
--> When you run above script it will ask the user to input --> Enter the mission_name:  Then when you enter it will run the script.


--> Arithmatic operations:

  --> expr 6 + 3
      ans: 9
  --> expr 6 \* 3  -->  we need to escape because * is reserved keyword in regex.
	  ans: 2

--> A=6  B=4   --> expr $A + $B

--> echo $((A+B))

--> echo $((A*B))

--> To get floating point --> A=10 B=3  --> expr $A / $B | bc -l  --> 3.333




--> Flow Control:




	


--> if condition you need to write the condition in [] only and you need to give gap b/w bracket variable and operator as well.

--> = used to compare only strings

--> To compare numeric values use  -eq  -ne  -gt  -lt

--> we can use double [[]] but this is adavanced version and works only in bash
	
	Ex:  [[ "abcd" = *bc* ]] --> if abcd contains bc (true)
		 [[ "abc" = ab[cd] ]] --> if 3rd character of abc is c or d.

--> [cond1] && [cond2]  =  [[ cond1 && cond2 ]]

--> [cond1] || [cond2]  =  [[ cond1 || cond2 ]]

--> [ -e FILE ] --> if file exists

--> [ -d FILE ] --> if file exists and is a directory

--> [ -s FILE ] --> if file exists and has size greater than 0

--> [ -x FILE ] --> if file is executable

--> [ -w FILE ] --> if file is writable


--> if [  ] 
	then
	    code
	fi
	

--> for loop

	for i in list
	do 
	  code i
	done
	
--> if there is lot of big list we can store that list in a variable and apply in for like below

	Ex: list stored in mission.txt file

	for i in `cat mission.txt`                 
	do 
	  code i
	done
	
	OR
	
	
	for i in $(cat mission.txt)
	do 
	  code i
	done
	
	
--> for i in {0..100}  --> it will take upto 100
	do 
	  code i
	done
	
--> real life use case

	for server in `cat server.txt`
	do 
	 ssh $server "uptime"
	done
	


--> while loop:

		while [ $status = "true" ]
		do 
			sleep 2
			status=status
		done
		
	if [ $status = "failed" ]
	then
		echo failed
	fi
	
--> If the status true it will run the loop until it changes the value to true to failed.

--> Real life example

		while true
		do 
		  echo "1. shutdown"
		  echo "2. restart"
		  echo "3. Exit"
		  read -p "Enter the choice:" choice
		  
		  if [ $choice -eq 1 ]
		  then
		     echo shutdown
		  elif [ $choice -eq 2 ]
		  then 
		     echo restart
		  elif [ $choice -eq 3 ]
		  then
		     break  --> exit out of the loop
		  else
		     continue   --> will go to top and re run the whole
		  fi





-->  SHEBANG


--> #!/bin/bash --> Always start your script with shebang


--> Exit Codes

--> 0 means --> successful

--> greaterthan 0 means --> Failed

--> To check exit code --> echo $?

--> In a shell script always return a exit code.



--> Function -->  function function_name(){
					Block of code

					}


--> function add(){

	echo $(( $1 + $2 ))

	}
	
sum=$( add 3 5 )
echo $sum



--> basic function

evenfunction() {
        result=$(( $1 + $2 ))

        if ([ `expr $result % 2` == 0 ])
        then
                echo $result
        else
                echo "Its ODD VALUE"
        fi
}

evenfunction $1 $2


--> If condition

a=$1
b=$2

if (( (a + b) % 2 == 0 ))
then
        echo "EVEN VALUE"
else
        echo "ODD VALUE"
fi


if (( ( ($1 + $2) % 2 == 0 && ($1 + $2) % 5 == 0 ) || ( ($1 + $2) % 3 == 0 ) ))
then
        echo "EVEN"
else
        echo "ODD"
fi


if (( $1 < $2 ))
then
        echo "$1 is lessthan $2"

elif (( $1 == $2 ))
then

        echo "$1 is equal to $2"
else
        echo " $1 is greaterthan $2"
fi


--> loop

for i in {1..50}
do
        if (( ( $i % 2 == 0 || $i % 5 == 0 ) && ( $i % 15 != 0 ) ))
        then
                echo $i
        fi
done


for i in {1..50}
do
        if (( ( i % 2 == 0 )  || ( i % 5 == 0 ) && ( i % 15 != 0 ) ))
        then
                echo $i
        fi
done





##########################################    Golang ############################################



--> Golang is a opensource programming language provided and supported by Google.

--> Golang is a compiled programming language.

--> In compiled programming language it will take whole file at once converts to binary file.

--> In interpreted programming language it will excute code line by line and converts to machine code.

--> It means when you write a code it is translated into language which understandable by computer.

-->  

package main  --> Every go program must start with package.

import "fmt"   --> fmt -- shortform of format 

func main() {           --> You don't need to explicitily call the function because its a main function
	fmt.Println("Hello World")
}

--> Every executable program must contain main package and main function.

--> main Package:

--> The main package is required for defining the entry point of an executable program in Go.
--> Any Go program meant to produce a runnable binary must have a main package.
--> If a program doesn't use the main package, it cannot be built as an executable but can still be compiled as a library (e.g., package utils).


--> main Function:

--> The main function is the entry point of the program. It is where execution starts when you run the program.
--> The Go runtime automatically looks for the main function in the main package to begin execution.
--> If the main function is missing in the main package, the compiler will throw an error.


###################################Data Types and Vars:##############################################


--> Go (Golang) is a statically typed programming language.

	var x int = 10
	fmt.Println(x)
	
	Here, x is explicitly declared as an int, and only integer values can be assigned to it.
	
	
-->	Type Inference: Go has type inference using the := syntax, but it is still statically typed:

	x := 10  // The compiler infers that x is of type int
	fmt.Println(x)
	
	Even though the type isn't explicitly written, the type of x is determined at compile time and cannot change.
	
	
--> Integers:(int)

	uint --> means "Unsigned integer" --> 0 and positive numbers
	int -->  means "Signed integer" --> Negative and positive numbers
	
--> float has 2 types float32 and float64

--> String defined by using "string"

--> Boolean defined by using "bool"

--> how to declare a variable

	var <variable-name> <data-type> = <value>
	
	var s string = "Hello"
	
	var number int = 568
	
	var f float64 = 77.90
	
--> fmt.Println("Hello") --> prints the input and go to New line.

--> format specifier: (fmt.Printf)

	%v --> (print the value of the variable)
	
	var name string = "Kodekloud"
	fmt.Printf("Nice, at %v", name)
	
	O/P: Nice, at Kodekloud

--> %d --> fomats decimal integers

	var grades = 23
	fmt.Printf("Marks: %d", grades)
	
	O/P: Marks: 23
	
--> package main

import "fmt"

func main() {
var name string = "Joe"
var i int = 78
fmt.Printf("Hey, %v! you have scored %v/100 ", name, i)
}

O/p: Hey, Joe! you have scored 78/100

--> When you write printf only then you can use below values.

--> %T --> data type of the value

--> %c --> characters

--> %q --> quoted characters/

--> %t --> true or false

--> %f --> floating numbers

--> %s --> String

--> var s,t string = "foo", "bar"

--> var ( s string = "Hello"
		  i int = 35)
		  
--> Short variable declaration:

	s := "Hello world" --> when you write like this you don't need to declare var and data type
	

--> func main() {
	country := "India"
	fmt.Println(country)

	{
		country = "USA"
		fmt.Println(country)
	}
	fmt.Println(country)

}

O/P:  
India
USA
USA


--> func main() {
	country := "India"
	fmt.Println(country)

	{
		country := "USA"   (Carefully observe declaration of new variable)
		fmt.Println(country)
	}
	fmt.Println(country)

}

O/P:  
India
USA
India



--> Zero values --> When you declare a variable and don't assign any values.

	bool --> false
	int --> 0
	float --> 0.0
	string --> ""
	pointers, functions, interface, maps --> nil
	
--> 	func main() {

	var name string 
	var age int

	fmt.Print("Enter your name and age:")
	 count , err := fmt.Scanf("%s %d", &name, &age)
	fmt.Println("count", count)
	fmt.Println("err", err)
	fmt.Println("name", name)
	fmt.Println("age", age)
	fmt.Println("Hello", name, "you are", age, "years old")
	
	}
	
O/P: Enter your name and age:Suri Sur
count 1
err expected integer
name Suri
age 0
Hello Suri you are 0 years old

--> 	age := 25
	name := "John"

	fmt.Printf("Name:%v Type of %T\n",name, name)
	fmt.Printf("Age:%v Type of %T\n", age, age)
	
O/p: Name:John Type of string
Age:25 Type of int



--> strconv --> which is a package which converts string to integer to vice-versa

		--> Itoa --> integer to string --> return one value which is passed

		--> Atoi --> string to integer --> return 2 values integer and err

--> 	age := 25
	s := strconv.Itoa(age)
	fmt.Printf("%q", s)
	
O/p: "25"

--> 	age := "25"
	i , err := strconv.Atoi(age)
	fmt.Printf(" %v Type of %T\n", i, i)
	fmt.Printf(" %v Type of %T", err, err)
	
O/P:  25 Type of int
 <nil> Type of <nil>
 
 
--> const value never change:

	const <const-name> <data-type> = value
	
-->  const age = 12 --> Untyped constant --> type infered when compiling

--> const age int = 12 --> typed constant --> explicitily mention type

--> when you declare a constant at the same time only you need to assign value

	const name string --> give you error.

--> Even shorthand declaration is also not applicable.

	const name := "suri"
	

--> #################################### Operators and control flow: #########################################

	--> if (condition){
	
		}else {
		
		}
		
--> else part need to start right after the If parenthesis otherwise it will throw error.

--> 	for i := 0; i <= 5; i++  {
		if (i == 3){
			break
		}
		fmt.Println(i)
	}
	
O/P: 0 1 2

--> 	for i := 0; i <= 5; i++  {
		if (i == 3){
			continue
		}
		fmt.Println(i)
	}
	
O/P: 0 1 2 4 5


--> ################################  Arrays and Slices and maps:  ########################################


--> Arrays: array is a collection of similar data elements stored at contiguous memory locations.

	contiguous means it store data continually one after another.
	
	
-->	How to declare an array:

	
	var <array-name> [<size of the array>] <datatype> = [<size of the array>]<datatype>{}
	
	var rollno [3] int = [3]int{10, 20, 30}
	
		rollno := [3]int{10, 20, 30}
		
		rollno := [...]int{10, 20, 30}
		
--> To check the length of the array --> len(array)

--> To check the capacity of the array --> cap(array)

	for index, element := range array-name {
	
	fmt.Println(index, "=>", element)
	
	}
	
--> Multi dimensional array:

	arr := [3][2]int{{2,4}, {4,16}, {8,64}}
	fmt.Println(arr[2][1])
	
	O/P: 64
	
	
--> Slice : continuous segment of underlying array

--> elements can be added or removed

--> If you have slice of length 2 it has capacity of 3.

--> When you add morethan 1 varaible to slice which greater than capacity of above then it will create new slice with double capacity and length equals to previous variables + newly added variables.

--> How to declare a Slice:

	<slice-name> := []<data-type>{values}
					|
					You don't need to specify the length.
					
--> <slice-name> := make([]<data-type>, length)
					
--> How to create a slice from array:

	slice := arr[ start-index : end-index ]
	
--> slice is nothing but reference to the array 

--> When you change slice it will affect in the array as well.


--> How to append to slice:

	slice1 := append(slice, element-1, element-2)
	
	slice2 := append(slice, anotherSlice...)
	
--> copying from slice

	num := copy(dest_slice, src_slice)
	
	num value returns the number of variables added to the dest_slice
	
--> Maps: unordered collection of key/value pairs.

--> implemented by hash tables

--> provide efficient add, get and delete operations.

--> How to declare a map:

	 <map-name> := map[<key-data-type>]<value-data-type>{key-value-pair}
	
		codes := map[string]string{"en": "English","fr": "french"}
		
		codes["en"] --> English
		
		
--> map returns 2 values value , found

	codes := map[string]string{"en": "English","fr": "french"}
	
	value, found := codes["en"]
	fmt.Println(found, value)
	
	value, found := codes["hh"]
	fmt.Println(found, value)
	
	true 1
	false 0
	
	
-->  How to add key value pair in map

	codes := map[string]string{"en": "English","fr": "french"}

	codes["it"] = "Italian"
	
--> How to update key value pair:

	codes["en"] = "English Language"
	
--> How to delete key value pair from map:

	delete(codes, "en") --> delete(map-name, "Key")
	
--> for key, value := range codes{
	fmt.Println(key "=>" value)
  }
  
--> how to delete using for loop:

	for key, value := range codes{
	delete(codes, key)
  }
  
  
--> ######################################  Functions  #################################################


--> help us divide a program into small manageble, repeatable and organisable chunks.

--> func <function-name>(<params>) <return-type> {
			
			//////// Body //////
			return
	}
	
--> func addNumbers (a int, b int) int {
		
		sum := a + b
		
		return sum
		
	}
	
	addNumbers(2,3) --> Whatever the values we are passing here are "arguments"
	
	a and b are the "parameters"

--> We can return two values so we need to mention 2 types in func declaration


func addNumbers (a int, b int) (int, int) {
		
		sum := a + b
		diff := a-b
		
		return sum, diff
		
	}
	
	sum, diff := addNumbers(2,3)
	fmt.Println(sum, diff)
	
--> func addNumbers (a int, b int) (sum int, diff int) {
		
		sum = a + b
		diff = a-b
		
		return
		
	}
	
	sum, diff := addNumbers(2,3)
	fmt.Println(sum, diff)
	
--> variadic function --> which means we can pass varying variable at the last by using ...

	func test (a int, b int, c ...int) int {
	
	sum := a+b
	for index, num := range c {
		fmt.Println(index, num)
		sum += num
	}
	fmt.Println(sum)
	
	}
	
	test(1,2,3,4,5)
	
--> you can't directly access c ...int. you need to loop through it and take the single value.
	
--> You can use _ to avoid return value

func addNumbers (a int, b int) (sum int, diff int) {
		
		sum = a + b
		diff = a-b
		
		return
		
	}
	
	sum, _ := addNumbers(2,3)
	fmt.Println(sum)
	
	
	
--> Recursive functions:

	function calls itself by direct or indirect means.
	
	function keeps calling itself until base condition is met.
	
--> Anonymous function:

	used for short-term use
	
	x := func(a int, b int) int {
	
	return a * b
	
	}(20,30)
	
	fmt.Pritnln(x)
	

--> High order functions

	recieves function as argument or returns function as output.
	
--> defer statement --> it delays the execution of function until the surrounding function returns.

	func main() {
	
	printName("Joe")
	defer printrollno(23)
	printAdress("street-32")
	
	O/P:
	
	Joe
	street-32
	23
}	


#####################################  Pointers  ########################################


--> a pointer is a variable that holds the address of the another variable.

--> They point where the memory is allocated and provide ways to find or even change the value located at the memory location.

-->  x := 77

--> to get the address of x --> &x = 0x0301 

--> to get the value at that address --> *address --> *0x0301 = 77

--> Declaring and Initializing pointers:

	var <pointer-name> *<data-type> = &<variable-name>
	
	short-hand declaration -->  <pointer-name> := &<variable-name>
	
--> There are 2 ways to pass arguments to the function

	Passing by value
	Passing by reference
	
--> Passing by value:

	Function is called by directly passing the value of the variable as an argument.
	
	when it goes to function the exact copy of the variable is created and if you change anything it will affect copy and not the real one.
	
		func modify(s string){
		
			s = "Hello"
		
		}
		
		func main(){
		a := "World"
		modify(a)
		}
	
	
	
	
	
	
--> Passing by reference:

	the address of the variable is passed into the function call as the actual parameter.
	
	When you change the value it will actually change the original value.

		
		func modify(s *string){
		
			s = "Hello"
		
		}
		
		func main(){
		a := "World"
		modify(&a)
		}
		
--> slice, maps are by default passed by reference


		func main(){

		my_map := map[string]int{"John": 25, "Jane": 30, "Bob": 40}

		fmt.Println(my_map)
		modify(my_map)
		fmt.Println(my_map)
		}


		func modify(x map[string]int){
			x["John"] = 50
		}

		map[Bob:40 Jane:30 John:25]
		map[Bob:40 Jane:30 John:50]


--> 

		func main (){

		slice := []int{1,2,3}
		fmt.Println(slice)
		modify(slice)
		fmt.Println(slice)
		}

		func modify(x []int){
			x[0] = 50
		}
		
		
-->  ################################  structs, methods & interfaces ############################################


--> struct: 

	user-defined data type
	
	its a structure that groups together data elements
	
	its provide a way to reference a series of grouped values through a single variable name
	

--> declaring and intialising:


	type <struct-name> struct {
	
			// values
	}
	
	
	type Student struct {
	
	Name string
	Roll no int
	marks []int
	grades map[int]string
	
	}

	
-->  test := new(Student) --> another way to initialize struct


--> st := Student{Name: "Suri", Roll: 10} --> another way to initialize struct

	fmt.Printf("%+v", st)

--> How to pass structs to function

	type Student struct {
	
	Name string
	Roll no int
	marks []int
	grades map[int]string
	
	}
	
	
	func test(s Student){  --> this is pass by value
	
	fmt.Println(s.Name)
	
	}
	
	
	func test(s *Student){  --> this is pass by reference
	
	fmt.Println(s.Name)
	
	}
	

--> Methods:

	Its same like function by just adding an extra parameter section immediately after the func keyword that accepts func keyword.
	
	This argument is called a reciever.
	
	func (<reciever>) method-name (<parameters>) return-params {
	
	
	
	}
	
-->	How to use
	
	
main(){
	s := Test{Name: "Suri", Age: 25}
fmt.Println(s.Greet())
fmt.Println(s.calcAge())

}

type Test struct {
	Name string
	Age int
}

func (t Test) Greet() string{
	return "Hello" + t.Name
}

func (t *Test) calcAge() int  {
	t.Age += 1
	currentAge := t.Age
	return currentAge
}
	
	
	
	
	
--> Interfaces:

	An interface specifies a method set.
	
	Its like a blueprint for method set.
	
	They specify set of methods, but do not implement them.
	
	type <interface-name> interface {
	
		// method signatures
	
	}
	
	
	type Fixeddeposit interface {
	
	getRateofInterest() float64
	calcReturn() float64
	
	
	}
	
	
--> A type implements an interface by implementing its methods.

--> go language interfaces are implemented implicitly.

--> implementing interfaces:

main(){

s := Test{Name: "Suri", Age: 25}
var c check
c=s
fmt.Println(c.Greeting())
fmt.Println(c.calcAge())

}

type check interface{
	Greeting() string
	calcAge() int
}

type Test struct {
	Name string
	Age int

}

func (t Test) Greeting() string{
	return "Hello " + t.Name
}

func (t Test) calcAge() int  {
	t.Age += 1
	currentAge := t.Age
	return currentAge
}



##################################               PYTHON               #################################################

--> Python is an interpreted language and dynamic laguage as well.

--> In order to understand by the system python interpreter takes the code and parse it line by line and then converts to machine understandable code.

--> Its a backend programming language.

--> To check version --> python3

--> for python files .py extension should be must

--> To excute a python file --> python3 file_name.py

--> PI = 3.14 --> To declare a constant, its not really constant but it tells other programers that the value of PI shoudn't change.

--> iq = 100

--> user_age = iq/5 --> The whole statement is known as statement. statement is nothing but performs an action.

--> Expression is nothing but which gives a result. iq/5 gives a result.


--> We can create a long string in following way.

	test_string = '''
	
					WOW
					Hi
					HELLO
					
					'''
					
--> You need to start with 3 single quotes and ends with 3 single quotes.


--> Formatted strings

	name = John
	age = 22
	
	print(f'hi {name}.You are {age} years old')
	
	Another way
	
	print('hi {}.You are {} years old'.format(name, age))
	
--> string index --> When you store a string it will store in indexes.

	test = "me me me"
			01234567
			
--> You can access each and every part of the string

	print(test[0]) --> m
	print(test[7]) --> e
	
	print(test[0:4]) --> me m --> It won't include end index.
	
	print(test[0:4:2]) --> m m --> You can skip or step over by using after the end index.
	
	num = '012345'
	
	print(num[-1]) --> 5 --> If you specify - it will start from end.
	
	print(num[::-1]) --> 543210
	
--> Strings are immutable in python

	num = '012345'
	
	num[0]=9 --> You can't do that.
	
--> 


################### Literals ##########################


--> Literal types:

	1. Integers
	2. Floating point numbers
	3. Strings
	4. Booleans


--> print is a builtin function in python.

	print("Hello \n How are you")
	
	o/p:  Hello 
			How are you
			
--> print("Hello", end="")
	print("How are you")
	
	O/P:  HelloHow are you
	
--> Instead of new line it will add "" to the next line and print in the same line

--> print("Hello", "python", "programer",  sep="-")
	
	O/P:  Hello-python-programer
	

--> Operators:

	exponential --> **  ex: 2**3 = 8
	
	multiplication --> *
	
	for division --> /  --> the result always will be float
	
	if you want integer for integer values use --> //
	

--> We can mention negative values by unary operator -->  -6, -8

--> Priority order:

	+  -  --> Unary operators
	** --> exponential
	* / // % --> multiply, division, modulo
	+ -  --> Addition, subtraction
	
--> valid variable names in python --> agent_state, agentState, _agent_state

--> there is no need for special declaration keyword required like var, let and const

--> To prompt user for input we will use input method:

	age = input("How old are you?"_)
	How old are you? 22
	print(age)
	
	O/P: 22
	
--> In input method the value is always string even it is integer its always string

--> To typecast string to int --> int(age)


--> In python with strings we can use + and * operators

	print("Hi" + "Hello") --> HiHello
	print("Hi" * 3 ) --> HiHiHi
	
--> print(int("22")) --> 22 integer

--> print(str(22)) --> 22 string

--> conditional statements:

	if condition:
	  
	  print("Ok")
	  
--> Indentation is very important.


	if condition:
	  
	  print("Ok")
	  
	elif condition:
	  
	  print("Not Ok")
	  
	else:
	  
	  print("Not Ok")
	
--> loops:

	while true:
		body
		
	else:
	  body
	  
--> for i in range(10)
	  
	  print(i)
	  
--> you can specify the range as well

	for i in range(2,5)
	  
	  print(i)
	  

--> In python logical and , or not are different from other progarmming languages:

--> we use direct names like and , or , not

	age1 = 24
	age2 = 16
	
	if(age1 >= 18 and age2 >= 18 ):
	  print("Both adults")
	elif(age1 >= 18 or age2 >= 18:
	   print("One adult")
	else:
	  print("Both children")
	  
	  
--> is_hungry = false
	if(not is_hungry):
	  print("you are not hungry")
	


	
--> Bitwise operators:

	They work on bits

		& --> conjuction --> if both values are 1 then returns 1
		
		| --> Disjunction --> if atleast one value is 1 then returns 1
		
		~ --> Negation --> if atleast one value is 1 then returns 1 and if both values are 1 then returns 0
		
		^ --> Exclusive --> if 1 returns 0 and if 0 returns 0
		
		

--> ####################################  Lists ###################################

--> list is a mutable datatype:


	countries = ["USA", "UK", "INDIA"]
	
--> len(countries) ==> 3

--> To delete --> del countries[0]

--> List Methods 

list.append()   

list.insert()  

list.sort()  --> It will modify the original array.

list.reverse() --> It will modify the original array.

list.pop()

list.remove()

list.clear()

list.index("value", startindex, endindex) --> you can check the value where it is.


--> All list methods doesn't create or return new list they just modify the existing list.

--> list.pop() will give you poped value as return.

--> countries.append("Spain") --> ["USA", "UK", "INDIA", "Spain"]

--> It will just append the value to the existing list but when you assign it to new list it won't assign.

	new_list = countries.append("Spain")
	print(new_list) --> None --> It won't produce a value.
	new_list2 = countries 
	print(new_list2) --> ["USA", "UK", "INDIA", "Spain"]

--> countries.insert(2, "Italy") --> ["USA", "UK", "Italy", "INDIA", "Spain"]

-->  We can swap values in pyhton
	
	
	countries[0], countries[1] = countries[1], countries[0]
	
--> [ "UK", "USA", "Italy", "INDIA", "Spain"]

--> ages = [56, 72, 24, 46]

	ages.sort()
	
	print(ages)
	
	[24,46,56,72]
	
--> sort will modify the original array

	ages = [56, 72, 24, 46]

	ages.reverse()
	
	print(ages)
	
	[46,24,72,56]
	
	
--> pop will remove the last index but you can specify which index to remove

	ages = [56, 72, 24, 46]
	
	ages.pop()
	
	print(ages) --> [56, 72, 24]
	
	ages.pop(0)
	
	print(ages) --> [ 72, 24]
	
--> we can also have remove but it will remove the values we specified

	ages = [56, 72, 24, 46]
	
	ages.remove(46)
	
	print(ages) --> [56, 72, 24]
	
	
--> we can check the value of index

	ages = ["a", "b", "c", "d"]
	
	ages.index("b", 0, 2) --> B is which values need to check , 0 --> from where , 2 --> till where need to check.

		o/p : 1 

-->  print(i in "hello i am suresh") --> True


--> basket = ["x","a", "b", "e", "c", "d",]

	print(sorted(basket)) -->  it won't change the origginal array.

	print(basket)
	
	O/P: ['a', 'b', 'c', 'd', 'e', 'x']
		 ['x', 'a', 'b', 'e', 'c', 'd']

-->  List unpacking

	a, b, c = [1,2,3]
	
	print(a)
	print(b)
	print(c)
	
	O/P: 1,2,3
	
	
-->  a, b, c, *other = [1, 2, 3, 4, 5, 6, 7, 8, 9]
	
	print(a)
	print(b)
	print(c)
	print(other)
	
	O/P: 1 2 3 [4,5,6,7,8,9]
	
--> slicing a list means creating a new lsit --> listname[start_index : end_index]


--> ######################  Functions ###########################

--> def --> means definition it tells python that we are creating a function

	def input_number():
	   return int(input("Enter a number"))
	   
-->  we can also set default parameters to the functions

		def input_number(num = 10):
	   return int(input("Enter a number")) * num
	   
--> In functions if you return you will have that value and if you don't return you will have "None"

--> if you want to access a variable which is defined inside a function scope you can use global keyword

def input_number():
  global own_num
  own_num = 50
  result = own_num * 2
  return result

print(input_number())
        
print(own_num) 


--> first you need to call the function and after that you can  access global variable.




####################### Dictionaries & Tuples ############################3


--> tuple --> tuple is a immutable datatype

	tuple1 = (1,2,3)
	print(tuple1)
	
	o/p: (1,2,3)
	
--> tuple can contain any kind of datatype:
	
	age = 22

	tuple2 = (1,"Sara", age , (1,2))
	
	
	
	
--> tuple has only 2 methods

	my_tuple = (1,2,3,4,5,5)

	count --> print(my_tuple.count(5)) --> 2 --> It will count the 5 no.of occurances
	
	index --> print(my_tuple.index(4)) --> 3 --> It will give you index.
	

	
--> Sets:

	--> Unordered collection of unique objects.
	
	--> Duplicates are not allowed.
	
	--> Sets are mutable.
	

	my_set = {1,2,3,4}
	
--> we can convert a list into a set.

	list1 = [1,2,3,4,4]
	
	print(set(list1)) --> [1,2,3,4]
	


	
	
	
	
--> Dictionaries : consist of key : value pairs

-->  we can't iterate over the below dictionary

	usernames = {
	
	"lydia" : "lydiahallie",
	"sarah" : "sarah123"
	}
	
--> we need to access by key name like shown below

-->	print(usernames["sarah"])

--> We have methods for Dictionaries --> dictionary.keys()  dictionary.values()  dictionary.items()

--> print(usernames.keys()) --> (['lydia', 'sarah']) -->  we can iterate over this 

--> print(usernames.values()) --> (['lydiahallie', 'sarah123'])

--> print(usernames.items()) --> ([('lydia', 'lydiahallie'),('sarah', 'sarah123')])

-->  To add we will use update method --> usernames.update({"chole": "chloe123"})

--> TO delete --> del usernames["sarah"]

--> To remove all you can use clear method --> usernames.clear()

--> To remove last item --> usernames.popitem()

--> To copy --> usernames.copy()





###########################  Exceptions ###############################


--> You can handle exceptions using try exceptions block

	try:
	  name = "Lydia"
	  print("My name is " + naem)
	  
	except:
	  print("Something went wrong")
	  
	print("All done")
	
o/p :    Something went wrong
		 All done

--> By using raise keyword you can raise exceptions





#############################################    GIT   ##################################################



-->  Open-source distributed version control system.

-->  When you initialize git it will create .git folder.

-->  Storing changes in local git repo is called commit.

-->  When you do git commit it will internally store the copy of those files in .git folder.

--> Working area contains 2 parts --> 

	1. untracked section -->  you haven't done git add , the file is not even under tracking
	
	2. modified section  --> you did git add and git commit,  you revert back the changes to the working area

-->  When file in staged area and when you changed the same file , You can discard changes in working directory

		git restore filename.txt
	
--> To come to modified section the file should be atleast commited once.
	
--> To remove the file from staging area to working area untracked section

	git rm --cached filename.txt
	
--> if you don't want to commit or push some files forever, you can put those files in .gitignore folder.

--> HEAD is nothing but where you are right now in git repository.

--> When you do git merge there are 2 types of merges git can perform.

1. fast-forward

	--> when the current branch has no extra commits compared to the branch that we are merging.
	
	--> It won't create extra merge commit.
	
2. no-fast-forward
	
	--> when the current branch has extra commits compared to the branch that we are merging.
	
	--> It will create extra merge commit.
	
	
--> You can explicitily do No fast-forward even if the current branch has no extra commits

	git merge --no-ff feature
	
--> git fetch origin remote-repo-branch-name --> it fetches the latest changes to local repo

--> In our local repo git points to previous master it didn't know about the latest changes

--> To point our git to latest and updated commit --> git merge origin/remote-repo-branch-name

--> git pull origin remote-repo-branch-name  --> It will directly pull latest changes and points our git to latest commit or updates.

--> Fork is nothing but exact copy of the git repository when you change anything in Forked git repo, it won't effect 
original repo.

--> If you want to merge the changes you did back to the main repo, you need to raise a pull request.


--> To check which branches are pushed to remote --> git branch -a

--> git rebase will take your new branch commits to the end of the main branch tip.

--> In main branch you can't see your feature branch commits, but in feature branch you can see main branch commits as well.


You can reset in 2 ways


--> git reset --soft commit-ID --> reset commit history to the specified commit-ID

	if you use --soft flag, we can still access the changed file.Type git status you can find the changes.
	
--> git reset --hard commit-ID 

	if you use --hard flag, it will be deleted permanently.
	

--> When you accidentally deleted the commit by git reset --hard commit-ID, but the commit is as much as important. So you can do 

	 git reflog --> It will show all the reset and commited operations.
	 
	 then get the commitID and do
	 
	 git reset --hard commit-ID
	 
	 


##########################################  JENKINS ####################################################




--> What files need to backup when you are backing up jenkins

	JENKINS_HOME
	
--> In JENKINS_HOME we have 2 kind of files

	1. Configuration files --> config.xml
	2. jobs --> all our pipelines
	
--> For backup we can use thin backup

--> In thinbackup plugin we have backup and restore as well.


-->  pipeline {    --> pipeline --> The task you are trying to accomplish
		 agent any --> Build agent
		  
		 stages {
			 stage("Build") {
				 steps {
					 echo "Building"
				 }
			 }
			 stage("Test") {
				 steps {
					 echo "Testing"
				 }
			 }
			 stage("Deploy") {
				 steps {
					 echo "Deploying"
				 }
			 
			 }
		 }
	}


--> You can use different images at different stage


pipeline {  
		 agent none 
		  
		 stages {
		  stage("Back-end") {
			agent {
				docker {image 'maven:3.8.1}	
			}
		    steps {
			sh 'mvn --version'
			}
		}
		   stage("Front-end") {
			agent {
				docker {image 'node:16-alpine}	
			}
		    steps {
			sh 'node --version'
			}
		}
	}
}

--> Through webhook if there is any change detected in repo git will send notification to Jenkins.


--> To write a shared library in github create a folder under the name of vars and write the code for shared library.

--> Its in groovy script.

The extension should be .groovy

sample code below


helloWorld.groovy


def call(){

 sh 'echo Hi from Devops team'

}



--> Because by default jenkins searches for shared library in vars folder.

--> In jenkins in configure system in global pipelines you need to configure shared library path.

--> To import the shared library in pipeline code at the top write below

	@Library("shared-library-name") _
	
--> That " _" --> means everything under the shared library.


 @Library("shared-library-name") _	
	pipeline {  
		 agent none 
		  
		 stages {
		  stage("Back-end") {
			agent {
				docker {image 'maven:3.8.1}	
			}
		    steps {
			helloWorld()  --> You need to mention file name not the function name.
			}
		}

		}
	}




###################################################  Docker #########################################



--> Containers share same os kernel.

--> Docker uses LXE containers.

--> When you install docker on windows and try to run a container which is based on linux, it actually runs a linux container runs on linux virtual machine which is running on windows.

--> 			Docker 


							seperate libs      seperate libs
								application    application
	
								container-1    container-2

										 Docker
										
										   OS
									
									Hardware infrastructure
									
									
									
--> Virtual Machine


								VM-1                 VM-2
			
						seperate libs & OS      seperate libs & OS
							application             application
										
										   Hypervisor
									
									Hardware infrastructure
									
				
--> To execute a command in a running container

		docker exec container-ID/Name cat /etc/hosts --> Command to run
		

--> To set the environment variable while running the docker

	docker run -e APP_COLOR=green image-name
	
	
--> APP_COLOR is environment varible which is in code.

--> -e APP_COLOR=green setting the environment variable

-->	To inspect the container

	docker inspect containerID
	
-->  Dockerfile


FROM Ubuntu

RUN apt-get update
RUN apt-get install python

RUN pip install flask
RUN pip install flask-mysql

COPY . /opt/souce-code

ENTRYPOINT FLASK_APP=/opt/source-code/app.py flask run 

--> Everything on the left side in capitals are Instructions

--> Right after the instructions are arguments

--> ENTRYPOINT allows us to specify a command that will be run when the image run as a container.

--> Each and every line creates new layer

--> Each layer only remembers previous layer

--> If you want to check the layers

	docker history image-name
	
--> In docker below commands creates new layer. Only these 3 commnads will create new layers.

	RUN
	COPY
	ADD
	
	EXPOSE          All these createa a new layer but won't affect the filesystem.
	WORKDIR
	VOLUME
	USER
	LABEL
	
--> In docker below commands won't creates new layer.

	CMD
	ENTRYPOINT

	
--> While you are pushing the docker image which doesn't contain your username, it defaultly pushes it to docker library.

Everyone don't have the access to docker default library, thats why you will get Access denied error.



--> docker run ubuntu sleep 5 

--> sleep 5 is a command which overrides the command value which is in ubutnu docker file.

--> While you are writing CMD

	CMD["command","param1"]
	
	CMD["sleep","5"]
	
--> If there are 2 ENTRYPOINTs in Docker file it will consider the latest ENTRYPOINT.

--> If there are 2 CMDs in Docker file it will consider the latest CMD.

--> 

	Dockerfile
	
	FROM centsos:latest

	ENTRYPOINT["yum" ,"-y", "install"]
	CMD["git"]
	
--> docker build . -t test

--> docker run test httpd

--> It will be added to ENTRYPOINT, CMD will be ignored.

--> docker run test

--> the CMD will not be ignored, because no arguments will be passed.


--> Docker compose

 --> to run multiple images
 
 
     
	docker-compose.yml
	
	version: 2
	services:
	     web:
		    image:"ubuntu:latest"
			
		 database:
		    image:"mongodb"
			networks:
				back-end
			
		 messaging:
		    image:"redis:alpine"
			networks:
				back-end
		 vote:
		    image:"voting-app"  --> In this place you can use build: ./vote -->It will check the code and run the image
			ports:
			  - 5000:80
			networks:
				front-end
				back-end
	     result:
		    image:"result-app"
			ports:
			  - 5001:80
			depends_on:
			  - redis
			networks:
				front-end
				back-end
					
	networks:
		front-end
		back-end
			
			
-->  To run the compose file --> docker-compose up

--> When you are running the container you can name the container

	docker run -itd --name=test redis
	
	
--> To links 2 container we will use links "--links"

     docker run -itd --name=vote -p 5000:80 --link redis:redis  voting-app
	 
	 
--> What internally it will do is in /etc/hosts file in voting-app it will create an entry with Ip of redis container
	 
--> The first redis refers to the name of the already running container.

--> The second redis is the alias that the new container (voting-app) will use to refer to it.


--> we have 3 docker compose versions

--> The main diff b/w version 1 and version 2 is in version 2 we will write services and under servcies we will write.

--> If you want to run with version 2, you need mention version 2

--> In version 1 when it runs the conatiners it will attach all the containers to the default bridge network.

--> In version 2 it will create a dedicated brige network and attach all the containers to that network.So you don't need to use links in version 2.

--> version 2 introduces depends_on feature.whatever we mention on depends_on it will wait till that runs.

--> version 3 supports docker swarm

--> for networks you need to add the network at root level adjacent to the services

--> When you install docker docker-compose by defaultly will not download, we need to install explicitily.



-->  Docker storage:


--> when you install docker it will create below folder structure

	/var/lib/docker --> it contains images, containers, volumes
	
--> This is where docker stores all its data

--> When you are building image all the layers are Read only is a image layer -- Read only

--> When you are running image it will create container layer and is Read/writable layer on top of the image layers

--> It will store the data such as logfiles, temporary files generated by the container.

-->  the life of the container layer as long as the container alive.

--> Docker will copy all the files from image layer to container layer.So whren you change the soureccode or anything means you are changing the file which is in container layer.

--> So the changes will work only particular that container only.

--> when container destroys it will delete the all the changes and modified files as well.



--> So if you want to persist the data you need use volumes

--> docker volume create test_volume

--> above will create test_volume inside --> /var/lib/docker/volumes/test_volume

-->  docker run --mount type=bind,source=/data/mysql,target=/var/lib/mysql mysql

--> All the data which is stored in mysql will be stored in test_volume.



--> Docker Engine

--> When you install docker on a host you are installing below things

	1. Docker deamon --> background process which contains images, containers, all other objects
	2. REST API  -->  REST API will act as a communicator between CLI and Deamon
	3. Docker CLI --> Where we will give our instrutions
	
--> we can control the containers on how much resources it can use

	docker run --cpus=.5 ubuntu  --> 50% of host cpu
	
	docker run --memory=100m ubuntu --> 100MB
	
	
	
--> Docker Networking



	when you install docker it will create 3 networks automatically
	
	1. Bridge --> Defalut network container gets attached to by default
	2. none
	3. host --> It will be attached to the docker running host network
	
--> If you want to attach to specific network

	docker run --network=network-name ubuntu
	
--> whenerver we are creating container, that container will create veth which is Docker0.

--> this Docker0 will connect to the host system and now we can able to communicate with host OS.




--> ###################   Docker Advanced #################################


Docker on windows

1. Docker on windows using toolbox

2. Docker for windows


--> Windows containers

	2 types containers in windows.
	
	1. Windows Server conatiner --> kernel is shared
	
	2. Hyper-V isolation --> each container has seperate kernel
	

--> 2 types of images in windows

	1. Windows Server Core
	
	2. Nano Server
	
	
--> We can run docker from another system to connect to host on another system 

	docker -H ssh://user@192.168.1.100 run nginx

	
--> Docker uses namespaces to isolate workspaces.

--> host has seperate namesspace

--> containers have sepearto namespace.

--> Host, containers no one has any idea about PIDs another conatiner or host.


--> By default aufs is the storage driver for debian and ubuntu.

--> Each storage drivers stores data differently.

--> aufs stores data in 3 different layers

	1. diff
	2. layers
	3. mnt
	
--> To check information about docker

    docker info
	
-->  you ran the dockerfile with 5 steps. first time it will build everything from scratch.

--> For second time, When you ran the same dockerfile with 3rd step changed, upto 3 times it will use cache , after 3rd step it will build everything from scratch for the remaining steps.


--> To see the images space usage.

    docker system df -v
	
--> To promote existing worker node to manager-node

	docker node promote node-name   --> Need to run in master node.
	
--> To join cluster as a manager node

	docker swarm join-token manager
	
--> the above command will give you token with command, paste it in the node that you want to join the cluster as a manager.



--> Docker Service:


--> Docker service will run across the swarm cluster.

--> There are 2 types of services

     replicas and global
	 
--> docker service create --replicas=3 my-server

--> If you want to run service in each and every node, but not specify through the replicas then global service will come into picture.

--> docker service create --mode global monitor-agent

--> the above command will run the monitor-agent instance in every node in the cluster.




--> Advanced networking


--> docker network create --driver overlay --subnet 10.0.9.0/24 my-overlay-network

--> overlay network is advanced networking option 

--> its an inernal private network

--> The main advantage is it will expands across all the nodes which are participating in swarm cluster.

--> docker service create --replicas=2 --network my-overlay-network nginx

--> when we are creating the service we can attach the network

--> So we can get them communicated with each other through overlay network.

--> When you have only one node and you want to run 2 applcations. You can use only map one port like 80 only to one container.

--> We cannot have two mappings to the same port.

--> This is where INGRESS Network come into the picture.

--> Its same like overlay network which spans across the swarm cluster.

--> When you create a docker swarm by defaultly it will create INGRESS Networking and it has in built load balancer.

--> Docker has built in DNS server which runs at 127.0.0.11.

--> When you create docker with the name that name assigned with IP in DNS server.

--> So when you want to communicate with another docker you can connect with docker container name.


--> Docker stacks: managed using docker-compose.yml.

--> Docker Stack is specifically designed for Swarm mode.

--> Docker stack is supported in version 3 compose file.

--> Docker service will deploy single service in swarm cluster in multiple worker nodes, while docker stack will deploy multiple services in swarm cluster.

--> A Docker Stack is a collection of services that define an application in Docker Swarm mode. 

--> Docker servcie manages single service, while Docker stack manages multiple services.

--> in docker stack you can specify how many replicas you want to run, you can specify where you want to run

	Docker-compose.yml
	
	version:"3"
	services:
	
	redis:
		image: redis:alpine
		ports:
		  - "6379"
		deploy:
		  replicas: 2
		  placement:
			constraints: [node.role == manager]

-->  Deploy a Stack

docker stack deploy -c docker-compose.yml my_stack

--> List Running Stacks

docker stack ls

--> List Services in a Stack

docker stack services my_stack

--> List Containers in a Stack

docker stack ps my_stack

--> Remove a Stack

docker stack rm my_stack

-->  Inspect a Stack

docker stack inspect my_stack


--> Docker compose file:

-->	To run multiple containers will do docker run image-name, to avoid this we will write all the commands in docker-compose file and we will run the dokcer-compose up.Then it will run the whole containers.

--> Like in the same way above we have docker service to create a new service on docker swarm, it will create multiple instances of single service in swarm.

--> So to create multiple services, we need to run multiple commands instead of that we can create a stack file and run that stack file it will create all things in docker swarm.

--> Example:

	docker service create simple-web-app
	
	docker service create mongodb
	
	docker service create redis
	
--> For above services we need to run each and every command in manager node to create service in swarm.

--> Instaead of that you can create a docker-compose.yml file like below

	version:"3"
	services:
	
		web:
		  image: redis:alpine
		  deploy:
			   replicas: 2
		database:
		  image: mongodb
		  deploy:
			   placement:
				   constraints:
					 - node.role == manager
					 
		messaging:
		  image: redis:alpine
		  deploy:
			   replicas: 2
			   resources:
					limits:
						cpus: 0.01
						memory: 50M

--> You can do docker stack deploy -c docker-compose.yml my_stack1, It will run all the services.


--> We can create our own dokcer registry

	docker run -itd -p 5000:5000 --restart always --name registry registry:2

--> By running above you can create your own docker registry in private.

--> Tag the image with localhost or Ip:

	docker build . -t localhost:5000/custom-app

--> When you are pushing to the your own private registry you need to specify ip

	docker push localhost:5000/custom-app
	
--> When you try to tag a name to docker image or container which has no name, it will replace the name of the existing image.

--> When you try to tag a name to docker image or container which has name, it will create the new image name pointing to the existing image.

--> If you have built a Docker image without specifying a name (repository and tag), Docker assigns it a default name as <none>:<none>, calling it a dangling image.

-->	# Create two custom networks
docker network create frontend
docker network create backend

# Run the API container in the backend network
docker run -dit --name api --network backend my-api-image

# Run the NGINX container in the frontend network
docker run -dit --name web --network frontend nginx

# Now connect the NGINX container to the backend network too
docker network connect backend web



--> ########################################## KUBERNETES ##########################################




--> Kubernetes wrap up containers inside a virtual box called Pod.

--> A pod can have multiple containers

--> In kubernets IP Address is available at pod level,as in docker swarm it will be on container level

--> Containers with in the pod share same namesapace and can communicate with each other.

--> In kubernetes we use services to enable communication between the pods and enable external access to the application.
	
--> By using kubectl command line interface we will interact with kubernetes.


--> In YAML when you are declaring key value pair after key before value there should be some space

--> Key/Value pair

	Fruit: apple
	Meat: chicken
	
--> Array/List
	
	Fruits:
	-	orange
	-	apple
	-	banana
	
	Vegetables:
	-	carrot
	-	tomato
	
	
--> Dictionary/Map

	Banana:
		calories: 105
		fat: 0.4g
		carbs: 27g
		
--> In list order of items matters

	Fruits:
	-	orange
	-	apple
	-	banana
	
-->  Above one is !== to below one.

	Fruits:
	-	orange
	-	banana
	-	apple
	
--> In Dictionary/Map order of items doesn't matters

		Banana:
			calories: 105
			fat: 0.4g
			carbs: 27g
		
--> Above item === below item

		Banana:
			calories: 105
			carbs: 27g
			fat: 0.4g
		
		
		
--> Sample pod.yml 

	apiVersion: v1
	kind: Pod
	metadata:
		name: sample-app
		labels:
			app: my-sample-app
			type: frontend
			
	spec:
		containers:
	    - name: nginx
		  image: nginx
			
--> Pod can contain multiple containers so we need to mention lists.

--> We have repliaction controller previously that has been replaced by Replica Set

--> For replicaset

		apiVersion: apps/v1
		kind: ReplicaSet
		metadata:
		 name: sample-app
		 labels:
			app: my-sample-app
			type: front-end
			
		spec:
		 template:
		 
		   metadata:
		     name: sample-app
		     labels:
				app: my-sample-app
				type: front-end
		   spec:
			  containers:
			  - name: nginx
		        image: nginx
			
		 replicas: 3
		 selector:
		  matchLabels: 
		    type: front-end
		
--> replicaset needs selector --> Why do we need selector as we already specifying the pod details in spec section

because replicaset can manage the pods which name equals to the value in the labels name eventhough that pod is not created by replicaset.

--> When you first create a deployment it triggers a rollout and a rollout creates a new deployment revision

--> the above will be revision-1

--> In the same way when you do another process it will cretate new which is revision-2

--> In this way it keeps the track of all the changes

--> kubectl rollout status deployment/myapp-deployment --> To see the status of the rollout 

--> kubectl rollout history deployment/myapp-deployment  

--> kubectl rollout undo deployment/myapp-deployment --> To rollback deployment operation.


--> Deployment strategy

	Recreate --> In this we will down all the pods and then add new pods 
	
--> Because of this we will face application down time

	Rolling Update --> In this we will take down the previous version down and then add new one by one by one.
	
	This way we won't face application downtime.
	
--> If you do not specify  Deployment strategy during deployment it will take Rolling Update by default.



--> Networking in kubernetes:

	In kubernetes ip address is assigned to a Pod
	
--> Each pod in k8s gets its own internal ip assigned and its with in the range of 10.244. series

--> Why they will get 10.244 series beacuse internally k8s create a private network which with in the range of 10.244.0.0.
So when you create pod each pod will assigned to that network and will get the ip with in the range.

--> k8s expect us to crate bleow criteria

	All containers/PODs can communicate to one another without NAT
	
	All nodes can communicate with all containers and vice-versa without NAT
	
	
--> k8s service is just an object like pod,replicaset, deployment

--> One of its usecase is to listen to a port on the Node and forward that request to the port on the pod which is running on the same node.

--> This type of service is known as NodePort service.

--> NodePort valid range is 30000 - 32767

--> service-definition.yaml

	    apiVersion: v1
		kind: Service
		metadata:
		 name: myapp-service
		spec:
			type: NodePort
			ports:
			  - port: 80
				targetPort: 8080
				nodePort: 31000
		selector:
			app: my-app
			type: front-end
			
			  
Port 80 is the port exposed by the service.

TargetPort 8080 is the port on the pods to which the traffic will be forwarded.

NodePort 31000 is the external port through which clients can access the service from outside the cluster.


--> and ports is an array thats why we mentioned in array list

--> ports:
			- targetPort: 80
			  port: 80
			  nodePort: 30008
			  
--> if you use only above values it doesn't know where exactly the request should transfer, because there are 100s of pods running on same port, thats why we use selector

--> ClusterIp is the default ip, if you don't mention anything it will take clusterIP

--> To get the pods and services at one time --> kubectl get pods,svc



##################################################  Terraform  #############################################



--> terraform uses HCL --> Hashicorp Configuration Language

--> In terraform everything is resource. That why you will mention resource before for every resource creation.

--> HCL basics:

	resource "local_file" "pet"{
	filename = "root/pets.txt"
	content = "We love pets"
	}
	
--> resource --> Block name

--> local_file --> Resource Type --> local=provider ,  file=resource

--> pet --> Resource name --> this is useful for accessing that in below resource type

--> filename , content --> Arguments

--> Each Resource Type has specific arguments to expect.

--> Types of providers in terraform

	1. Official providers --> AWS,Azure,Googlecloud,localprovider --> maintained by hashicorp and major cloud providers
	
	2. partner providers --> heroku, digitalocean --> owned and maintained by third party tech company
	
	3. Community providers --> activedirectory, netapp-gcp --> maintained by individual memebers of Hashicorp
	
--> when you run terraform init plugins are installed by terraform

	you will see
		
	Installing hashicorp/local v2.0.0.
	
	here hashicorp--> Org Namespace local--> provider type

--> That you can see at .terraform --> here you can see plugins

--> terraform init -->  It will always download latest packages

--> variables.tf

	variable "variable_name"{
	
	default = "value" --> This is optional
	
	}
	
--> 
	~ â†’ The resource will be updated.

	+ â†’ A new resource will be created.
	
	- â†’ A resource will be destroyed.
	
	
--> variables block

	variable "variable_name"{
	
	default = "value" --> This is optional
	type = number/string/bool --> This is optional
	description = "some content" --> This is optional
	
	}
	
--> If we don't specify any type then it will be type = any

--> variable "variable_name"{
	
	default = ["mr", "mrs", "Sir"]
	type = list(string)
	
	}
	
--> variable "variable_name"{
	default = [2, 56,78]
	type = list(number)
	}
	
--> variable "variable_name"{
	
	default = {
	"color" = "brown"
	"name" = "bella"
	}
	type = map(string)	
	}
	
-->  variable "variable_name"{
	
	default = [2, 56,78]
	type = set(number)
	
	}
	
--> TO use environment variable

	export TF_VAR_filename = "Test"
	
--> If you want to use multiple variables

	create file named terraform.tfvars --> because this file automatically loaded by terraform
	
	filename = "Test"
	prefix = "Mr"
	Name = "Suri"
	
	variables.tf --> because this file automatically loaded by terraform
	
	variable "variable_name"{
	
	default = [2, 56,78]
	type = list(number)
	
	}
	
--> If you create with another name you need mention while applying in command line 

	terraform apply -var-file file-name.tfvars
	
--> Variable applying precedence order (Lowest to highest)

	1. Environment variables
	2. terraform.tfvars
	3. auto.tfvars
	4. command line argument
	
--> command line argument will have highest precedence

--> Implicit dependency

	resource "local_file" "pet"{
	type = string
	default = "My fav is ${random_pet.mypet.id}"
	
	}
	
	resource "random_pet" "mypet"{
	prefix = "Mr"
	length = 2
	
	}
	
--> Explicit dependency

	resource "local_file" "pet"{
	type = string
	default = "My fav is Mr Cat"
	
	depends_on = [random_pet.mypet]
	
	}
	
	resource "random_pet" "mypet"{
	prefix = "Mr"
	length = 2
	
	}
	
	
--> To output the value

	output "pet-name"{
	
	value = random_pet.mypet.id
	}
	
--> ######################   terraform commands   ###########################


	terraform validate --> validates the configuration
	
	terraform fmt --> formats the config files
	
	terraform show --> shows the current state of the resource
	
	terraform providers --> to list the providers used by config files
	
	terraform output --> to print all the output variables
	
	terraform output variable-name --> to print the specific output variable
	
	terraform refresh --> it will modify the state file according to real infrastructure
	
	
--> When you changed something in config file and apply

	first terraform will do is destroys the resource
	then it will create the resource
	
--> You can control above process by life-cycle rules

	first resource will be created and then deletes the old resource

	resource "local_file" "pet"{
	filename = "root/pets.txt"
	content = "We love pets"
	file_permission = "0700"
	
	lifecycle {
	create_before_destroy = true
	}
	}
	
	
-->  It won't delete the old resource

	resource "local_file" "pet"{
	filename = "root/pets.txt"
	content = "We love pets"
	file_permission = "0700"
	
	lifecycle {
	prevent_destroy = true
	}
	}
--> The above is useful when you want your resource should not be deleted

--> The above will work only when you do terraform apply , if you do terraform destroy resource will be deleted

--->  resource "local_file" "pet"{
	filename = "root/pets.txt"
	content = "We love pets"
	file_permission = "0700"
	
	lifecycle {
	ignore_changes = [tags]
	}
	}
	
--> It will ignore the changes to the tags

--> Data sources:

	In terraform we can provision using data source with that we can give input as a local file
	
	data "local_file" "dog"{
	filename = "root/dogs.txt"
	
	}
	
--> Data sources fetch information without managing resources

--> depends_on and lifecycle are the meta arguments

--> variable "filename" {
	default = [
	"/root/pets.txt",
	"/root/dogs.txt",
	"/root/cats.txt"
	]
	
	}
	
--> resource "local_file" "pet"{
	filename = var.filename[count.index]
	count = length(var.filename)
	}
	
--> O/P : pets.txt, dogs.txt, cats.txt

--> In above count variable has some drawbacks like when you remove 0th element then 1st element comes to 0 and 2nd to 1
This is why it will destroy and create a new resources.

--> To avoid above one we can use foreach 

set 

--> variable "filename" {
	type = set(string)
	default = [
	"/root/pets.txt",
	"/root/dogs.txt",
	"/root/cats.txt"
	]
	
	}
	
--> resource "local_file" "pet"{
	filename = each.value
	for_each = var.filename
	}
	
--> foreach will work with set or list


list

variable "filename" {
	type = list(string)
	default = [
	"/root/pets.txt",
	"/root/dogs.txt",
	"/root/cats.txt"
	]
	
	}
	
--> resource "local_file" "pet"{
	filename = each.value
	for_each = toset(var.filename)
	}
	
--> When we use foreach resources created in map , but incase of above count it is a list and identified by a index

--> In foreach its identified by a key name

--> version constraints

	To use specific version we need to provide required_providers section
	
	terraform {
	
	required_providers {
	local = {
	source = "hashicorp/local"
	version = "1.4.0"
	}
	}
	}
	
--> The above one is example provider for local

--> we can specify version = "> 1.4.0" for greater than, version = "< 1.4.0" for less than

--> version = "~> 1.4.0" incremental it can download 1.4.0, 1.4.1, 1.4.2 like that up to 1.4.9

--> One service in AWS need to connect to another service then we need to create a role and assign that role to the service.

--> When you do aws configure and given all the details the details were stored in 

	users home directory
	
	$ cat .aws/config/config
	
	$ cat .aws/config/credentials
	
--> To interact with aws below is command structure

	aws <command> <subcommand> [options and parameters ]
	
	
--> aws iam create-user --user-name lucy


--> When you do terraform apply for first time terraform.tfstate file will be created.

--> State file we can't edit or see with normal commands so we have Terraform state commands

	terraform state show aws_s3_bucket.finance --> It will give all the attributes related to the finance resource
	
	terraform state list  --> list all the resources with in the state file
	
	terraform state mv SOURCE DESTINATION --> to move from one place to another place with in the state file or another state file as well.
	
	terraform state pull --> to download remote state file and to see which is in cloud
	
	terraform state rm resource_address --> to delete items from the state file
	
	
--> Terraform provisioners:


remote-exec:

	resource "aws_instance" "Web"{
	
	ami = "ami-9844894894889"
	instance_type = "t2.micro"
	
	provisioner "remote-exec" {
	
		inline = [
					"sudo apt update"
					"sudo apt install -y"
					"sudo systemctl enable nginx"

		]
	}
	
	connection {
	type = "ssh"
	host = "seld.public_ip"
	user = "ubuntu"
	private_key = file("/root/.ssh/web")
	
	}
	
	key_name = aws_key-pair.web.id
	vpc_security_ids = [aws_security_group.ssh-access.id]
}


--> The above is the remote-exec provisioner when we will create resorce and want to run commands inside remote resource when created we can use above provider.

--> local-exec:

	resource "aws_instance" "Web"{
	
	ami = "ami-9844894894889"
	instance_type = "t2.micro"

	provisioner "local_exec" {
	
	command = "echo ${aws_instance.Web.public_ip} >> /tmp/ips.txt"
	
	}

}

--> We can store the values from terraform output by using local_exec provisioner

--> if thwre is an error inside provisioner and you don't want to entire code to fail inside provisioner you can specify like on_failure = continue, then even provisioner fails resource creation will continue.

--> Even you can set when provisioner to be run if you want to run provisioner to run whrn resource destroy then you need to set when = destroy inside provisioner

--> Instead of remote-exec you can use user_data,  which will also do same thing it will run commands in remote.
Remote-exec need connction from local to server to run remote-exec commands, user_data don't need that.

--> user_data

	
	
resource "aws_instance" "Web"{
	
	ami = "ami-9844894894889"
	instance_type = "t2.micro"
	
	user_data = <<-EOF
	
					"sudo apt update"
					"sudo apt install -y"
					"sudo systemctl enable nginx"
					
					EOF
}

--> How to debug in terraform

	First we need to set the environment variable $ export TF_LOG=TRACE
	
	We have upto 5 log levels --> INFO, WARNING, ERROR, DEBUG, Errors
	
--> To store terraform logs in a particular file --> $ export TF_LOG_PATH=/tmp/terraform.log

--> To disable logging $ unset TF_LOG_PATH


--> How to import existing infrastructure into the terraform configuration

	main.tf

	provider "aws"{
	region = "us-east-1"
	}
	import{
	id = "Instance-ID"
	to = aws_instance.example
	}

--> After above you need to run terraform init
								terraform plan -generate-config-out=generateresouce.tf
								
--> In generateresouce.tf file with aws_instance.example, you will have everything related to the particular instance

--> copy the whole configurations in main.tf file

--> Now when you run the terraform plan it still shows that 1 resource to add, because for this still you don't have a terraform state file.

--> to generate state file for above configuration you need type below command

	terraform import aws_instance.example instance-ID
	
	
--> Now if you run terraform plan it won't show resource to add

--> terraform refresh will refresh the state file according to the infrastructure


--> Terraform modules

	We can use the existing resource creation code to avoid duplication of code by using modules
	
	main.tf
	
	module "dev-server"{
	source = "../aws-instance"
	}
	
--> here source path is where the existing resource code resides

--> When you run terraform in a path terraform will consider all the files as configuration files which filename ends with .tf

--> We can import modules from terraform as well.

	main.tf
	
	module "security-group-ssh"{
	source = "terraform-aws-modules/security-group/aws/modules/ssh"
	}
	
--> The source path you can find it in official terraform page

--> If we want to test something with terraform file you can type terraform console, it will open a console here you can test whatever.

-->  Functions in terraform

	file function

	resource "aws_iam_policy" "adminUser"{
	name = "AdminUsers"
	policy = file("admin-policy.json")
	}
	
	length function

	resource "aws_iam_policy" "adminUser"{
	filename = var.filename
	count = length(var.filename)
	}
	
	
	
--> max(-1,2,-10,200,-250)  --> 200

--> min(-1,2,-10,200,-250)  --> -250

variable "num"{
type = set(number)
default = [250, 11, 5]
}

--> max(var.num)  --> 250

--> ceil(10.1)--> 11

--> ceil(10.9)--> 11

--> floor(10.1)--> 10

--> floor(10.9)--> 10

variable "ami"{
type = string
default = "ami-xyz, ami-abc, ami-def"
}

--> split(",", var.ami) --> ["ami-xyz", "ami-abc", "ami-def"]

--> substr(var.ami, 0,7) --> ami-xyz

variable "ami"{
type = string
default = ["ami-xyz", "ami-abc", "ami-def"]
}

--> join(",",var.ami) --> "ami-xyz, ami-abc, ami-def"


--> index(var.ami, "ami-abc") --> 1 --> returns index

--> element(var.ami, 2) --> ami-def --> returns element at that index

--> contains(var.ami, "ami-abc") --> true

--> Using terraform workspace we can create smae resources in multiple environments.

	terraform workspace new projectA --> to create a workspace
	
	terraform workspace list --> to list workspaces
	
	terraform workspace show --> It will return the workspace that we are in
	
	terraform workspace select projectB --> to switch to projectB workspace
	
	
	main.tf
	
	resource "aws_instance" "projectA"{
	ami = lookup(var.ami, terraform.workspace)
	instance_type = var.instance_type
	tags = {
	
	Name = terraform.workspace
	}
	}
	
	variables.tf
	
	variable region {
	default = "ca-central-1"
	}
	
	variable instance_type {
	default = "t2.micro"
	}
	
	variable ami {
	type = map
	default = {
	"projectA" = "ami-7676585858567"
	"projectB" = "ami-5425253253252"
	}
	}
	
--> When we do terraform apply in 2 workspaces it will create 2 state files for each workspace

--> When using workspaces instead of using terraform.tfstate file it will create terraform.tfstate.d there you will find each state file for each workspace

--> To check the terraform.tfstate.d --> tree terraform.tfstate.d



################################################ ANSIBLE ###################################################



--> When you install ansible it will create a configuration file at /etc/ansible/ansible.cfg

--> In ansible.cfg file we have defaults section, there we have lot of configurations

	inventory =  /etc/anisble/hosts
	log_path =   /var/log/ansible.log
	
	# ssh timeout
	
	timeout = 10
	fork    = 5
	
--> When you run ansible playbooks it will take the configurations from above path and run

--> You can make a copy of above config file and do changes particular to your playbook and when you run playbbok you can specify your customized configs

--> Below is my customized config file path

	/opt/ansible-web.cfg
	
--> $ANSIBLE_CONFIG=/opt/ansible-web.cfg ansible-playbook playbook.yml

--> If we have multiple config files then config files priority order (highest to lowest):

	1. file set through the environment variable
	
	$ANSIBLE_CONFIG=/opt/ansible-web.cfg
	
	values in ansible-web.cfg will have 1st priority because this file referred through environment variable
	
	2. current directory from where ansible playbooks are run
	
	3. .ansible.cfg file in users home directory
	
	4. default config file at /etc/ansible/ansible.cfg
	
	
--> We can check ansible configurations

	ansible-config list --> lists all configs
	
	ansible-config view --> shows current config file
	
	ansible-config dump --> shows the current settings ansible pickedup and from where it picked from
	
	
--> How to configure inverntory in ansible

--> Informations about the target systems will be stored in inventory file, if you don't create a inventory file ansible uses default inventory file located at /etc/ansible/hosts

--> example inventory file

	web ansible_host=server1.com ansible_connection=ssh ansible_user=root
	
--> Inventory params:

	ansible_connection-ssh/winrm/localhost
	ansible_port-22/5986
	ansible_user-root/admin
	ansible_ssh_pass-Password
	
--> There are 2 types of inventory format types

	INI --> straightforward --> [webservers]
	
								web1.example.com
								web2.example.com
	
	YAML --> 
	

--> We can create group by kind of servers if its webservers one group backend servers one group

--> Even with in the webservers we can group it into based on locations we can call it as a parent-child realtions

--> We can define common configs at parent group level and location configs at child group level

--> in YAML groups are defined by using keyword hosts and parent-child relations are defined using children

--> Variable precedence:(lowest to highest)

	1. Group level
	2. Group level can overrided by host level
	3. host level can overrided by variables declared in playbooks
	4. variables declared in playbooks can overrided by --extra-vars passing through command line while running playbook
	
--> If you want to print the output that we got when we run playbook

	---
	- name: Check /etc/hosts file
	  hosts: all
	  tasks: 
	  - shell: cat /etc/hosts
		register: result
		
	  - debug:
		   var: result
		   
		   
--> the result of cat /etc/hosts file will store variable declared in register, then under debug module you can print the variable.

--> If you don't want to use debug module, and you want to see the output when playbook runs then

	ansible-playbook -i inventory playbook.yaml -v
	
--> By mentioning -v you can see the output

--> ansible-playbook playbook.yaml --extra-vars "ntp_server=10.1.1.1"

--> When you pass ntp_server from command line it has global scope means it can be accessed from anywhere

--> web ansible_host=server1.com ntp_server=10.1.1.1 --> the ntp_server only has scope of this particular server

--> ---
	- name: Check /etc/hosts file
	  hosts: all
	  vars:
	   ntp_server: "10.1.1.1"
	  tasks: 
	  - debug:
		   var: {{ntp_server}}
		   
	- name: Check /etc file
	  hosts: all
	  tasks: 
	  - debug:
		   var: {{ntp_server}}
		   
--> In above also it has only access to above first host only not the second host

--> When ansible playbook runs first it will create subprocess based on how many hosts are there in hosts file
	
	web1 ansible_host=172.20.1.100
	
	web2 ansible_host=172.20.1.101 dns_server=10.5.5.5
	
	web3 ansible_host=172.20.1.102
	
--> for above when ansible playbook runs it will create 3 subprocesses with each hosts and copy those particular variables to that process

--> inverntory_hostname=web1             inverntory_hostname=web2       inverntory_hostname=web3
									 
	ansible_host=172.20.1.100			 ansible_host=172.20.1.101      ansible_host=172.20.1.103
	
										 dns_server=10.5.5.5
										 
										 
--> You cannot access dns_server in web1 and web3

--> If you want to access variables in another host that is declared inside another host you can access it by magic variables

--> '{{hostvars['web2'].dns_server}}' --> through this you can access

--> /etc/ansible/hosts

	web1 ansible_host=172.20.1.100
	
	web2 ansible_host=172.20.1.101 dns_server=10.5.5.5
	
	web3 ansible_host=172.20.1.102
	
	[web_servers]
	
	web1
	web2
	web3
	
	[americas]
	
	web1
	web2
	
	[asia]
	
	web3
	
--> '{{ groups['americas'] }}' --> web1 web2

--> '{{ groups_names }}' use in a play for web1 --> web_servers, americas

--> '{{ inventory_hostname }}' use in a play for web1 --> returns you the name configured for the host in the inventory file

--> When you connect the ansible host to target machine then ansible host gathers info of target machine like system architecture, ip addresses, mac address device info, volumes, mounts and date and time of the system through the setup module.

--> This setup module runs automatically by ansible

--> All these details(facts) are stored in ansible_facts variable.

--> You can disable this default behavior like gathering facts by mentioning below one playbook

	gather_facts: no
	
--> Checkmode --> If you want to check your playbook what exactly it is going to change you can run the playbook in check mode.

	if you run in checkmode it won't change anything on the hosts.

	ansible-playbook playbook.yaml --check
	
--> Diff mode --> provides before and after comparison of playbook changes run --check with --diff

	ansible-playbook playbook.yaml --check --diff
	
--> syntax-check --> it will verify the syntax of playbook

	ansible-playbook playbook.yaml --syntax-check
	
--> Ansible lint is a command line tool that performs linting on Ansible playbooks, roles and collections.

--> It will also check for syntax errors in playbooks

	ansible-lint playbook.yaml --> you can run like this
	
	
--> We can also use conditions in playbooks

	---
	- name: Install NGINX
	  hosts: all
	  tasks: 
	  - name: Install NGINX on Debian
		apt:
		  name: nginx
		  state: present
		when: ansible_os_family == 'Debian'
		
	  - name: Install NGINX on REdhat
		yum:
		  name: nginx
		  state: present
		when: ansible_os_family == 'RedHat'
		
--> Loops:

loop with lists

	name: create users
	hosts: all
	tasks:
	 - user: name= '{{ item }}' state= {{ present }}
	   loop:
		- joe
		- anna
		- ravi
		- peter
		- john
		
		
loop with dictionary 

    name: create users
	hosts: all
	tasks:
	 - user: name= '{{ item.name }}' state= '{{ item.uid }}'
	   loop:
		-  name: joe
		   uid: 1001
		-  name: anna
		   uid: 1002
		-  name: ravi
		   uid: 1003
		-  name: peter
		   uid: 1004
		-  name: john
		   uid: 1005

	 
--> Modules:

--> ansible modules divide into diff categories

	system
	commands --> excutes command in a remote node
	script --> Runs a local script on a remote node
	files
	database
	cloud
	windows	
	 
	 
--> commands --> excutes command in a remote node


	name: Play 1
	hosts: all
	tasks:
	 - name: execute command 'date'
	   command: date
	   
	 - name: Display resolv.conf
	   command: cat /etc/resolv.conf 

     - name: Display resolv.conf
	   command: cat resolv.conf chdir=/etc
    
     - name: create folder
	   command: mkdir /folder creates=/folder
	   	   
	  
--> script --> Runs a local script on a remote node

	name: Play 1
	hosts: all
	tasks:
	 - name: Run script on remote server
	   script: path/to/script.sh -arg1 -arg2
	   
--> service --> Manage services : start, stop, restart

	name: Play 1
	hosts: all
	tasks:
	 - name: start database service
	   service: name=postgresql state=started
	   
--> Above one you can write like below as well.

	name: Play 1
	hosts: all
	tasks:
	 - name: start database service
	   service:
	    name: postgresql
	    state: started
		
--> Ansible handlers will perform based on config changes

	name: Deploy application
	hosts: all
	tasks:
	 - name: Copy application code
	   copy:
	      src: app_code
		  dest: /opt/app
	   notify: Restart app service
		
	handlers:
	  - name: Restart app service
	    service:
		    name: application_service
			state: restarted
			
--> Ansible Roles: A role is a collection of Ansible resources (tasks, handlers, templates, etc.) that perform a specific function within an automation workflow.




###########################################   HELM  ########################################################



--> Helm is a package manager for Kubernetes that helps deploy, manage, and version applications efficiently. 

--> It simplifies Kubernetes configurations using Helm charts, which are reusable and customizable templates for Kubernetes resources.

--> In simple terms, Helm is like apt/yum (for Linux) or npm (for Node.js), but for Kubernetes.

--> You still use YAML, but they are stored inside Helm Charts as templates.

--> You don't write separate YAML files for each environment. Instead, Helm uses a values.yaml file to customize deployments.

--> Helm generates the YAML for you and applies it automatically.

--> Prerequisites to Install Helm:

	1. A Running Kubernetes Cluster --> Helm is a package manager for Kubernetes, so you must have a Kubernetes cluster up and running.
	
	2. Install kubectl (Kubernetes CLI) --> Helm interacts with Kubernetes via kubectl. Install it if it's not already installed:
	
	3.  Install Helm --> Once Kubernetes is ready, install Helm.
	
--> Helm Command Flow:

-->Let's say you run:

--> helm install myapp ./myapp

--> Behind the scenes, Helm:

--> Reads Kubernetes context from ~/.kube/config to determine which cluster to connect to.
--> Converts Helm templates (.yaml) into final Kubernetes YAML.
--> Sends an API request to the Kubernetes API Server.(does not call kubectl)
--> The API Server creates the resources inside the cluster.
--> Helm stores metadata about the release (myapp) for tracking & rollback.



--> Helm components:

	--> Helm chart is a collection of files they contains all the instructions helm needs to know to create a collection of objects in kubernetes cluster.
	
	--> When chart is applied to cluster a release is created.
	
	--> release is a single installation of application
	
	--> Within each release we have multiple revisions
	
	--> Each revision is like a snapshot of application at that point of time.
	
	--> When you do upgrades, config changes then new revision is created.
	
	--> To track all these things helm needs to store the data. this data is known as metadata
	
	--> Helm stores this metadata as secret in kubernetes cluster.
	
	--> Helm releases:
	
		helm install release-name chart-name
		
		helm install my-site bitnami/wordpress
		
	--> You can find all the helm repos at helm hub or ArtifactHUB.io
	

--> Chart.yaml

	apiVersion: v2  --> Helm api version
	appVersion: 5.8.1 --> version of the application inside this chart
	version: 12.1.27 --> version of this chart
	name: wordpress --> Name of the chart
    description: Web publishing platform
	type: application --> 2 types of charts --> 1. application  2. library
	dependencies: 
	  - condition: mariadb.enabled
	    name: mariadb
		repository: https://charts.bitnami.com/bitnami
		version: 9.x.x
	keywords:
	  - application
	  - blogs
	  - wordpress
	maintainers:
	  - email: containers.bitnami.com
	    name: Bitnami
		
--> Helm chart structure

	Hello-world-chart
		templates
		values.yaml
		chart.yaml
		License
		README.md
		charts
		
--> Helm CLI:

	--> helm --help   --> to see list of commands
	
	--> If you want to check charts from CLI in hub
	
		helm search hub wordpress
		
	--> helm repo add bitnami https://charts.bitnami.com/bitnami
	
	By running above command we are adding bitnami to our local repos, so when you run helm install then it helm check for the particular package in the above repo
	
	--> helm install my-release bitnami/wordpress
	
	--> my-release --> name of chart in cluster,  bitnami/wordpress --> repo from where to download
	
	--> When chart is deployed it will create a release
	
	--> To list releases --> helm list
	
	--> To uninstall release --> helm uninstall my-release
	
	--> helm repo --> used to add , remove, list, and index chart repos.
	
	--> helm repo list --> to list all the installed repos
	
	--> helm repo update --> its like apt update
	

--> Customize chart parameters:

	--> When you are installing default helm charts you can override the default values which are there in values.yaml through CLI while installing.
	
	--> helm install --set wordPressBlogName="Helm tutorials" my-release bitnami/wordPressBlogName
	
	--> If you have multiple names to override, you can create custom yaml file and refer it.
	
		custom-values.yaml
		
		wordPressBlogName: Helm tutorials
		wordPressEmail: john@example.com
		
	--> helm install --values custom-values.yaml my-release bitnami/wordPressBlogName
	
	--> If you want to change whole values.yaml file
	
	--> helm pull --untar bitnami/wordpress
	
		helm pull will download the chart in archive, so we need to unzip the file.Then when you do ls you will see the total helm structure. There in values.yaml file you can change anything you want.
		
	--> Then after changes run helm install with path to the current directory where is your customized chart resides
	
		helm install my-release ./wordpress
		
		--> Here ./ refers to current directory and wordpress is chart 
		
		
--> Lifecycle management with helm:

--> helm history release-name --> it will give all the releases with more description
	
--> helm rollback release-name 1 --> it will create a new revision and which is equal to revision 1, but it won't go back to revision 1.

--> helm rollback only restores Kubernetes resource configurations.It does NOT restore persistent data.

--> if you do helm history release-name --> you will see 3 revisions the third one same as revision 1

--> To create a chart structure you can do

	helm create chart-name
	
--> Built in methods in helm we should access it like Start with capital letters.

	.Values.image
	.Release.NAME
	
--> Verifying Helm Charts:

	We have 3 ways to verify charts
	
	1. Lint --> helm lint ./nginx-chart --> helm lint /path/to/chart-name
	
	--> It will check for any typo mistakes and synatx errors and formatting of the file.
	
	2. Template --> We need to check that templating will be replaced with actual values.
	
	--> For example .Release.NAME should be replaced with release name and  values.image templating names should be replaced with actual values.
	
	--> We can verify the above by -->  helm template ./nginx-chart  --> ./nginx-chart --> chart-name.
	
	--> It will generate output with all the values.
	
	--> If you run -->  helm template ./nginx-chart without setting name in metadata it will generate default name RELEASE-NAME
	
	--> If you want to specify release-name --> helm template hello-world ./nginx-chart 
	
	--> This is specifically for how the values are replacing exactly.
	
	3. Dry Run --> Some of the issues may not be captured in above 2 methods. Like if you write container instead of containers both above 2 won't capture this.
	
	--> For this we can use dry run
	
	--> helm install hello-world ./nginx-chart --dry-run
	
    --> It won't install really, it will do a dry run.
	

--> To use default values as fallback, in values.yaml file we need to use functions.

	We can call functions with in the template syntax.
	
	values.yaml
	image:
	 repository: nginx
	
--> 	{{ .Values.image.repository }} --> nginx 

		{{ upper .Values.image.repository }} --> NGINX
		
		{{ quote .Values.image.repository }} --> "nginx"
		
		{{ replace "x" "y".Values.image.repository }} --> "nginy"
		
		
--> If in the values.yaml file image name is absent we can pass default value

	{{ default "nginx" .Values.image.repository }}
	
	--> in the values.yaml file if repository name is absent it will take nginx value as default.
	
--> Conditionals:

	values.yaml
	
	replicaCount: 2
	image: nginx
	
	orgLabel: payroll
	
	
	#############################
	
	service.yaml
	
	apiVersion: v1
	kind: Service
	metadata:
	  name: {{ .Release.Name }}-nginx
	  
	  {{ if .Values.orgLabel }}
	  labels:
	     org: {{ .Values.orgLabel }}
		 
	  {{ end }}
	  
	spec:
	  ports:
	    - port: 80
		  name: http
	  selector:
	    app: hello-world
		
		
    ########################
		
--> If in values.yaml orgLabel is empty in service.yaml it won't execute.

--> apiVersion: v1
	kind: Service
	metadata:
	  name: {{ .Release.Name }}-nginx
	  
	  {{ if .Values.orgLabel }}
	  labels:
	     org: {{ .Values.orgLabel }}
		 
	  {{ end }}
	  
	spec:
	  ports:
	    - port: 80
		  name: http
	  selector:
	    app: hello-world
		
		
--> When manifest file is generated form above yaml file we will get white spaces in place of if and end.

To remove those spaces we can mention - before if and end.

	apiVersion: v1
	kind: Service
	metadata:
	  name: {{ .Release.Name }}-nginx
	  
	  {{- if .Values.orgLabel }}
	  labels:
	     org: {{ .Values.orgLabel }}
		 
	  {{- end }}
	  
	spec:
	  ports:
	    - port: 80
		  name: http
	  selector:
	    app: hello-world
		
--> By specifying - when it is going to generate manifest file it will trim the spaces for that whole line

--> we can write else if like below

	  {{- else if eq .Values.orgLabel "hr" }}
	  labels:
	     org: human resources
	  {{- end }}
	  
--> values.yaml
	
	app:
	 ui: 
	  bg: black
	  fg: red
	 db:
	   name: "users"
	   conn: "mongodb://localhost:27020/mydb"
	  
--> To access bg value we need to traverse through the entire object.

	for example:
	
	background: {{ .Values.app.ui.bg }}
	
--> Instead of above we can set scope and use from the scope.

	
	{{- with .Values.app }}
	
	background: {{ .ui.bg }}
	
	{{- end }}
	
--> With in above block you can access any values after .Values.app

--> We can set scopes with in the scopes as well.

--> We can access root variables with in the scope as well by putting $ before . symbol.

	configmap.yaml
	
	apiVersion: v1
	kind: ConfigMap
	metadata:
	  name: {{ .Release.Name }}-appinfo
	data:
	 {{- with .Values.app }}
	  {{- with .ui }}
	  background: {{ .bg }}
	  foreground: {{ .fg }}
	  {{- end }}
	  {{- with .db }}
	  databasename: {{ .name }}
	  connection: {{ .conn }}
	  {{- end }}
	 release: {{ $.Release.Name }}  --> Accessing global scope variable within the block.
	 {{- end }}
	
--> Ranges:

--> values.yaml

	regions:
	  - ohio
	  - newyork
	  - ontario
	  - london
	  - singapore
	  - mumbai
	  

-->	apiVersion: v1                                             
	kind: ConfigMap
	metadata:
	  name: {{ .Release.Name }}-appinfo
	data:
	 regions:
	 {{- range .Values.regions }}
	 
	  - {{ . }}  --> range is like a for loop and . denotes the current value in loop.
	  
	 {{- end }}
	 
--> O/P:

	apiVersion: v1                                             
	kind: ConfigMap
	metadata:
	  name: {{ .Release.Name }}-appinfo
	data:
	 regions:
	   - ohio
	   - newyork
	   - ontario
	   - london
	   - singapore
	   - mumbai

	
--> If we are using same line multiple times in the same file, we can remove that repetition by using named templates.

	In named templates we will move the line to _helpers.tpl file.
	
--> Why we are using _ beacuse when we run the helm chart, we don't want to create any resource for this file kubernetes cluster.

--> In _helpers.tpl file we should write like above

	_helpers.tpl

	{{- define "labels" }}
		app.kubernetes.io/name: nginx
		app.kubernetes.io/instance: nginx
	{{- end }}
	
	or
	
	_helpers1.tpl

	{{- define "labels" }}
		app.kubernetes.io/name: {{ .Release.Name }}
		app.kubernetes.io/instance: {{ .Release.Name }}
	{{- end }}
	
	
--> service.yaml 

	apiVersion: v1
	kind: Service
	metadata:
	  name: {{ .Release.Name }}-nginx
	  labels:
		{{- template "labels" . }}
	spec:
	  ports:
	    - port: 80
		  name: http
	  selector:
	    app: hello-world
		
--> If you use _helpers1.tpl file instead of _helpers.tpl you need to use {{- template "labels" . }} after labels you need to use . 

--> Because we need pass the current scope to the helper then only it can access the variables in helper file like .Release.Name value in _helpers1.tpl file.

--> If you want indentation to work corectly.

	service.yaml 

	apiVersion: v1
	kind: Service
	metadata:
	  name: {{ .Release.Name }}-nginx
	  labels:
		{{- include "labels" . | indent 2 }}
	spec:
	  ports:
	    - port: 80
		  name: http
	  selector:
	    app: hello-world

--> Why we need to use include because include is a function that output we can pass it to a pipe, but template is an action that output we can't pass it to a pipe.


--> Chart Hooks:

	When you upgrade the with helm install like DB by defaultly won't backup, so for this to backup automatically we can use the chart Hooks.
	
--> So we have below hooks available.

			   pre install, post install
			   pre delete, post delete
			   pre upgrade, post upgrade
			   pre rollback, post rollback
			   
--> How to configure above hooks: by using annotations we can congigure hooks.

	backup-job.yaml
	
	apiVersion: batch/v1
	kind: Job
	metadata:
	  name: {{ .Release.Name }}-nginx
	 
	  annotations:
		"helm.sh/hook": pre-upgrade
		"helm.sh/hook-weight": "5"
		"helm.sh/hook-delete-policy": hook-succeeded
		
	spec:
	 template:
		metadata:
			name: {{ .Release.Name }}-nginx
	   
		spec:
			restartPolicy: Never
			containers:
			- name: pre-upgrade-backup-job
			  image: alpine
			  command: ["/bin/backup.sh"]
			  
--> Why we need to mention hook-delete-policy --> becausse when hook executed it will stay in the cluster as a resource, So when hook successfully executed after that we need to delete the hook.

--> After you create the whole chart with all the required files, you can package it to share with others and to upload it to repo.

	helm package ./nginx-chart
	
--> The above command package the chart as a archive file nginx-chart-0.1.0.tgz

--> this 0.1.0 will take it from chart.yaml file.

--> helm verify ./nginx-chart=0.1.0.tgz --> To verify the tar file



######################################  Istio Service Mesh   ##################################################




--> A Service Mesh is a dedicated infrastructure layer that manages communication between microservices in a distributed application. 

--> It provides features like service discovery, traffic control, security, and observability without requiring changes to application code.

--> A service mesh consists of two key components:

--> Data Plane (Handles Traffic)

	Each microservice has a sidecar proxy (e.g., Envoy) that intercepts all incoming and outgoing traffic.
	These proxies handle load balancing, retries, security (TLS), and observability (logs, metrics, tracing).
	
--> Control Plane (Manages Policies)

	Configures and manages the proxies in the data plane.
	Provides centralized traffic management, security, and monitoring
	
--> Istio is an open-source service mesh that provides a way to manage microservices securely, reliably, and efficiently.

--> Istio Architecture Components

Istio is based on two main components:


	

--> Control Plane
	
	Consist of 3 components.
	Manages and configures the proxies for:

	Pilot â€“ Controls traffic and helps with service discovery.
	Citadel â€“ Manages security, authentication, and TLS certificates generation
	Galley â€“ Handles configuration validation and distribution.
	
	The above 3 components combined into one component called Istiod.
	
--> Data Plane

	Uses Envoy Proxy as a sidecar proxy for each service.
	Handles service-to-service communication, security, and monitoring.
	Communication between proxies happens through Data Plane.
	Istio implements these proxies using open source high performance proxy known as Envoy.
	These proxies talk to the server side component known as the Control Plane.
	Each service or pod has Istio Agent component along with Envoy Proxy.
	These Istio Agents are responsible for pasiing secrets and configuarations to the Envoy proxy from Control Plane.
	
					
					Control Plane
			Istiod --> Citadel,pilot,Galley
			
							||
							
						Data Plane
			
	Istio Agent   Istio Agent  Istio Agent Istio Agent
		
		Envoy		Envoy		Envoy		Envoy
		
		Sevice1    Service2 	Service3	Service4
		
		
--> Control Plane talks to the Data Plane.

--> Data Plane talks to the services through Istio Agent and Envoy Proxy to the service.
	
	
--> How Istio Works?

1ï¸ Each microservice in a Kubernetes cluster gets a sidecar proxy (Envoy).
2ï¸ All traffic between services flows through the proxy, allowing Istio to control and monitor it.
3ï¸ Istio applies traffic rules, security policies, and telemetry collection without modifying application code.


--> To install istio 
	
	--> istioctl install --set profile=demo -y
	
--> When you run above command It will install 3 components on kubernetes cluster.

	istio-ingressgateway  		istiod				istio-egressgateway
	
--> istio-system Namespace:

	Created automatically when you install Istio
	Hosts Istioâ€™s core components, such as istiod, ingress/egress gateways, and sidecar injection webhooks.
	
	
--> To visualize the istio service mesh we will use kiali

--> Kiali is a web based UI.


--> From outside the world traffic comes to istio-ingressgateway, becuase we will configure ingressgateway for inbound traffic and egressgateway for outbound traffic.

--> When incoming traffic comes to the service it needs to pass through the ingressgateway to virtual services in this we will define rules where it should route to and from there response went through egressgateway.


--> For Istio Service Mesh, we will use istio-ingressgateway rather than kubernetes nginx ingress controller.

--> Because istio-ingressgateway offers a lot of advantages over nginx ingress controller.

--> In Gateway we will define which traffic we need to capture.

--> In virtual services, we will define rules where it should route to.

-->  All routing rules configured through virtual services

--> Virtual Services define set of routing rules for traffic coming from ingress gateway to the services.

--> In Virtual Services we can define how much amount of traffic we can distribute to each service, but in traditional kubernetes we don't have that facility.




--> Security in istio:

	In the control plane, we have istiod which consist of certificate authority, Authentication policies, Authorization policies and Network configuration.
	
	certificate authority --> manages keys and certifications in istio. This is where certificates are validated and certificate signing requests are approved.
	
	Everytime a workload is started the envoy proxies requests certificates and key through the istio agents and that helps secure communications between services.
	
	The configuration API server component which is in Control Plane, distributes all the authentication, authorization and secure naming policies to the proxies.
	
--> For certificate management, istioagent will send a private key and CSR certificate signing request to istiod to control plane with credentials.

--> CA in istiod will validate the credentials and CSR certificate.

--> Upon successful validation, It will find the CSR and generates the crt certificate and sends back to istio agents.

--> istio-agent sends the private key and crt to the envoy proxy.



###############################################    GitOps with ArgoCD   ######################################



--> GitOps is a modern approach to managing infrastructure and application deployment using Git as the single source of truth.

--> GitOps has 4 principles:

	1. Declarative Configuration â€“ The entire system state (infrastructure, application configurations) is described in Git using declarative code (e.g., Kubernetes YAML, Terraform, Ansible).
	
    2. Version Control â€“ Every change is tracked, versioned, and auditable via Git.
	
	3.Automation â€“ Continuous Deployment (CD) pipelines automatically apply changes from Git to the infrastructure and applications.
	
	4. Reconciliation â€“ An agent continuously monitors the actual system state and ensures it matches the desired state in Git. If discrepancies arise, the system auto-corrects itself.

--> In DevOps Changes were pushed to the cluster.

--> In GitOps Changes were pulled by the operator.


--> ArgoCD is a declarative continuous deployment tool for the kubernetes.


ARGOCD:



--> ArgoCD is a declarative, GitOps continuous delivery tool for kubernetes resources defined in Git repository.

--> It continuously monitors running applications and comparing their live state to the desired state.

--> It Follows the GitOps pattern by using Git repositories as the source of truth for the desired state of an app and the target deployment environments.

--> Argo CD Concepts:

	Application: An Application in Argo CD is a group of Kubernetes resources (e.g., Deployments, Services, ConfigMaps, Secrets) that are defined in a Git repository and deployed to a Kubernetes cluster.
	
	Application source type: The Application Source Type defines how Argo CD reads and processes application manifests. Argo CD supports different source types  Eg: Helm
	
	Project: Provide a logical grouping of applications, Which is useful when Argo CD is used by multiple teams.
	
	Target state: The desired state of an application, as represented by files in a Git repository
	
	Live state: The live state of the application.
	
	Refresh: Compare the latest code in Git with the live state.  Figure out what is different.
	
--> ArgoCD is installed as kubernetes controller with in the kubernetes environment.

--> Argo CD can be installed in two primary modes:

	Core (Single-Tenant Mode)
	
		Features:
			Headless deployment (no UI or API server). 
			Lightweight components, easier setup. 
			Relies on Kubernetes RBAC for user access. 
			Web UI can be accessed via argocd admin dashboard command. 
			Manifests available at core-install.yaml. 
			
	Multi-Tenant Mode
	
	In Multi-Tenant Mode we have 2 installation options available
	
		1. Non High Availability --> it prvoides 2 manifests install.yaml and namespace-install.yaml
		
			--> The install.yaml manifest provides a standard installation with cluster admin access to Argo CD. 
		
		2. High Availability  --> it prvoides 2 manifests ha/install.yaml and ha/namespace-install.yaml
		
			
	
		Features:
		
			Allows multiple teams to manage applications independently. 
			Provides controls for limiting and isolating applications between teams. 
			Utilizes AppProjects to restrict application access and capabilities. 
			End-users can access Argo CD via the API server using the Web UI or argocd CLI. 
				
				
				
--> In argoCD the default timeout period for Reconciliation is 3 minutes.This value is configurable.

--> It is configured in argocd-repo-server as ARGOCD_RECONCILIATION_TIMEOUT

--> argocd-repo-server is responsible for checking the state from git.

--> The timeout value is stored in argoCD configMap and we can change the value.

	kubectl -n argocd patch configmap argocd-cm --patch='{"data":{"timeout.reconciliation:""300s"}}'
	
--> Restart the argocd-repo-server

	kubectl -n argocd rollout restart deploy argocd-repo-server
	
--> Instead of that we can create a webhook in git and when push event occurs webhook push events to argoCD.

--> This way we can reduce the timeout.

--> The following checks are made for specific types of kubernetes resources.

	1. For kubernetes Secrets
	
		It will determine the Whether the service is of type LoadBalancer and verifies the status.loadbalancer.ingress list isn't empty and hostname or ip atleast have 1 value.
		
	2. For ingress resources
	
	    It will determine the Whether status.loadbalancer.ingress list isn't empty and hostname or ip atleast have 1 value.
	
	3. For persistent volume claims
	
		It will determine the pvc.status.phase field. Checks if the PVCs bounded to actual volume.
		
	4. For Deployment,Replicaset,Statefulset,Daemonset 
	
		the Observed Gen = desired Gen
		
			updated replicas = desired replicas
			
	
--> To create a custom health check you need to edit and append the code in argocd-cm (configMap)

--> ArgoCD sync strategies:

	Manual & Automatic 
	
	Auto-pruning of resources : It descibes what happens when files are removed or deleted from Git.
	
	Self-Heal of cluster : It defines what argoCD does when you make kubectl edit changes directly to the cluster.
	
	
--> If Auto-pruning is enabled When you delete any resource from the Git the argoCD will the delete resource from cluster if it is disabled then it won't delete.
	
--> If Self-Heal is enabled --> when you make kubectl edit changes directly in the cluster not through github,Then it will pull the present state of the yaml file from github and reset the state.

--> To detect delete changes --> Auto-pruning should be enabled.

--> To detect state changes from Git --> Self-Heal should be enabled.

--> Instead of creating Application from argoCD UI, we can create it by using declarative approach means by writing Yaml file and in the cluster apply that yaml file.

--> We can create multiple applications using one declarative approach.

--> We can create agroCD application using helm chart.

--> When you create argoCD application using helm chart, that application is under the control of argoCD and not in the helm control.

--> ArgoCD supports multi-cluster deployment as well.

--> In Kubernetes, the kubectl command-line tool interacts with multiple clusters by using a configuration file called kubeconfig (usually stored at ~/.kube/config).

--> When working with multiple clusters, you need a way to:

	Set up access to each cluster.

	Manage authentication (credentials) for different users.

	Switch between clusters easily.
	
--> Kubernetes uses three key components in the kubeconfig file:

	Cluster: Defines the API server address and cluster settings.

	User (Credentials): Defines how the user authenticates with the cluster.

	Context: Binds a Cluster and User together for easy access.
	
--> To add Prod cluster to the existing one for multiple deployments:


-->	 Add a Kubernetes Cluster (set-cluster)
	 This command registers the cluster in the kubeconfig file.

	kubectl config set-cluster prod \
  --server=https://139.59.29.103:6443 \
  --certificate-authority=/path/to/ca.crt \
  --embed-certs=true

-->	Add User Credentials (set-credentials)
	Now, we need to set authentication for the user who will access the cluster.
	
	kubectl config set-credentials admin \
  --client-certificate=/path/to/client.crt \
  --client-key=/path/to/client.key \
  --embed-certs=true
  
-->	Create a Context (set-context)
	Now that we have a Cluster and User, we need to link them together using a Context.
	
	kubectl config set-context admin-prod \
  --cluster=my-cluster \
  --user=my-user \
  --namespace=default


--> First we need to define a context within the kubeconfig file.

	kubectl config get-contexts --> To list the contexts.

--> Then we need add external cluster to the context.

	argocd cluster add context-name --> To add external cluster to the context.
	
--> Then if you check the cluster list you can see external cluster listed.

	argocd cluster list
	
--> It will store cluster details in secrets.

	kubectl get secrets 
	
--> User Management:



--> By Defalut argoCD has one admin user.

	By default, ArgoCD stores user credentials inside a Kubernetes Secret in the argocd namespace.
	
	The initial admin user is stored in argocd-secret.

--> argoCD supports 2 types of users:

	1. Local users
	2. Through SSO by integrating argoCD through Okta or similar SSO products.
	
--> New Users can be defined in argocd-cm configMap, but their passwords are NOT stored there. 
	Instead, the argocd-cm ConfigMap only contains user definitions and permissions.
	
--> User passwords are stored in argocd-secret.
	The actual password hash is stored in argocd-secret.

	accounts.USERNAME: apiKey,login
	accounts.SID.enabled: "false"
	
--> The argocd-rbac-cm (Role-Based Access Control ConfigMap) is used to control user permissions in ArgoCD.

--> While argocd-cm defines users, argocd-rbac-cm defines what those users can do in ArgoCD.


--> Each user can be associated with 2 capabilities.

	apiKey and login
	
--> apiKey allows generating json web token authentication for api access.

--> login allows user to login using UI.

--> By Default New users have no access.

--> argoCD has 2 pre-defined roles --> readonly and admin



--> How argocd and Prometheus can be configured to scrape argocd metrics.

--> The Prometheus uses kuberenetes custom resources to simplify the deployment and configuration of prometheus, alert manager and grafana and other realted monitoring components.

--> Prometheus operator uses Service or Pod monitoring CRD to perform auto discovery and auto configuration of scraping targets.



--> For service monitoring follwoing steps are required.

	1. We need to have actual services which expose metrics at defined endpoint port and identified with label.
	
	2. ArgoCD out of the box exposes several services exposing prometheus metrics.
	
	3. We have a services for 
	
		argocd-repo-server
		
		argocd-metrics
		
		argocd-server-metrics
		
		argocd-applicationset-controller.
		
    4. We need to have Service Monitor, which is a custom resource to discover services based on matching labels.
	
	5. argoCD also provides sample service manifests which can be applied on kubernetes server.
	
	6. The Prometheus operator uses prometheus CRD (Custom Resource Definition) to match the service monitors based on labels and generate configuration for prometheus.
	
	7. The Prometheus operator calls the config reloader component to automatically update the configuration yaml with argocd scarping target details.
	
	8. This is how argocd metrics are scraped by prometheus.
	
--> ArgoCD exposes multiple metrics services that Prometheus can scrape.

--> Prometheus scrapes metrics from services, so we need actual services that expose metrics on a defined endpoint.

--> These services provide real-time ArgoCD data, such as application sync status, repository request counts, and error rates.

	kubectl get svc -n argocd
	
	NAME                                   TYPE        CLUSTER-IP     PORT(S)  
	argocd-repo-server                     ClusterIP   10.100.10.12   8084/TCP  
	argocd-metrics                         ClusterIP   10.100.10.13   8082/TCP  
	argocd-server-metrics                  ClusterIP   10.100.10.14   8083/TCP  
	argocd-applicationset-controller       ClusterIP   10.100.10.15   8085/TCP  
	
--> Each service exposes Prometheus metrics on a specific port.

--> ArgoCD out of the box provides multiple services that already expose Prometheus metrics

--> Instead of manually configuring services, ArgoCD automatically exposes its internal metrics using these services.

--> ArgoCD exposes the following key services:

	        Service Name				Metrics Port			Purpose
			
			argocd-repo-server	             8084	    Monitors repository requests
			argocd-metrics	                 8082		General ArgoCD metrics
			argocd-server-metrics			 8083 		Tracks API requests & errors
			argocd-applicationset-controller 8085	    Tracks ApplicationSet sync status
			
			
--> Each ArgoCD component has its own metrics. This helps Prometheus monitor specific ArgoCD functionalities like repo syncs, application health, and API performance.

--> Prometheus does not automatically detect new services. A ServiceMonitor allows Prometheus to discover ArgoCD services dynamically.

--> The ServiceMonitor automatically finds and scrapes services based on labels.

--> The Prometheus Operator manages Prometheus instances and automatically configures them based on ServiceMonitors.

--> The Prometheus Custom Resource Definition (CRD) ensures that Prometheus finds all services labeled for monitoring.

--> Whenever a new service is added, the Prometheus Operator updates its configuration dynamically.

--> The config-reloader automatically updates the Prometheus scrape targets without requiring a manual restart.


How to raise alerts based on argocd metrics with in the prometheus server.

--> As part of the prometheus operator, we also get AlertManager Configs and PrometheusRules Monitoring CRD.

--> PrometheusRules is a Custom Resource that defines recording and alerting of rules for a prometheus instance.

--> We will create a prometheus rule with alerting rules.

--> Once the prometheus rule is created prometheus operator uses the prometheus rules CRD and generates the configuration for prometheus.

-->  The Prometheus operator calls the config reloader component to automatically update the rules file.




###################################  Prometheus Certified Associate (PCA)  ###############################



--> Observability : The ability to understand and measure the state of the system based upon data generated by system.

--> Prometheus is a monitoring solution that is responsible for collecting and aggregating metrics.

--> Prometheus only handles the metrics.

--> Prometheus is a open source monitoring tool that collects metrics data, and provide tools to visualize the collected data.

--> Prometheus collects metrics by sending http requests to /metrics endpoint of each target.

--> Prometheus allows you to generate alerts when metrics reach a user specified threshold

--> So what kind of metrics prometheus can monitor

	CPU/Memory utilization
	
	Disk space
	
	Service uptime
	
	Application specific data
		Number of exceptions
		Latency
		Pending requests
	

--> Prometheus is designed to monitor Numeric data

--> Prometheus don't monitor --> Events, system logs, traces

--> Recording rules:

Recording rules allow you to precompute frequently needed or computationally expensive expressions and save their result as a new set of time series.

--> Prometheus architecture

									Prometheus
									
						
	Targets	-->		Scrapes metrics data  --> Stores metrics data --> Accepts PromQL Query  
					
					    Retrieval					TSDB					HTTP Server
						
						
-->  For short-lived jobs prometheus can't pull data.

--> So we will use Pushgateway, short-lived job push data to Pushgateway and from Pushgateway prometheus can pull data.

--> Using service Discovery prometheus can discover the targets dynamically.

--> Most systems by default don't collect metrics and expose them on an HTTP endpoint to be consumed by a prometheus server.

--> Exporters collect metrics and expose them in a format prometheus expects.

--> Prometheus is configured to monitor itself by default.

--> When you run prometheus ./prometheus

	It can cause issues.
	
	1.It will run in foreground, so when you close the terminal it will close the sever.
	
	2. It doesn't start on-boot
	
--> So instaed of that we can run as systemd service and it will run in background

--> Prometheus installation systemd

	sudo useradd --no-create-home --shell /bin/false prometheus --> Create a user.
	
	sudo mkdir /etc/prometheus --> To store prometheus yaml file.
	
	sudo mkdir /var/lib/prometheus --> To store all the time series data.
	
	sudo chown prometheus:prometheus /etc/prometheus --> Give permissions
	
	sudo chown prometheus:prometheus /var/lib/prometheus --> Give permissions

    Download the binary for prometheus --> wget URL
	
	tar -xvf prometheus --> untar the file
	
	sudo cp prometheus /usr/local/bin --> copy executables to bin

	sudo cp promtool /usr/local/bin --> copy executables to bin
	
	sudo chown prometheus:prometheus /usr/local/bin/prometheus

	sudo chown prometheus:prometheus /usr/local/bin/promtool
	
	sudo cp -r consoles /etc/prometheus --> for dashboard and visualization
	
	sudo cp -r console_libraries /etc/prometheus
	
	sudo chown -R prometheus:prometheus /etc/prometheus/consoles
	
	sudo chown -R prometheus:prometheus /etc/prometheus/console_libraries
	
	sudo cp prometheus.yaml /etc/prometheus/
	
	sudo chown  prometheus:prometheus /etc/prometheus/prometheus.yaml
	
	sudo -u prometheus /usr/local/bin/prometheus \
		--config.file /etc/prometheus/prometheus.yaml \
		--storage.tsdb.path /var/lib/prometheus/ \
		--web.console.templates=/etc/prometheus/consoles \
		--web.console.libraries=/etc/prometheus/console_libraries
		
--> Create a unit file for prometheus

	sudo vi /etc/systemd/system/prometheus.service
	
	[Unit]
	Description=Prometheus
	Wants=network-online.target
	After=network-online.target
	
	[Service]
	User=prometheus
	Group=prometheus
	Type=simple
	ExecStart=/usr/local/bin/prometheus \
		--config.file /etc/prometheus/prometheus.yaml \
		--storage.tsdb.path /var/lib/prometheus/ \
		--web.console.templates=/etc/prometheus/consoles \
		--web.console.libraries=/etc/prometheus/console_libraries
		
	[Install]
	WantedBy=multi-user.target
	
--> sudo systemctl daemon-reload

--> sudo systemctl start prometheus

--> sudo systemctl enable prometheus

--> Node Exporter will responsible for collecting metrics on a linux host and presenting it and so that prometheus can scrape it.

--> wget Node Exporter URL

--> tar -xvf node_exporter --> Untar

--> cd node_exporter

--> ./node_exporter

--> to test --> curl localhost:9100/metrics

--> We need to configure node_exporter as systemd service

--> sudo cp node_exporter /usr/local/bin

--> sudo useradd --no-create-home --shell /bin/false node_exporter

--> sudo chown node_exporter:node_exporter /usr/local/bin/node_exporter

--> sudo vi /etc/systemd/system/node_exporter.service

	[Unit]
	Description=Node Exporter
	Wants=network-online.target
	After=network-online.target
	
	[Service]
	User=node_exporter
	Group=node_exporter
	Type=simple
	ExecStart=/usr/local/bin/node_exporter 
		
	[Install]
	WantedBy=multi-user.target
	
	
--> sudo systemctl daemon-reload

--> sudo systemctl start prometheus

--> sudo systemctl enable prometheus



--> Authentication and Encryption

	Node Exporter with TLS
	
	
	Assuming you are in node_exporter directory
	
	generate a certificate with openssl
	
	node_exporter.crt node_exporter.key
	
	Create a config.yaml file
	
	vi config.yaml --> Assuming creating conig.yaml in path as certificates path
	
	tls_server_config:
	  cert_file: node_exporter.crt
	  key_file: node_exporter.key
	  
	./node_exporter --web.config=config.yaml
	
	sudo mkdir /etc/node_exporter
	
	mv node_exporter.* /etc/node_exporter --> move crt and key
	
	sudo cp config.yaml /etc/node_exporter
	
	chown -R node_exporter:node_exporter /etc/node_exporter
	
	vi /etc/systemd/system/node_exporter.service 
	
	add below line
	
	ExecStart=/usr/local/bin/node_exporter --web.config=/etc/node_exporter/config.yaml
	
	system daemon-reload
	
	systemctl restart node_exporter
	
--> curl -k https://localhost:9100/metrics




--> Prometheus TLS config

	Copy node_exporter.crt to Prometheus server
	
	scp username@password@node:/etc/node_exporter/node_exporter.crt /etc/prometheus
	
	chown prometheus:prometheus node_exporter.crt
	
	vi /etc/prometheus/prometheus.yml
	
	scrape_configs:
	  - job_name: "node"
	    scheme: https
		tls_config:
		  ca_file: /etc/prometheus/node_exporter.crt
		  insecure_skip_verify: true
		static_configs:
		 - targets: ["192.168.1.168:9100"]
		 
		 
--> systemctl restart prometheus

--> Prometheus Authorization

	Auth configuration Node Exporter
	
	sudo apt install apche2-utils
	
	htpasswd -nBc 12 "" | tr -d':\n'
	
	It will ask for password and give hash password.
	
	Copy that hash password
	
	vi /etc/node_exporter/config.yml
	
	
	tls_server_config:
	  cert_file: node_exporter.crt
	  key_file: node_exporter.key
	basic_auth_users:
	  prometheus: hashpassword
	  
--> systemctl restart node_exporter

	Auth configuration Prometheus
	
	
	vi /etc/prometheus/prometheus.yml
	
	scrape_configs:
	  - job_name: "node"
	    scheme: https
		basic_auth:
		  username: xxxxxx
		  password: xxxxxx
		tls_config:
		  ca_file: /etc/prometheus/node_exporter.crt
		  insecure_skip_verify: true
		static_configs:
		 - targets: ["192.168.1.168:9100"]
	
	systemctl restart prometheus
	
	
--> Every metric with combination of Unique labels is a time series.
	
-->  Prometheus Metrics

	<metric_name>[{<label_1="value_1">,<label_N="value_N">}]<metric_vale>
	
	labels provide info on which this metric is for and what the state
	
	metric_vale is the current value
	
--> Metrics have type and help attributes

		Help --> description of what the metric is
		
		Type --> specifies what type of metric
		
			4 different types of metrics
			
			Counter --> How many times did X happen
			Gauge   --> What is the current value of X
			Histogram --> How long or How big something is
			Summary --> Similar to histogram, How many observations fell below X
			
			
	Metric Rules:
			
			
--> Metric name specifies a general feature of a system to be measured

--> May contain ASCII letters, numbers, underscores and colons

--> Must match regex [a-zA-Z_:][a-zA-Z0-9_:]*

--> Colons are reserved only for recording rules



	Labels:
	
--> Key-value pair associated with metric

--> Allows you to split up a metric by a specified criteria

--> Metric can have more than one label

--> ASCII letters, numbers, underscores
			
--> Must match regex [a-zA-Z0-9_:]*		

--> Metric name is just another label

	node_cpu_seconds_total{cpu=0}  =    {_name_=node_cpu_seconds_total,cpu=0}
	
--> Every metric is assigned 2 labels by default(instance and job)

--> Prometheus Configuration:

	global:
	  scrape_interval: 1m
	  scrape_timeout: 10s
	  
	scrape_configs:
	  - job_name: "node"
	    scrape_interval: 15s
		scrape_timeout: 15s
		scheme: https
		metrics_path: /stat/metrics
		sample_limit: 1000
		static_configs:
		  - targets: ['172.16.12.1:9090']
		  
    
	alerting: 
	
	
	rule_files: 
	
	
	remote_read: 
	remote_write: 
	
	
	storage: 
	
	
--> When you change the prometheus config file, you need to restart the prometheus.

--> systemctl restart prometheus

-->Promtools is a utility tool shipped with Prometheus that can be used to:

	Check and validate configuration
	
	Validate Prometheus.yml
	Validate rules files
	Validate metrics passed to it are correctly formatted
	can perform queries on a prometheus server
	Debugging and Profiling a prometheus server
	Perform unit tests
	
--> For example to validate the prometheus.yaml

	promtool check config /etc/prometheus/prometheus.yaml
	
--> Metrics can also be scraped from containerized environments

	Docker Engine metrics, Container metrics using cAdvisor
	
--> To enable Docker Engine Metrics

	vi /etc/docker/daemon.json
	
	daemon.json
	
	{
	"metrics-addr": "127.0.0.1:9323",
	"experimental": true
	}
	
--> sudo systemctl restart docker

--> curl localhost:9323/metrics

		cAdvisor metrics
		
--> $ vi docker-compose.yml

	docker-compose.yml
	
	version: '3.4' 
	services:
	  cadvisor:
		image: gcr.io/cadvisor/cadvisor
		container_name: cadvisor
		privileged: true
		devices:
		  - "/dev/kmsg: /dev/kmsg"
		volumes:
		  - /:/rootfs:ro
		  -	/var/run:/var/run:ro
          -	/sys: /sys: ro
		  - /var/lib/docker/:/var/lib/docker:ro
		  - /dev/disk/:/dev/disk:ro
		ports:
		8080:8080
	
--> $ docker-compose up
	$ curl localhost:8080/metrics
	
--> Docker Engine metrics:

	âœ“ How much cpu does docker use
	âœ“ Total number of failed image builds
	âœ“ Time to process container actions
	âœ” No metrics specific to a container
	
-->	cAdvisor metrics:

	âœ“ How much cpu/mem does each container use
	âœ“ Number of processes running inside a container
	Container uptime
	âœ“ Metrics on a per container basis
	
--> PromQL Data Types

	A PromQL expression can evaluate to one of four types:
	
	âœ“ String - a simple string value (currently unused)
	âœ“ Scalar a simple numeric floating point value 
	âœ” Instant Vector - set of time series containing a single sample for each time series, all sharing the same timestamp
	âœ“ Range Vector - set of time series containing a range of data points over time for each time series
	
	
--> Range Vector returns metrics over the course of certain time period.

--> If you want all time series from node1

	node_filesystem_avail_bytes{instance="node1"}
	
--> We can define multiple

	node_filesystem_avail_bytes{instance="node1", device!="tmpfs"}
	
--> for range vectors

	node_arp_entries{instance="node1"}[2m]
	
--> To get historic data use offset modifier after the label matching

	node_memory_Active_bytes{instance="node1"} offset 5m
	
	It will return value 5 minutes ago
	
-->  node_memory_Active_bytes{instance="node1"} @1663265188

	To go exactly at that particular time       Above is UNIX time stamp --> Sep 15,2022 6:06:28 PM GMT
	
--> node_memory_Active_bytes{instance="node1"}[2m] @1663265188 offset 10m

	get 2 minutes worth of data 10 minutes before Sep 15,2022 6:06:28 PM GMT

--> 
	Binary operator precedence
	When an PromQL expression has multiple binary operators, they follow an order of precedence, from highest to lowest:
	
	
	1. ^
	2.*,/, %, atan2
	3. +, -
	4. ==, !=, <=, <, >=, >
	5. and, unless
	6. or
	
	Operators on the same precedence level are left-associative. For example, 2 * 3 % 2 is equivalent to (2 * 3) % 2.
	However ^ is right associative, so 2 ^ 3 ^ 2 is equivalent to 2 ^ (3 ^2)	
	
	
--> We can use ignoring keyword to ignore

Ignoring keyword can be used to "ignore" an labels to ensure there is a match between 2 vectors

$http_errors

http_errors{method="get", code="500"} 
http_errors{method="put", code="501"} 
http_errors{method="post", code="500"}
40
23
60
3000

$http_requests

http_requests {method="get"}
http_requests{method="del"}
http_requests {method="post"}
421
288
372

$http_errors{code="500"} / ignoring(code) http_requests

--> With the above same example we can tell what labels to match

	$http_errors{code="500"} / on(method) http_requests
	
--> To scrape metrics from the application running on instance.

	The prometheus client libraries provide an easy way to add instrumantation to your code in order to track and expose metrics for prometheus
	
	1. Track metrics In the prometheus expected format
	2. Expose metrics via /metrics path
	
--> We need to import prometheus_client in our to expose our application metrics



--> Service Discovery:

Service Discovery allows Prometheus to populate a list of endpoints to scrape that can get dynamically updated as new endpoints get created & destroyed.

--> Prometheus has built in support for several service discovery mechanism.

--> File Service Discovery:

A list of jobs/targets can be imported from a file

This allows you to integrate with service discovery systems Prometheus doesn't support out of the box 

Supports json and yaml files


--> Service Discovery with AWS

Prometheus.yml

	scrape_configs:
	  â€” job_name: EC2
		ec2_sd_configs:
		 - region: <region>
		   access_key: <access key> 
		   secret_key: <secret key>
		
--> Credentials should be setup with an IAM user with the Amazon EC2ReadOnlyAccess policy.

--> Re-labeling allows you to filter Prometheus targets and metrics by rewriting their lable set.

--> re labeling occurs before scrape occurs and only has access to labels added by service discovery

--> 	scrape_configs:
	      â€” job_name: EC2
		    relabel_configs:-->re labeling occurs before scrape occurs and only has access to labelsadded by service d
			metric_relabel_configs:-->this re labeling occurs after scrape
		    ec2_sd_configs:
			 - region: <region>
			   access_key: <access key> 
			   secret_key: <secret key>
			   
--> Prometheus.yml

	scrape_configs:
	  â€” job_name: EC2
		relabel_configs:
		 - source_labels: [__meta_ec2_tag_env] --> Source_labels â€“ array of labels to match on
		   regex: prod --> Regex - uses regular expressions to match on a specific value
		   action: keep|drop|replace --> Keep means we will scrape target if it has the source_label, drop means we will not scrape target Doing a keep here means any ec2 instance where this label is not set to prod will not be scraped
		  
		  You can specify any one.
		  
--> scrape_configs:
	  â€” job_name: EC2
		relabel_configs:
		 - source_labels: [env,team] --> To match {env="dev"} and {team="marketing"}
		  regex: dev;marketing
		  action: keep
		  
--> scrape_configs:
	  â€” job_name: EC2
		relabel_configs:
		 - source_labels: [env,team] --> To match {env="dev"} and {team="marketing"}
		  regex: dev-marketing
		  action: keep
		  sepeartor: "-"
		  
--> Prometheus.yml

	scrape_configs: 
	 - job_name: example 
	   relabel_configs:
	    - source_labels: [____address___] --> __address____=192.168.1.1:80
		  regex: (.*):.*
	      target_label: ip 
		  action: replace 
		  replacement: $1


EC2 instances have label:

Let's say we want to convert this to the following label
{ip="192.168.1.1"}

The action property should be set to replace

target_label should be the name of the new label

regex is set to (.*):.* which will assign everything before the into a group(can be referenced with $1)

--> scrape_configs:
	  â€” job_name: EC2
		relabel_configs:
		  - regex: __meta_ec2_owner_id
		    action: labeldrop
			
--> To drop the __meta_ec2_owner_id discovery label.

--> scrape_configs:
	  â€” job_name: EC2
		relabel_configs:
		  - regex: instance|job
		    action: labelkeep
			
--> labelkeep can be used to specify labels that should be kept.

--> Note All other labels will be dropped.

--> in above example only instance & job labels will be kept all other labels will be dropped.

-->
	scrape_configs: 
	 - job_name: example 
	   relabel_configs:
		 - regex: __meta_ec2_(.*) 
		   action: labelmap 
		   replacement: ec2_$1
		   
--> If you find __meta_ec2_ami , it will create a new label --> ec2_ami

--> metric_relabel_configs:

	scrape_configs: 
	 - job_name: example 
	   metric-relabel_configs:
		 - source_labels: [__name__]
		   regex: http_errors_total 
		   action: replace
		   target_label: __name__
		   replacement: http_failures_total
		   
--> To rename metric from http_errors_total to http_failures_total

--> 

	scrape_configs: 
	 - job_name: example 
	   metric-relabel_configs:
		 - regex: code 
		   action: labeldrop
		   
--> to drop a label named code

--> Push gateway acts as a middleman between the batch job and the prometheus server.

--> Installing Push gateway:

	wget tar file URL
	
	tar xvf pushgateway
	
	cd pushgateway
	
	./pushgateway
	
--> Create a systemd service.

	
$ sudo useradd --no-create-home --shell /bin/false pushgateway
$ sudo cp pushgateway /usr/local/bin
$ chown pushgateway:pushgateway /usr/local/bin/pushgateway


$ vi /etc/system/system/pushgateway.service

[Unit]
pushgateway.service
Description=Prometheus Pushgateway
Wants-network-online.target
After=network-online.target
[Service]
User-pushgateway
Group-pushgateway
Type=simple
ExecStart=/usr/local/bin/pushgateway
[Install]
WantedBy=multi-user.target

--> $ sudo systemctl daemon-reload
	$ sudo systemctl restart pushgateway
	$ sudo systemctl enable pushgateway

--> curl localhost:9091/metrics

--> We need to configure prometheus to scrape from pushgateway

--> 	scrape_configs:
	      - job_name: pushgateway
		    honor_labels: true
			static_configs:
			  - targets: ["192.168.1.168:9091"]
			  
--> Metrics can be pushed to the Pushgateway using one of the following methods:

	Send HTTP Requests to Pushgateway
	Prometheus Client Libraries
	
-->  HTTP

Send HTTP POST request using the following URL :
http://<pushgateway_address>:<port>/metrics/job/<job_name>/<label>/<value>/<label2>/<value2>

--> <job_name> will be the job label of the metrics pushed
--> <label>/<value> in the url path is used as a grouping key - allows for grouping metrics together to update/delete multiple metrics at once
			
--> Push metric "example_metric 4421" with a job label of {job="db_backup"}

	$ echo "example_metric 4421" | curl --data-binary @- http://localhost:9091/metrics/job/db_backup
	
-->	Metric data has to be passed in as binary data with the -data-binary flag
-->	@- tells curl to read the binary data from std-in which is the output of the echo command

--> When sending a POST request, only metrics with the same name as the newly pushed metrics are replaced (this only applies to metrics in same group)

--> When sending a PUT request, all metrics with in specific group get replaced by the new metrics being pushed.

--> DELETE request delete all metrics within a group.

--> Metrics can be pushed to the Pushgateway through client libraries 

There are 3 functions within a client library to push metrics:

push --> Any existing metrics for this job are removed and the pushed metrics are added --> Like PUT

pushadd --> Pushed metrics override existing metrics with same names. All other metrics in group remain unchanged
--> Like POST

--> delete --> All metrics for a group are removed --> Like DELETE


--> Prometheus is only responsible for triggering alerts. It does not send notifications such as emails, text messages.

--> That responsibility is offloaded onto a separate process called Alertmanager

--> rules.yml

	groups:
	  - name: node
		rules:
		  -	alert: node down
			expr: up{job="node"} == 0
			for: 5m

--> The for clause tells Prometheus that an expression must evaluate to true for a specified period of time before firing alert.

--> Labels can be added to alerts 

	groups:
	  - name: node
		rules:
		  -	alert: node down
			expr: up{job="node"} == 0
			labels:
			  severity: warning
			  
--> Annotations can be used to provide additional information.

	groups:
	 - name: node
	   rules:
	    - alert: node_filesystem_free_percent
		  expr: 100 * node_filesystem_free_bytes{job="node"} / node_filesystem_size_bytes{job="node"} < 70
	      annotations:
		   description: "filesystem {{.Labels.device}} on {{.Labels.instance}}is low on space, current available space is {{.Value}}"
		   
--> Alertmanager installation:

	
$ wget https://github.com/prometheus/alertmanager/releases/download/v0.24.0/alertmanager-0.24.0.linux-amd64.tar.gz

$ tar xzf alertmanager-0.24.0.linux-amd64.tar.gz

$ cd alertmanager-0.24.0.linux-amd64

./alertmanager

--> we need to configure prometheus to use that alertmanager.

prometheus.yml

		global:
		 scrape_interval: 15s
		 evaluation_interval: 15s
		alerting:
		  alertmanagers:
		    - static_configs:
		        - targets:
				  -	alertmanager1:9093
				  -	alertmanager2:9093
		rule_files:
		  - "rules.yml"
		scrape_configs:
		
--> Creating a systemd service

--> sudo useradd --no-create-home --shell /bin/false alertmanager

--> sudo mkdir /etc/alertmanager

--> sudo mv alermanager.yml /etc/alertmanager

--> sudo chown -R alermanager:alermanager /etc/alertmanager

--> sudo mkdir /var/lib/etc/alertmanager

--> sudo chown -R alermanager:alermanager /var/lib/etc/alertmanager
	
--> sudo cp alertmanager /usr/local/bin

--> sudo cp amtool /usr/local/bin

--> sudo chown alermanager:alermanager /usr/local/bin/alertmanager

--> sudo chown alermanager:alermanager /usr/local/bin/amtool

--> sudo vi /etc/systemd/system/alertmanager.service

	[Unit]
	Description=alertmanager
	Wants=network-online.target
	After=network-online.target
	
	[Service]
	User=alertmanager
	Group=alertmanager
	Type=simple
	ExecStart=/usr/local/bin/alertmanager \
		--config.file=/etc/alertmanager/alertmanager.yml
		--storage.path=/var/lib/alertmanager
	Restart=always
		
	[Install]
	WantedBy=multi-user.target
	
--> sudo systemctl daemon-reload

--> sudo systemctl start alertmanager

--> sudo systemctl enable alertmanager

--> alertmanager.yml

	global:
	  smtp_smarthost: 'mail.example.com:25' 
	  smtp_from: 'test@example.com'
	route:
	  receiver: staff
	  group_by: ['alertname', 'job']
	  routes: 
	  - match_re:
		job: (node windows) 
		receiver: infra-email 
	  -	matchers:
	      job: kubernetes
	    receiver: k8s-slack
	receivers:
	- name: 'k8s-slack'
	  slack_configs:
	  - channel: '#alerts'
	    text: https://example.com/alerts/{{.GroupLabels.app }}/
		
--> Global section applies global configuration across all sections which can be overwritten

--> The route section provides a set of rules to determine what alerts get matched up with which receivers

--> A receiver contains one or more notifiers to forward alerts to users


--> Sub routes:

	
	route:
	  routes:
	   # database team.
	   - match:
	       team: database
	     receiver: database-pager
		 routes:
		  - match:
			 severity: page
		    receiver: database-pager
		  - match:
		     severity: email
		    receiver: database-email
	   #api team.
		- match:
		    team: api
		  receiver: api-pager
	      routes:
		   - match:
			  severity: page
			  env: dev
			 receiver: api-ticket
		   - match:
		      severity: page
			 receiver: api-pager
	       - match:
			  severity: ticket
	         receiver: ap ticket

--> route:
	  routes:
	  - receiver: alert-logs
	    continue: true
	  - matcher:
	       team: database
	    receiver: database-pager
		
--> By default When first alert matches it will stop going below, to continue we can put continue: true

--> Prometheus.yml

	
	global:
	  scrape_interval: 15s 
	  scrape_timeout: 10s
	  
	rule_files:
	  - "rules.yaml"
	  
	alerting:
	  alertmanagers:
	    - static_configs:
	        - targets:
				- localhost:9093
				
	scrape_configs:
	  - job_name: "prometheus"
		static_configs:
	      -	targets: ["localhost:9090"]
	  - job_name: "node"
	    static_configs:
	      - targets: ["192.168.1.168:9100"]
	  - job_name: "dbl"
	    static_configs:
	      - targets: ["192.168.1.168:9200"]
	  - job_name: "db2"
	    static_configs:
	      - targets: ["192.168.1.168:9300", "192.168.1.168:9400"]
		  
		  
--> Monitoring Kubernetes:

--> Monitor applications running on Kubernetes infrastructure
--> Monitor Kubernetes Cluster
	Control-Plane Components (api-server, coredns, kube-scheduler) Kubelet(cAdvisor) â€“ exposing container metrics

	Kube-state-metrics - cluster level metrics(deployments, pod
	metrics)
	Node-exporter - Run on all nodes for host related metrics(cpu, mem, network)
	
--> By default cluster level metrics are not available.

	To collect cluster level metrics (Pods, deployments) the kube-state-metrics container must be deployed
	
--> Prometheus Chart Overview:

	--> statefulset.apps/prometheus-prometheus-kube-prometheus-prometheus --> This container is our prometheus server.
	
	--> statefulset.apps/prometheus-prometheus-kube-prometheus-alertmanager --> AlertManager instance
	
	--> deployment.apps/prometheus-grafana --> this will automatically install grafana
	
	--> deployment.apps/prometheus-kube-prometheus-operator --> handles the lifecycle of prometheus instance like updating configs, restarting the process.
	
	--> deployment.apps/prometheus-kube-state-metrics --> metrics about deployments, services, pods this container will scrape the data and expose as metrics.
	
	--> daemonset.apps/prometheus-prometheus-node-exporter --> responsible for deploying node exporter pod on every single node.
	
	
	
--> kubectl get crd 

	prometheuses.monitoring.coreos.com --> Used to create prometheus instance
	
	servicemonitors.monitoring.coreos.com --> Add additional targets for prometheus to scrape.
	
--> Service Monitor define a set of targets for prometheus to monitor and scrape.

--> kubectl get prometheuses.monitoring.coreos.com -o yaml 

	serviceMonitorNamespaceSelector: {}
	   serviceMonitorSelector:
		 matchLabels:
		   release: prometheus
		   
--> This prometheus label allows prometheus to find service monitors in the cluster and register them so that it can start scraping the application the servicemonitor is pointing to.

--> To add Rules, the prometheus operator has a CRD called prometheusrules which handles registering new rules to a prometheus instance.

--> To add AlertmanagerRules, the prometheus operator has a CRD called AlertmanagerConfig which handles registering new rules to a Alertmanager.

--> kubectl get alertmanagers.monitoring.coreos.com -o yaml

	spec:
	    alertmanagerConfigNamespaceSelector: {}
		alertmanagerConfigSelector: {}
		
--> This label allows Alertmanager to find AlertmanagerConfig objects in the cluster and register them

--> the helm chart by default does not specify a label, so we will have to go into the chart and update this value.

--> When deploying Alertmanager, it needs to discover the AlertmanagerConfig CRDs so it can register and process alerts.

--> PrometheusRule â†’ Defines alerts (when an alert should fire).

--> AlertmanagerConfig â†’ Processes alerts (where the alert should be sent and how it should be handled).



####################################    Grafana  Loki  ##########################################



--> What is Loki?

	Loki is a log aggregation system designed to store and query logs
	Designed to be cost effective and easy to operate
	â€¢ Loki does not index full text from logs (like Elasticsearch)
	â€¢ Loki only indexes labels (metadata) from logs
	â€¢ Makes it more cost effective and performant
	â€¢ Configuration and query language similar to Prometheus
	
--> To get the logs from application we need to install client in the host system. that client will collect the logs from application

--> We can use Promtail as a client.



#####################################    Certified Kubernetes Administrator ######################################



For study purpose go to https://github.com/SureshBisadi/CKA-2024/tree/main/Resources


--> Instead of writing Yaml file, we can dry run and save that file output in yaml file

	kubectl run nginx --image=nginx --dry-run=client -o yaml > pod.yaml
	
--> kubectl explain rs --> Gives you information about replication controller.

--> Replication controller is a legacy one and replicaset is a new one.

--> In Replication controller it will manage the pods created by it, but in ReplicaSet it will manage the pods Which were not part of the replicaset by using selector field.

--> Deployment manages the ReplicaSet and ReplicaSet manges the pods.

--> When you create a deployment it will automatically create ReplicaSet.

--> The yaml for deployment and replicaSet is almost same.

--> NodePort --> Can be accessed from externally 

--> port --> through which other services can talk to this pod

--> Targetport --> The actual application is listening on


--> When you don't specify the namespace it will be a default namespace. All the control plane components will be created in kube-system namespace.

--> Pods with in the namespace can communicate with each other.

--> In k8s cluster Ip addresses are at cluster level.

--> exposed services are at namespace level, means you can't access the service whihc is in another namespace until unless if you use Full Qualified NameSpace.

--> All the yaml files of static pods or control plane pods are stored at --> /etc/kuberenetes/manifest

--> Because kubelet will monitor that particular path only.

--> We can also schedule the pod to which worker it should go by specifying nodeName in pod.yaml

--> A taint is a way to mark a node so that pods will avoid being scheduled on it, unless the pod explicitly tolerates the taint

--> Taint we will do it on Node, toleration we will do it on Pod.

--> Along with tolerations we have effects. We have 3 types of effects

	noSchedule, preferNoSchedule, noExecute
	
--> NoSchedule --> checks only newer pods 

--> noExecute --> checks existing + newer pods

--> preferNoSchedule --> No guarantee

--> To add label to node --> kubectl label node node-name gpu=false

--> Taint will restrict that node which pod to accept, while labels checks on which node it can run.

--> taint does not guarantee pod to be scheduled, thats why we use node affinity it will guarantee pod to be scheduled on one of the nodes.

--> By using nodeSelector we can scedule our pod on which node to run.

--> HPA --> Horizontal Pod Autoscaling --> adding extra pods to the existing pod.

--> VPA --> Vertical Pod Autoscaling --> resizing the Pod(Like increasing the Memory and CPU)

--> VPA needs restart, so your application may face some downtime.



--> Health  Probes:

	startup  --> for slow/legacy apps
	
	readiness --> ensure the application is ready
	
	liveness --> restart the application if fails
	
--> Roles are namespace scoped.

--> Namespace scoped --> pods,deployments,replicasets,services  --> Roles

--> Cluster scoped --> Namespace, Nodes --> Cluster Roles

--> In kubernetes we have 2 types of users.

	1. Human: 
	
	2. Service-account: Used by non-human like Jenkins, prometheus.
	
--> What is a Service Account in Kubernetes?

Default Identity for Pods: Every pod in Kubernetes runs under a service account.

Purpose: Itâ€™s used to authenticate and authorize API requests your pod might make to the Kubernetes API server (e.g., listing pods, accessing secrets, etc.).

--> What are "Secrets" in a Service Account?

When you create a service account, Kubernetes automatically:

Creates a Secret that contains:

JWT token (used to authenticate with Kubernetes API)

CA certificate (for secure communication)


--> A StorageClass in Kubernetes provides a way to describe the "class" of storage you want.

--> Flow of Dynamic Volume Provisioning in Kubernetes

You write a PVC, asking for some amount of storage and referencing a StorageClass.

Kubernetes sees the PVC, and goes:

"Okay, no existing PV matches this... but there's a StorageClass linked. I'll use that to create a new PV!"

The StorageClass (which has a provisioner like kubernetes.io/aws-ebs, pd-standard for GCP, etc.) tells the cloud or local system to create the actual disk.

That disk becomes a new PersistentVolume (PV).

The PVC and PV get automatically bound, and then the pod can mount the volume via the claim.


--> Kubernetes Networking:


	Before diving into the Kubernetes specifics, here are some networking principles that Kubernetes is built on:

	Pod-to-Pod Communication	Every pod must be able to talk to every other pod in the cluster â€” flat network.
	
	Non-NATed	Communication between pods should not require NAT.
	
	Unique Pod IPs	Each pod gets its own IP (like a mini VM).
	
	Service Abstraction	Pods are ephemeral, so Services give a stable way to access them.
	
	
--> 

						Component	                           Role
						
						Pod						Has its own network namespace and IP
						Service					Stable access point to pods
						Endpoints				Tracks which pod IPs are behind each Service
						kube-proxy				Maintains network rules on nodes for Service load-balancing
						CNI Plugin				Implements networking (IP allocation, routing, etc.)
						CoreDNS					DNS service inside cluster
						Ingress					Manages external traffic into the cluster
						
					
--> What Are CNI Plugins?

Container Network Interface (CNI) plugins extend Kubernetes by providing network connectivity to pods. They configure network interfaces within pods and ensure communication across nodes.

--> Node Readiness: A Kubernetes node remains in a NotReady state until a CNI plugin is installed. Without it, pods cannot receive proper networking configuration, causing them to remain in a pending state.

--> Connectivity Assurance: Core components like kube-proxy depend on the CNI plugin to set up pod-level networking rules.

--> Function and Responsibilities

kube-proxy runs on every node and manages the network rules necessary for service load balancing and pod communication. It dynamically updates iptables (or IPVS rules) based on the Kubernetes service definitions, ensuring that requests are correctly routed to pod endpoints.

--> Overlay Networking

Overlay networks are virtual networks built on top of an existing physical network. They use encapsulation techniques (such as VXLAN or GRE) to transport packets between pods across different nodes. 

This approach abstracts the underlying network, enabling a consistent pod IP addressing scheme without requiring changes to physical infrastructure.

--> When Kubernetes creates a pod, it creates a mini-network for that pod.

This includes:

A network namespace for the pod

A virtual ethernet (veth) pair

A bridge (cni0) on the host


--> 1. eth0 â€” Inside the Pod

âœ… This is what the pod sees as its main interface:

It gets an IP like 10.244.1.10

It sends and receives traffic via eth0

So from inside the pod:

ip a

Shows:

eth0: inet 10.244.1.10/24


But eth0 is just one end of a virtual pipe...


ðŸ”Œ 2. vethXXXX â€” On the Host (Node)

This is the other end of the veth pair.

Think of this like:

[ pod eth0 ] <== pipe ==> [ host vethXXXX ]

Traffic sent from the pod goes into eth0 â†’ exits via vethXXXX on the host

This is how the host can route traffic into/out of the pod

ðŸ“ vethXXXX lives on the node, inside the root network namespace.

ðŸ§° 3. cni0 â€” The Virtual Bridge on the Host

Imagine cni0 as a virtual switch (a software network switch on the host).

Why do we need it?

It connects all pods on the same node together

It's the central place where all pod veth interfaces plug into

It allows inter-pod communication on the same node without needing tunnels or NAT

ðŸ“ cni0 also lives on the host, created by the CNI plugin.

ðŸ§ª How It All Connects â€” Visual Diagram

eth0 (Pod A) â†â†’ [vethABC1] â†â†’ cni0 (bridge) â†â†’ [vethXYZ2] â†â†’ eth0 (Pod B)


So when Pod A sends a packet to Pod B:

Packet goes out eth0 inside Pod A

Travels through vethABC1 to cni0 bridge

Bridge sees itâ€™s meant for vethXYZ2 (Pod B)

Packet goes into Pod Bâ€™s eth0

âœ”ï¸ Fast, no NAT, no tunnel â€” all inside one Linux kernel.


--> node worker1 drain --> drain means the existing workloads will be removed and no new workloads will be applied.

--> drain -- evicting the pods
	cordon -- unschedulable
	
--> Upgrade strategies:

	All at once
	rolling update (One at a time)
	Blue green
	
--> first we need to update version on /etc/apt/sources.list.d/kubernetes.list







###########################################   GitHub Actions   ################################################

				Trigger   		   Use
		Events -------> Workflows -------> Actions



--> Github checks for yaml file which is at .github/workflows/

--> Events:

	Github triggered events --> push, pull_request, public
	
	Scheduled events --> schedule
	
	Manually triggered --> workflow_dispatch (external_systems)
	
--> Core components of Github actions:

	Workflow
	Jobs
	Steps
	
	
	
	#######################################  Hashicorp vault ###############################################
	
	
--> Basic vault CLI commands:

	vault  --> to list of many vault CLI commands
	
	vault version --> version of the vault
	
	vault read  --> used to read secrets from vault
	
	vault write --> write secrets to vault
	
--> Vault server modes --> Dev mode and Prod mode


--> Vault architecture:

			
				API
				 |
	Barrier	-->	Core
				 |
			Storage backend
			
--> Vault data is encrypted using the encryption key in the keyring, 
	the keyring is encrypted by the master key
	the master key is encrypted by the unseal key.
	
--> To run a production vault server --> vault server -config=./vault-config.hcl

--> Most Vault auth methods need to be explicitily enabled

--> This is done with the vault auth enable command





#####################################################################################



| Component               | Resource Kind                                          | Pod?               | Description                                                                                                    |
| ----------------------- | ------------------------------------------------------ | ------------------ | -------------------------------------------------------------------------------------------------------------- |
| ðŸ§  `controller-manager` | `Deployment` â†’ Pod                                     | âœ… YES              | The **main controller pod**. It does all the CRD watching, GitHub communication, runner management             |
| ðŸ” GitHub App Secret    | `Secret`                                               | âŒ NO               | Just a **Kubernetes Secret** you create manually (holds GitHub App ID, install ID, and private key)            |
| ðŸ“„ CRDs                 | `CustomResourceDefinition`                             | âŒ NO               | These are **API extensions** that define new K8s resource types like `Runner`, `RunnerDeployment`              |
| ðŸ›¡ï¸ RBAC                | `ServiceAccount`, `Role`, `ClusterRole`, `RoleBinding` | âŒ NO               | These are **permissions configurations**, not pods â€” they tell Kubernetes what the controller is allowed to do |
| ðŸŒ `webhook` (optional) | `Deployment` â†’ Pod (with Ingress)                      | âœ… YES (if enabled) | A separate optional **pod** that listens to GitHub webhook events to trigger scaling                           |
| ðŸ“Š Metrics endpoint     | Part of controller pod or exposed via `ServiceMonitor` | âœ… YES (inside pod) | This is not a separate pod, it's part of the `controller-manager` pod â€” exposed via `/metrics` endpoint        |




When you install ARC via Helm:

ðŸ§± Pods You Get (in actions-runner-system namespace):
Pod	Purpose
controller-manager	âœ… Main controller: manages all ARC CRDs
webhook (optional)	Optional: receives GitHub webhook events for autoscaling

ðŸ“„ CRDs Installed:
CRD	Purpose
Runner	Represents a single GitHub runner (pod)
RunnerDeployment	Replica-style runner controller (like a Deployment)
RunnerReplicaSet	Underlying object managed by RunnerDeployment
RunnerScaleSet	GitHub-native autoscaling scale set definition
AutoScalingRunnerSet	Advanced autoscaling object for just-in-time runners
EphemeralRunnerSet	Temporary CRD used for job-driven runners







####################################### IMP ############################################




--> pod gets ip from CNI plugin 

--> Services gets ip from kubeserver-api.yaml --> there we have services cidr range

--> to check cni cidr range --> check daemon sets

because calico will run as daemon sets on every node.

--> kubectl get ds -n kubesystem

--> Then check describe the pod and check.

--> If you don't find any ipv4 thing, it taking defaulty from CRDs.

--> To check CRD  kubectl get crd -n kube-system

--> then you have ippools.crd.projectcalico.org

--> kubectl get crd ippools.crd.projectcalico.org -o yaml

--> If you don't find here check the Ippool

--> kubectl get ippool

--> kubectl get ippool default-ipv4-ippool -o yaml


Kubernetes Resources: Cluster-Level vs Namespace-Level


âœ… Cluster-Level Resources (NOT tied to any namespace)

These are unique to the entire cluster and managed globally:

Resource									Description

Node									Physical/virtual machines in the cluster
Namespace								Logical partitions of the cluster
ClusterRole								Role with permissions cluster-wide
ClusterRoleBinding						Binds a ClusterRole to users/groups cluster-wide
PersistentVolume (PV)					Storage volume not tied to a namespace
CustomResourceDefinition				Defines custom APIs
StorageClass							Defines how dynamic volumes are provisioned
CertificateSigningRequest				TLS cert requests (not namespaced)


ðŸ“¦ Namespace-Level Resources (scoped inside a namespace)

These are the most common and must be created within a namespace:

Resource									Description

Pod										Unit of execution
Deployment								Manages replica sets of pods
DaemonSet								Pod on every node
Service									ClusterIP, NodePort, LoadBalancer
Ingress									Routes external HTTP(S) traffic
ConfigMap								Key-value pairs (non-sensitive)
Secret									Encrypted config
ServiceAccount							Pod identity
Role									Namespaced permission set
RoleBinding								Bind Role to user/service account
PersistentVolumeClaim (PVC)				Request for storage within a namespace




--> In secrets if we use data we need use base64 values and if we use stringData we can use plain texts.

apiVersion: v1
kind: Secret
metadata:
  name: app-secret
type: Opaque
stringData:
  username: Suresh
  password: "1234"

################################################################################



1. Core Argo CD Components (Common to Both Installs)

Type							Name											Purpose

Deployment						argocd-server									Serves UI/API/CLI
Deployment						argocd-repo-server						Clones Git repos, renders manifests
Deployment					argocd-application-controller				Watches Applications, performs sync
Deployment					argocd-dex-server							OIDC login (optional)
Deployment				argocd-applicationset-controller			Dynamic Application generation (optional)
StatefulSet					argocd-redis										Caching layer

ðŸ“¦ 2. Secrets (Auto-Generated or Required)

Secret Name														Purpose										

argocd-secret							Core configuration: server certs, session key, admin password (legacy)
argocd-initial-admin-secret				Stores the default admin password (base64)
argocd-ssh-known-hosts					Git SSH known hosts	Auto-populated
cluster-<id>			Access credentials for added clusters (argocd cluster add)	Only in install.yaml setup

ðŸ“ 3. ConfigMaps

ConfigMapName									Purpose

argocd-cm										Core settings (RBAC, URL, features, branding, etc.)
argocd-rbac-cm									RBAC policies
argocd-cmd-params-cm							Environment parameters for components
argocd-ssh-known-hosts-cm						Known SSH hosts for Git

ðŸ§© 4. CRDs (CustomResourceDefinitions)
	
CRD												Description

applications.argoproj.io						The core Application resource
appprojects.argoproj.io							Logical grouping and policy control for apps
applicationsets.argoproj.io						Auto-generates Applications from Git/cluster templates

ðŸ” 5. RBAC Resources

ðŸŸ¢ install.yaml (cluster-wide)

Resource								Name													Scope

ClusterRole				argocd-server, argocd-application-controller, etc.					Cluster-wide
ClusterRoleBinding		Binds SAs to roles	Cluster-wide
ServiceAccount			argocd-server, argocd-repo-server, argocd-application-controller, etc.	In argocd namespace
Role + RoleBinding			For internal Redis and metrics										Namespace

ðŸŸ¡ namespace-install.yaml (scoped)

Resource													Name		
					
Role										Same as above, but only in your namespace	
RoleBinding									Binds internal SAs in your namespace	
âŒ No ClusterRole, ClusterRoleBinding		Fully namespace-isolated	
ServiceAccount								Same (controller, repo, server)	In the target namespace

ðŸŒ 6. Services (Network Access)

Service Name									Type								Description

argocd-server						ClusterIP / LoadBalancer / NodePort			Access Argo CD UI/API
argocd-repo-server							ClusterIP							Internal Git repo render
argocd-application-controller				ClusterIP							Internal sync component
argocd-redis								ClusterIP							Redis for internal cache

You typically expose argocd-server using Ingress or port-forward.


RBAC Policy Syntax Recap

p, <subject>, <resource>, <action>, <object>, <effect>


subject: user or role

resource: applications, appprojects, etc.

action: get, create, update, delete, sync, etc.

object: the name of the resource (can use wildcard *)

effect: allow or deny



To check logs that who created,  deleted  etc --> check deploy argocd-server logs

kubectl logs deploy/argocd-server -n argocd -f





######################################      Prometheus          ##################################



1. Custom Resource Definitions (CRDs)

These CRDs are part of the Prometheus Operator ecosystem and define new Kubernetes objects:

CRD Name										Description

Prometheus						Defines a Prometheus instance configuration
ServiceMonitor					Defines how to scrape metrics from a set of Services
PodMonitor						Defines how to scrape metrics from a set of Pods
Alertmanager					Defines an Alertmanager instance
PrometheusRule					Defines alerting and recording rules
ThanosRuler (optional)			Defines Thanos ruler instance

These CRDs let you manage Prometheus configs declaratively as Kubernetes resources.

ðŸ“¦ 2. Deployments

Deployment								Description

prometheus-operator				Controller that manages Prometheus, Alertmanager, and CRDs
prometheus-prometheus-*			The actual Prometheus server
prometheus-alertmanager-*		The Alertmanager deployment
prometheus-grafana				Grafana UI for visualization
prometheus-kube-state-metrics	Collects k8s object state as metrics
prometheus-node-exporter		DaemonSet running on each node to expose system metrics

ðŸ§¾ 3. ConfigMaps

These are used for configurations:

ConfigMap														Purpose

prometheus-prometheus-prometheus-rulefiles-*			Alerting and recording rules
prometheus-grafana-dashboards-*							JSON dashboards for Grafana
prometheus-grafana-datasource							Grafana datasource config (Prometheus)
prometheus-prometheus-prometheus-scrape-confg			Prometheus scrape targets
prometheus-alertmanager									Alertmanager configuration

ðŸ” 4. Secrets

Secret														Purpose

prometheus-grafana							Stores admin credentials for Grafana
TLS secrets	Optional â€” 						for enabling HTTPS in services
Alertmanager secret	Optional, 				if you use external notification integrations 

ðŸŒ 5. Services

Service								Type									Description

prometheus-operated					ClusterIP							Internal Prometheus service
alertmanager-operated				ClusterIP							Internal Alertmanager service
grafana							ClusterIP or LoadBalancer				Grafana UI access
prometheus-kube-state-metrics		ClusterIP							K8s metrics
prometheus-node-exporter			ClusterIP							Node metrics per node


 6. ServiceAccounts


Component										ServiceAccount

Prometheus Server						prometheus-prometheus-prometheus
Alertmanager							prometheus-prometheus-alertmanager
Prometheus Operator						prometheus-kube-prometheus-operator
Grafana									prometheus-grafana
Node Exporter							prometheus-node-exporter
kube-state-metrics						prometheus-kube-state-metrics



What Do You Get When You Install Prometheus in Non-Kubernetes?


You get the core Prometheus ecosystem, but NOT the Kubernetes-native features.

âœ… Components Installed

Component							Installed?									Notes

Prometheus Server						âœ…								The heart of Prometheus
Time Series Database (TSDB)				âœ…								Stores all scraped metrics
Alertmanager						Optional							Can be installed separately
Node Exporter							âœ…								System metrics collector 
Pushgateway	Optional			For short-lived jobs
Grafana	Optional					Dashboards
Exporters (MySQL, Redis, etc)		Optional							You can add as needed





You do not get:

CRDs (no Kubernetes API)

ServiceMonitor/PodMonitor (theyâ€™re Kubernetes CRDs)

Prometheus Operator (only works in Kubernetes)





--> Why we need exporter because most systems, applications and services by default don't expose them on an http endpoint to be consumed by a Prometheus server.

--> Exporters collect metrics and expose them in a format prometheus expects and expose them at /metrics endpoint.

--> Prometheus has several native exporters.

	Node exporters (Linux Servers)
	Windows
	Mysql
	Apache
	HAproxy
	
--> We have expoeters for systems, But we don't have exporters for our custom applications.

	Number of errors/exceptions
	Latency of requests

--> So Prometheus comes with client libraries that allow you to expose any application metrics you need prometheus to track

--> Node expoerters and prometheus we need to configure as systemd service.

--> Every metric is assigned by 2 lables by default --> instance and job

	instance --> target Ip
	job --> job name (Which is configured in prometheus.yaml file)
	
-->  Instrumentation means --> Adding code to your application or system to emit metrics that Prometheus can scrape and understand

--> Metrics can also be scraped from scraped containerized environments.

--> Docker Engine metrics and Container metrics using cAdvisor





1. ping

Purpose: Test basic connectivity (can I reach that host?).

How it works: Sends ICMP â€œecho requestâ€ packets and waits for â€œecho reply.â€

Use when:

You want to check if a server is up and reachable.

Measure latency (round-trip time).

Example:

ping google.com


ðŸ”¹ 2. nslookup

Purpose: Query DNS servers to resolve domain names to IPs (and vice versa).

How it works: Talks directly to DNS servers to fetch records (A, MX, etc.).

Use when:

You want to check DNS resolution problems.

Verify if a domain resolves to the expected IP.

Example:

nslookup myexample.com

ðŸ”¹ 3. dig (Domain Information Groper)

Purpose: Like nslookup but more powerful and detailed.

How it works: Queries DNS records with full details (TTL, authoritative servers, etc.).

Use when:

You need detailed DNS troubleshooting.

Want to check specific DNS records (A, MX, TXT, CNAME).

Verify which DNS server is responding.

Example:

dig myexample.com A
dig myexample.com MX

ðŸ”¹ 4. telnet

Purpose: Test if a specific port on a host is open.

How it works: Creates a raw TCP connection to a given host:port.

Use when:

You want to test if a service (e.g., webserver, database) is listening on a port.

Example: test if port 80 (HTTP) or 443 (HTTPS) is open.

Example:

telnet myexample.com 80

ðŸ”¹ 5. traceroute (Linux) / tracert (Windows)

Purpose: Show the path packets take to reach a destination.

How it works: Sends packets with increasing TTL (time-to-live) and records each hop (router) until destination.

Use when:

You want to debug routing issues.

See where packets are being dropped or delayed.

Example:

traceroute google.com   # Linux
tracert google.com      # Windows





Build secrets allow you to pass sensitive information to a Docker build without leaving it in the final image layers.
First, you pass the secret using --secret in the build command, and then you consume it in the Dockerfile with RUN --mount=type=secret.
The secret is only available temporarily inside /run/secrets/<id> or as an environment variable during the build step.


Step 1: Pass a secret to Docker build

You provide the secret at build time using --secret:

a) From a file
docker build --secret id=aws,src=$HOME/.aws/credentials .


id=aws â†’ the name you will refer to in the Dockerfile

src=... â†’ path to the secret file on your host

b) From an environment variable
export API_TOKEN="my-secret-token"
docker build --secret id=API_TOKEN,env=API_TOKEN .


This mounts API_TOKEN into the build container as a file by default: /run/secrets/API_TOKEN

Step 2: Consume the secret in the Dockerfile

You use the --mount=type=secret flag in the RUN instruction.

a) Default file mount

# Dockerfile
RUN --mount=type=secret,id=aws \
    AWS_SHARED_CREDENTIALS_FILE=/run/secrets/aws \
    aws s3 cp s3://mybucket/file.txt /data/


Secret aws is temporarily available at /run/secrets/aws

After the RUN finishes, the secret disappears â€” it is not stored in image layers.

b) Custom target path


RUN --mount=type=secret,id=aws,target=/root/.aws/credentials \
    aws s3 cp s3://mybucket/file.txt /data/


Secret file will appear at /root/.aws/credentials instead of the default /run/secrets/<id>.